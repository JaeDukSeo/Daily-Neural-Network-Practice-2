{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<ol>\n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction (Pursuit Methods)</li>\n",
    "<li>f: Linear Reward Penalty (Pursuit Methods)</li>\n",
    "<li>g: UBC 1</li>\n",
    "<li>h: UCB 1-Tuned</li>\n",
    "<li>i: Thompson Sampling (beta)</li>\n",
    "<li>j: Thompson Sampling (uniform)</li>\n",
    "<li>k: Neural Network</li>\n",
    "<li>l: softmax </li>\n",
    "<li>m: Gradient Bandits</li>\n",
    "<li>n: Non Stationary</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.383974Z",
     "start_time": "2019-01-14T05:31:26.552073Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy,time,sys\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)\n",
    "tf.set_random_seed(678)\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.403920Z",
     "start_time": "2019-01-14T05:31:39.394945Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Best Choice:  11 0.7364685816073836\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 12\n",
    "num_ep  = 20\n",
    "num_iter= 1000\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:32:08.136712Z",
     "start_time": "2019-01-14T05:32:08.110745Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.481 0.057 0.354 0.528 0.599 0.423 0.17  0.275 0.084 0.182 0.084 0.758]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1,num_bandit) < gt_prob\n",
    "        temp_expect    = temp_expect + current_reward\n",
    "\n",
    "    a_expect[eps,:] = temp_expect/temp_choice\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:36.813205Z",
     "start_time": "2019-01-14T06:25:36.653641Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.484 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "b_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    b_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:37.435275Z",
     "start_time": "2019-01-14T06:25:37.184025Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.461 0.049 0.316 0.513 0.577 0.401 0.164 0.244 0.071 0.184 0.074 0.734]\n"
     ]
    }
   ],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "c_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "  \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    c_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:27:32.900065Z",
     "start_time": "2019-01-14T06:27:32.701597Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.436 0.018 0.359 0.438 0.487 0.39  0.155 0.221 0.034 0.118 0.076 0.734]\n"
     ]
    }
   ],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "d_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.99 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    d_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:27:33.249131Z",
     "start_time": "2019-01-14T06:27:33.086566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(b_regret_total.mean(0),label='greedy')\n",
    "plt.plot(c_regret_total.mean(0),label='e greedy')\n",
    "plt.plot(d_regret_total.mean(0),label='decay e greedy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:04.790954Z",
     "start_time": "2019-01-13T08:53:03.877787Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.095 0.062 0.086 0.098 0.104 0.09  0.069 0.078 0.063 0.07  0.063 0.123]\n",
      "Expected Normalized\n",
      "[0.379 0.247 0.343 0.391 0.416 0.359 0.278 0.312 0.251 0.282 0.254 0.494]\n"
     ]
    }
   ],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "e_regret_total = np.zeros((num_ep,num_iter))\n",
    "      \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(e_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(e_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:06.356294Z",
     "start_time": "2019-01-13T08:53:05.320152Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.093 0.063 0.085 0.099 0.105 0.088 0.071 0.078 0.063 0.07  0.065 0.12 ]\n",
      "Expected Normalized\n",
      "[0.374 0.251 0.34  0.396 0.423 0.353 0.283 0.313 0.254 0.28  0.259 0.483]\n"
     ]
    }
   ],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.001\n",
    "    beta  = 0.0001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(f_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:17:20.818768Z",
     "start_time": "2019-01-14T06:17:20.548499Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.497 0.058 0.34  0.492 0.564 0.427 0.134 0.274 0.057 0.139 0.075 0.737]\n"
     ]
    }
   ],
   "source": [
    "# g UBC\n",
    "g_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "g_estimation   = np.zeros((num_ep,num_bandit))\n",
    "g_reward       = np.zeros((num_ep,num_iter))\n",
    "g_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(2*np.log(iter+1)/(temp_pull_count+1)))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    g_pull_count[eps,:]   = temp_pull_count\n",
    "    g_estimation[eps,:]   = temp_estimation\n",
    "    g_reward[eps,:]       = temp_reward\n",
    "    g_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(g_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:21:50.280119Z",
     "start_time": "2019-01-14T06:21:49.983217Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.46  0.047 0.34  0.514 0.568 0.428 0.142 0.244 0.058 0.145 0.066 0.732]\n"
     ]
    }
   ],
   "source": [
    "# h UBC Tuned\n",
    "h_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "h_estimation   = np.zeros((num_ep,num_bandit))\n",
    "h_reward       = np.zeros((num_ep,num_iter))\n",
    "h_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit) \n",
    "    temp_estimation   = np.zeros(num_bandit) \n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        current_min_value = 1\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(np.log(iter+1)/(temp_pull_count+1)*current_min_value))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    h_pull_count[eps,:]   = temp_pull_count\n",
    "    h_estimation[eps,:]   = temp_estimation\n",
    "    h_reward[eps,:]       = temp_reward\n",
    "    h_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(h_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:56:39.592302Z",
     "start_time": "2019-01-13T08:53:08.806546Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.412 0.141 0.367 0.409 0.588 0.402 0.194 0.325 0.194 0.233 0.214 0.743]\n"
     ]
    }
   ],
   "source": [
    "# i Thompson Sampling (beta) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.beta(a=1+w,b=1+t-w).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:43.572283Z",
     "start_time": "2019-01-13T08:56:40.286696Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.702 0.469 0.715 0.747 0.768 0.701 0.648 0.616 0.503 0.516 0.603 0.885]\n"
     ]
    }
   ],
   "source": [
    "# j Thompson Sampling (uniform) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.uniform(w/(t+0.000000001),1-w/(t+0.000000001)).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:45.319236Z",
     "start_time": "2019-01-13T08:59:44.170002Z"
    },
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.468 0.073 0.311 0.505 0.543 0.373 0.13  0.27  0.071 0.164 0.08  0.7  ]\n",
      "Scaled \n",
      "[0.487 0.061 0.318 0.527 0.568 0.384 0.123 0.273 0.059 0.159 0.069 0.736]\n"
     ]
    }
   ],
   "source": [
    "# k neural network (with adam)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "            \n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def d_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    weights = np.random.randn(num_bandit,1)\n",
    "    moment  = np.zeros_like(weights); \n",
    "    velocity = np.zeros_like(weights);\n",
    "    epsilon  = 1.0 \n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        if np.random.uniform(0,1)>epsilon:\n",
    "            current_choice = np.argmax(weights)\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "        else:\n",
    "            current_choice = np.random.choice(np.arange(num_bandit))\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "\n",
    "        layer1 = current_input @ weights\n",
    "        layer1a= sigmoid(layer1)\n",
    "\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        # KL Divergence https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "        grad3 = np.log(layer1a+0.0000001) - np.log(temp_estimation[current_choice]/(temp_pull_count[current_choice])+0.0000001)\n",
    "        grad2 = d_sigmoid(layer1)\n",
    "        grad1 = current_input\n",
    "        grad  = grad1.T @ (grad3 * grad2)\n",
    "        \n",
    "        moment   = 0.9*moment + (1-0.9) * grad\n",
    "        velocity = 0.999*velocity + (1-0.999) * grad**2\n",
    "        moment_hat   = moment/(1-0.9)\n",
    "        velocity_hat = velocity/(1-0.999)\n",
    "        weights  = weights - 0.08 * (moment_hat/(np.sqrt(velocity_hat)+1e-8))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # Decay the learning rate\n",
    "        epsilon = epsilon * 0.999\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = np.squeeze(sigmoid(weights))\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))\n",
    "print('Scaled ')\n",
    "print((gt_prob.max()-gt_prob.min())*(k_estimation.mean(0)-k_estimation.mean(0).min())/(k_estimation.mean(0).max()-k_estimation.mean(0).min()) + gt_prob.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:46.606806Z",
     "start_time": "2019-01-13T08:59:46.039910Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.498 0.062 0.367 0.515 0.609 0.416 0.173 0.272 0.077 0.189 0.088 0.738]\n"
     ]
    }
   ],
   "source": [
    "# l softmax\n",
    "l_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "l_estimation   = np.zeros((num_ep,num_bandit))\n",
    "l_reward       = np.zeros((num_ep,num_iter))\n",
    "l_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "l_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    tempture = 300\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation/tempture) / np.sum(np.exp(temp_estimation/tempture))\n",
    "        cdf = np.cumsum(pi)\n",
    "        current_choice = np.where(np.random.uniform(0,1) < cdf)[0][0]\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        tempture = tempture * 0.999999\n",
    "        \n",
    "    l_pull_count[eps,:]   = temp_pull_count\n",
    "    l_estimation[eps,:]   = temp_estimation\n",
    "    l_reward[eps,:]       = temp_reward\n",
    "    l_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    l_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(l_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:10:36.712012Z",
     "start_time": "2019-01-14T06:10:36.708024Z"
    }
   },
   "outputs": [],
   "source": [
    "# m gradient base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n non stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:47.235732Z",
     "start_time": "2019-01-13T08:59:47.225357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions  NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "2. [ Archived Post ] Random Note about Multi-Arm Bandit Problem 2. (2019). Medium. Retrieved 13 January 2019, from https://medium.com/@SeoJaeDuk/archived-post-random-note-about-multi-arm-bandit-problem-2-5c522d1dfbdc\n",
    "3. Vieira, T. (2014). KL-divergence as an objective function  Graduate Descent. Timvieira.github.io. Retrieved 13 January 2019, from https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "4. Some Reinforcement Learning: The Greedy and Explore-Exploit Algorithms for the Multi-Armed Bandit Framework in Python. (2019). Datasciencecentral.com. Retrieved 13 January 2019, from https://www.datasciencecentral.com/profiles/blogs/some-reinforcement-learning-the-greedy-and-explore-exploit\n",
    "5. (2019). Cs.mcgill.ca. Retrieved 13 January 2019, from https://www.cs.mcgill.ca/~vkules/bandits.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
