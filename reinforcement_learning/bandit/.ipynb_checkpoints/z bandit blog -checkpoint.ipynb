{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<ol>\n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction (Pursuit Methods)</li>\n",
    "<li>f: Linear Reward Penalty (Pursuit Methods)</li>\n",
    "<li>g: UBC 1</li>\n",
    "<li>h: UCB 1-Tuned</li>\n",
    "<li>i: Thompson Sampling (beta)</li>\n",
    "<li>j: Thompson Sampling (uniform)</li>\n",
    "<li>k: Neural Network</li>\n",
    "<li>l: softmax </li>\n",
    "<li>m: Gradient Bandits</li>\n",
    "<li>n: Non Stationary</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.383974Z",
     "start_time": "2019-01-14T05:31:26.552073Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy,time,sys\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)\n",
    "tf.set_random_seed(678)\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.403920Z",
     "start_time": "2019-01-14T05:31:39.394945Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Best Choice:  11 0.7364685816073836\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 12\n",
    "num_ep  = 20\n",
    "num_iter= 1000\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:32:08.136712Z",
     "start_time": "2019-01-14T05:32:08.110745Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.481 0.057 0.354 0.528 0.599 0.423 0.17  0.275 0.084 0.182 0.084 0.758]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1,num_bandit) < gt_prob\n",
    "        temp_expect    = temp_expect + current_reward\n",
    "\n",
    "    a_expect[eps,:] = temp_expect/temp_choice\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:01.541630Z",
     "start_time": "2019-01-13T08:53:01.282222Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.49 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "b_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    b_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:23:39.596383Z",
     "start_time": "2019-01-14T06:23:39.335559Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.442 0.048 0.331 0.445 0.588 0.42  0.147 0.282 0.061 0.175 0.085 0.737]\n"
     ]
    }
   ],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "c_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "  \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    c_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:18.276234Z",
     "start_time": "2019-01-14T06:25:18.094030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27b4e65f470>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(c_regret_total.mean(0))\n",
    "plt.plot(c_regret_total.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:03.343630Z",
     "start_time": "2019-01-13T08:53:02.983500Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.471 0.066 0.344 0.512 0.575 0.43  0.183 0.279 0.066 0.179 0.101 0.732]\n"
     ]
    }
   ],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.999 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:04.790954Z",
     "start_time": "2019-01-13T08:53:03.877787Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.095 0.062 0.086 0.098 0.104 0.09  0.069 0.078 0.063 0.07  0.063 0.123]\n",
      "Expected Normalized\n",
      "[0.379 0.247 0.343 0.391 0.416 0.359 0.278 0.312 0.251 0.282 0.254 0.494]\n"
     ]
    }
   ],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(e_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(e_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:06.356294Z",
     "start_time": "2019-01-13T08:53:05.320152Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.093 0.063 0.085 0.099 0.105 0.088 0.071 0.078 0.063 0.07  0.065 0.12 ]\n",
      "Expected Normalized\n",
      "[0.374 0.251 0.34  0.396 0.423 0.353 0.283 0.313 0.254 0.28  0.259 0.483]\n"
     ]
    }
   ],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.001\n",
    "    beta  = 0.0001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(f_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:17:20.818768Z",
     "start_time": "2019-01-14T06:17:20.548499Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.497 0.058 0.34  0.492 0.564 0.427 0.134 0.274 0.057 0.139 0.075 0.737]\n"
     ]
    }
   ],
   "source": [
    "# g UBC\n",
    "g_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "g_estimation   = np.zeros((num_ep,num_bandit))\n",
    "g_reward       = np.zeros((num_ep,num_iter))\n",
    "g_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(2*np.log(iter+1)/(temp_pull_count+1)))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    g_pull_count[eps,:]   = temp_pull_count\n",
    "    g_estimation[eps,:]   = temp_estimation\n",
    "    g_reward[eps,:]       = temp_reward\n",
    "    g_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(g_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:21:50.280119Z",
     "start_time": "2019-01-14T06:21:49.983217Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.46  0.047 0.34  0.514 0.568 0.428 0.142 0.244 0.058 0.145 0.066 0.732]\n"
     ]
    }
   ],
   "source": [
    "# h UBC Tuned\n",
    "h_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "h_estimation   = np.zeros((num_ep,num_bandit))\n",
    "h_reward       = np.zeros((num_ep,num_iter))\n",
    "h_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit) \n",
    "    temp_estimation   = np.zeros(num_bandit) \n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        current_min_value = 1\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(np.log(iter+1)/(temp_pull_count+1)*current_min_value))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    h_pull_count[eps,:]   = temp_pull_count\n",
    "    h_estimation[eps,:]   = temp_estimation\n",
    "    h_reward[eps,:]       = temp_reward\n",
    "    h_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(h_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:56:39.592302Z",
     "start_time": "2019-01-13T08:53:08.806546Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.412 0.141 0.367 0.409 0.588 0.402 0.194 0.325 0.194 0.233 0.214 0.743]\n"
     ]
    }
   ],
   "source": [
    "# i Thompson Sampling (beta) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.beta(a=1+w,b=1+t-w).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:43.572283Z",
     "start_time": "2019-01-13T08:56:40.286696Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.702 0.469 0.715 0.747 0.768 0.701 0.648 0.616 0.503 0.516 0.603 0.885]\n"
     ]
    }
   ],
   "source": [
    "# j Thompson Sampling (uniform) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.uniform(w/(t+0.000000001),1-w/(t+0.000000001)).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:45.319236Z",
     "start_time": "2019-01-13T08:59:44.170002Z"
    },
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.468 0.073 0.311 0.505 0.543 0.373 0.13  0.27  0.071 0.164 0.08  0.7  ]\n",
      "Scaled \n",
      "[0.487 0.061 0.318 0.527 0.568 0.384 0.123 0.273 0.059 0.159 0.069 0.736]\n"
     ]
    }
   ],
   "source": [
    "# k neural network (with adam)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "            \n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def d_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    weights = np.random.randn(num_bandit,1)\n",
    "    moment  = np.zeros_like(weights); \n",
    "    velocity = np.zeros_like(weights);\n",
    "    epsilon  = 1.0 \n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        if np.random.uniform(0,1)>epsilon:\n",
    "            current_choice = np.argmax(weights)\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "        else:\n",
    "            current_choice = np.random.choice(np.arange(num_bandit))\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "\n",
    "        layer1 = current_input @ weights\n",
    "        layer1a= sigmoid(layer1)\n",
    "\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        # KL Divergence https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "        grad3 = np.log(layer1a+0.0000001) - np.log(temp_estimation[current_choice]/(temp_pull_count[current_choice])+0.0000001)\n",
    "        grad2 = d_sigmoid(layer1)\n",
    "        grad1 = current_input\n",
    "        grad  = grad1.T @ (grad3 * grad2)\n",
    "        \n",
    "        moment   = 0.9*moment + (1-0.9) * grad\n",
    "        velocity = 0.999*velocity + (1-0.999) * grad**2\n",
    "        moment_hat   = moment/(1-0.9)\n",
    "        velocity_hat = velocity/(1-0.999)\n",
    "        weights  = weights - 0.08 * (moment_hat/(np.sqrt(velocity_hat)+1e-8))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # Decay the learning rate\n",
    "        epsilon = epsilon * 0.999\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = np.squeeze(sigmoid(weights))\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))\n",
    "print('Scaled ')\n",
    "print((gt_prob.max()-gt_prob.min())*(k_estimation.mean(0)-k_estimation.mean(0).min())/(k_estimation.mean(0).max()-k_estimation.mean(0).min()) + gt_prob.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:46.606806Z",
     "start_time": "2019-01-13T08:59:46.039910Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.498 0.062 0.367 0.515 0.609 0.416 0.173 0.272 0.077 0.189 0.088 0.738]\n"
     ]
    }
   ],
   "source": [
    "# l softmax\n",
    "l_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "l_estimation   = np.zeros((num_ep,num_bandit))\n",
    "l_reward       = np.zeros((num_ep,num_iter))\n",
    "l_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "l_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    tempture = 300\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation/tempture) / np.sum(np.exp(temp_estimation/tempture))\n",
    "        cdf = np.cumsum(pi)\n",
    "        current_choice = np.where(np.random.uniform(0,1) < cdf)[0][0]\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        tempture = tempture * 0.999999\n",
    "        \n",
    "    l_pull_count[eps,:]   = temp_pull_count\n",
    "    l_estimation[eps,:]   = temp_estimation\n",
    "    l_reward[eps,:]       = temp_reward\n",
    "    l_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    l_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(l_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:10:36.712012Z",
     "start_time": "2019-01-14T06:10:36.708024Z"
    }
   },
   "outputs": [],
   "source": [
    "# m gradient base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n non stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:47.235732Z",
     "start_time": "2019-01-13T08:59:47.225357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions  NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "2. [ Archived Post ] Random Note about Multi-Arm Bandit Problem 2. (2019). Medium. Retrieved 13 January 2019, from https://medium.com/@SeoJaeDuk/archived-post-random-note-about-multi-arm-bandit-problem-2-5c522d1dfbdc\n",
    "3. Vieira, T. (2014). KL-divergence as an objective function  Graduate Descent. Timvieira.github.io. Retrieved 13 January 2019, from https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "4. Some Reinforcement Learning: The Greedy and Explore-Exploit Algorithms for the Multi-Armed Bandit Framework in Python. (2019). Datasciencecentral.com. Retrieved 13 January 2019, from https://www.datasciencecentral.com/profiles/blogs/some-reinforcement-learning-the-greedy-and-explore-exploit\n",
    "5. (2019). Cs.mcgill.ca. Retrieved 13 January 2019, from https://www.cs.mcgill.ca/~vkules/bandits.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
