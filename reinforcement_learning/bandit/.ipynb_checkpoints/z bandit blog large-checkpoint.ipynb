{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<ol>\n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction (Pursuit Methods)</li>\n",
    "<li>f: Linear Reward Penalty (Pursuit Methods)</li>\n",
    "<li>g: UBC 1</li>\n",
    "<li>h: UCB 1-Tuned</li>\n",
    "<li>i: Thompson Sampling (beta)</li>\n",
    "<li>j: Thompson Sampling (uniform)</li>\n",
    "<li>k: Neural Network</li>\n",
    "<li>l: softmax </li>\n",
    "<li>m: Gradient Bandits</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T09:08:02.602498Z",
     "start_time": "2019-01-14T09:07:59.565321Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy,time,sys\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)\n",
    "tf.set_random_seed(678)\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T09:14:08.333190Z",
     "start_time": "2019-01-14T09:14:08.320208Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.963e-01 3.457e-01 3.940e-02 3.219e-01 9.101e-01 5.097e-01 6.107e-01\n",
      " 6.928e-01 4.142e-01 5.656e-01 2.767e-01 9.413e-02 5.395e-01 4.435e-01\n",
      " 6.662e-01 9.208e-02 1.530e-01 7.446e-02 5.772e-01 7.942e-02 9.051e-01\n",
      " 7.291e-01 9.097e-01 3.260e-01 1.912e-01 7.887e-01 4.240e-02 6.255e-01\n",
      " 2.291e-02 7.146e-01 1.360e-01 3.511e-01 7.320e-01 5.116e-01 4.448e-01\n",
      " 3.702e-01 6.859e-01 8.909e-01 5.329e-01 9.193e-02 8.489e-01 2.207e-01\n",
      " 2.050e-02 1.154e-01 8.374e-01 4.480e-01 2.933e-01 5.043e-01 3.156e-01\n",
      " 8.169e-01 4.842e-01 1.856e-01 9.017e-01 1.833e-01 7.597e-01 9.808e-01\n",
      " 7.148e-01 7.997e-02 5.469e-01 3.888e-01 6.136e-01 1.520e-01 1.349e-01\n",
      " 2.493e-01 3.017e-02 5.595e-01 1.263e-01 1.958e-01 8.585e-01 8.537e-01\n",
      " 7.748e-01 8.730e-01 5.480e-01 8.650e-01 4.029e-01 6.053e-01 3.376e-01\n",
      " 6.420e-01 3.129e-01 1.465e-01 4.518e-01 8.860e-01 5.715e-01 4.841e-01\n",
      " 7.920e-01 8.943e-01 2.852e-01 8.840e-01 6.107e-02 2.341e-01 6.369e-01\n",
      " 1.074e-01 5.746e-01 5.445e-02 8.488e-02 3.358e-01 2.169e-02 9.986e-01\n",
      " 5.361e-01 4.704e-01 5.451e-01 9.020e-01 3.656e-01 2.097e-01 2.589e-01\n",
      " 6.769e-01 9.010e-01 4.773e-01 8.704e-01 9.480e-01 5.742e-01 4.833e-02\n",
      " 9.665e-01 4.727e-01 6.751e-01 9.967e-01 6.914e-01 6.321e-01 5.686e-01\n",
      " 5.381e-01 3.089e-01 1.085e-01 6.605e-01 9.623e-01 8.525e-01 3.609e-01\n",
      " 6.048e-01 4.570e-01 7.889e-01 3.266e-01 7.398e-01 2.059e-01 4.612e-01\n",
      " 3.568e-01 6.624e-01 5.015e-01 8.172e-02 8.618e-01 6.501e-02 3.390e-01\n",
      " 6.979e-01 1.403e-01 5.724e-01 5.642e-01 7.521e-01 9.235e-01 8.854e-01\n",
      " 8.909e-02 3.882e-01 5.475e-01 6.446e-02 1.406e-01 2.869e-01 3.522e-01\n",
      " 2.688e-01 9.299e-01 3.654e-01 3.447e-01 2.979e-01 2.484e-01 4.413e-01\n",
      " 1.772e-01 1.027e-01 1.869e-01 1.059e-01 8.357e-02 4.996e-01 2.804e-01\n",
      " 1.460e-01 4.795e-01 4.816e-01 8.752e-03 3.746e-01 3.280e-02 2.134e-02\n",
      " 9.105e-01 1.684e-01 4.234e-02 8.948e-01 5.525e-02 4.698e-01 6.332e-02\n",
      " 1.526e-01 2.356e-01 9.736e-01 5.589e-01 8.604e-01 1.735e-01 6.778e-01\n",
      " 1.697e-02 3.206e-01 1.856e-01 5.431e-01 8.584e-01 1.459e-01 2.273e-01\n",
      " 5.137e-02 9.684e-02 4.577e-01 2.859e-01 2.609e-01 2.261e-01 9.071e-01\n",
      " 2.224e-01 9.128e-01 2.866e-01 9.211e-01 2.636e-02 5.991e-01 7.604e-01\n",
      " 8.545e-01 9.592e-01 7.419e-01 7.497e-01 9.743e-01 4.156e-01 9.913e-01\n",
      " 2.315e-01 6.228e-01 8.633e-01 5.826e-01 4.963e-02 7.568e-02 3.736e-01\n",
      " 5.580e-01 5.780e-01 8.907e-01 9.963e-01 4.373e-01 5.796e-01 8.228e-01\n",
      " 3.918e-01 2.587e-01 3.920e-01 1.823e-01 5.145e-01 7.134e-01 7.668e-01\n",
      " 9.606e-01 3.265e-01 1.324e-01 4.191e-01 2.681e-01 3.753e-01 6.177e-01\n",
      " 7.950e-01 5.246e-01 7.064e-01 1.599e-01 3.538e-01 6.610e-01 5.531e-01\n",
      " 5.927e-02 3.743e-01 6.029e-02 6.077e-01 7.910e-01 9.456e-03 2.712e-01\n",
      " 8.202e-01 2.700e-01 6.398e-01 3.210e-01 6.083e-02 1.585e-01 9.779e-01\n",
      " 9.596e-01 4.100e-01 3.910e-01 4.431e-01 1.999e-02 8.525e-01 8.987e-02\n",
      " 3.968e-01 7.653e-01 6.045e-01 2.524e-02 2.200e-01 1.829e-01 5.484e-01\n",
      " 9.245e-01 8.867e-01 4.657e-01 4.289e-01 4.432e-02 1.529e-01 6.248e-01\n",
      " 2.302e-02 3.446e-01 9.511e-01 8.881e-01 2.754e-01 7.790e-01 1.939e-01\n",
      " 3.284e-01 8.038e-02 7.964e-01 1.459e-01 4.676e-01 6.899e-01 9.580e-01\n",
      " 2.860e-01 8.658e-01 8.058e-01 7.729e-01 3.259e-01 5.799e-02 6.837e-01\n",
      " 2.373e-01 9.714e-01 2.923e-01 6.657e-01 4.624e-01 7.649e-01 1.466e-01\n",
      " 1.621e-01 1.688e-01 6.729e-01 5.435e-01 3.150e-01 9.118e-01 9.140e-01\n",
      " 6.234e-01 7.248e-01 3.895e-01 1.079e-02 4.918e-01 8.787e-01 1.936e-03\n",
      " 1.281e-01 9.705e-01 4.319e-01 3.975e-01 2.867e-01 5.189e-01 7.465e-01\n",
      " 9.344e-01 6.993e-02 5.320e-01 1.523e-01 9.521e-01 8.790e-01 2.659e-02\n",
      " 4.351e-01 3.571e-01 9.401e-02 1.377e-01 9.827e-02 8.691e-02 6.278e-01\n",
      " 8.647e-02 5.095e-01 9.642e-01 8.486e-01 7.473e-01 6.074e-01 1.896e-01\n",
      " 1.365e-01 2.842e-03 3.109e-01 1.192e-01 3.795e-01 3.668e-01 2.854e-01\n",
      " 3.452e-01 2.673e-01 2.933e-01 6.538e-01 1.396e-01 1.959e-01 1.342e-01\n",
      " 4.729e-01 9.142e-01 4.761e-01 3.592e-01 6.651e-01 3.454e-01 6.930e-01\n",
      " 5.749e-01 1.846e-01 9.246e-02 8.416e-01 7.133e-01 3.925e-01 7.497e-01\n",
      " 9.903e-01 6.178e-01 2.374e-01 5.281e-01 7.395e-01 4.255e-01 2.500e-01\n",
      " 9.374e-01 2.251e-01 8.334e-01 2.894e-01 2.126e-01 9.823e-01 5.825e-01\n",
      " 5.983e-01 9.824e-01 7.388e-01 8.850e-01 1.503e-02 9.387e-01 1.087e-01\n",
      " 2.686e-01 5.022e-01 2.729e-01 9.636e-01 4.910e-01 7.766e-01 1.192e-02\n",
      " 5.579e-01 9.920e-01 8.345e-01 9.015e-01 3.389e-01 6.070e-01 7.244e-01\n",
      " 9.342e-01 8.307e-01 9.638e-02 9.604e-01 6.616e-01 1.307e-01 9.897e-01\n",
      " 4.823e-01 6.415e-02 6.051e-01 5.816e-01 4.306e-01 1.517e-01 6.478e-01\n",
      " 2.432e-01 9.633e-01 1.090e-01 7.114e-01 2.658e-02 1.768e-01 6.354e-01\n",
      " 8.686e-04 3.810e-01 9.233e-01 1.142e-01 2.317e-01 8.112e-01 1.664e-01\n",
      " 7.637e-01 7.730e-01 9.462e-01 8.736e-02 3.047e-01 8.989e-01 9.223e-01\n",
      " 9.634e-01 2.640e-01 8.407e-01 8.535e-02 5.130e-01 3.576e-01 6.884e-01\n",
      " 1.577e-01 6.739e-01 9.351e-01 1.299e-01 2.174e-01 9.276e-01 1.420e-01\n",
      " 3.702e-01 1.864e-01 3.244e-01 9.900e-01 1.409e-01 3.073e-02 1.842e-01\n",
      " 6.282e-02 4.760e-01 2.725e-01 4.892e-01 9.533e-01 1.379e-01 3.438e-01\n",
      " 8.569e-01 2.764e-01 4.437e-01 9.919e-01 5.754e-01 7.821e-01 8.110e-01\n",
      " 8.207e-01 6.822e-02 8.038e-02 4.869e-01 3.147e-01 3.346e-01 8.909e-01\n",
      " 9.862e-01 3.797e-01 6.385e-01 9.082e-01 6.173e-01 5.253e-01 9.469e-03\n",
      " 5.216e-02 7.832e-01 5.119e-01 8.914e-01 8.649e-01 8.443e-01 1.368e-01\n",
      " 8.608e-01 7.176e-01 3.514e-01 2.925e-01 4.264e-01 1.152e-01 6.530e-01\n",
      " 1.727e-01 6.100e-01 5.846e-01 7.550e-01 4.472e-01 3.783e-01 5.566e-01\n",
      " 5.035e-01 3.751e-01 3.625e-01 2.221e-01 7.827e-01 3.876e-01 3.654e-01\n",
      " 9.430e-01 4.330e-01 5.518e-01 8.080e-01 3.522e-01 6.415e-01 2.588e-02\n",
      " 4.873e-01 8.269e-01 6.027e-01 7.839e-01 3.376e-01 6.923e-01 7.118e-01\n",
      " 3.074e-01 5.906e-01 6.306e-01 6.917e-01 5.127e-01 9.752e-01 1.071e-02\n",
      " 8.600e-01 1.637e-01 9.080e-01 9.611e-01 9.150e-01 3.285e-01 9.371e-01\n",
      " 4.450e-01 4.118e-01 7.795e-01 1.313e-01 1.343e-01 9.394e-01 7.982e-01\n",
      " 4.060e-01 5.635e-01 1.018e-01 4.738e-01 4.763e-01 6.173e-01 5.390e-01\n",
      " 5.577e-01 1.432e-01 1.782e-01 5.244e-01 7.026e-01 9.065e-01 9.446e-02\n",
      " 5.937e-01 5.153e-01 9.226e-01 4.770e-01 1.501e-01 8.875e-01 5.548e-01\n",
      " 2.562e-01 3.063e-01 7.289e-01 8.655e-01 6.458e-01 2.332e-01 9.076e-01\n",
      " 7.311e-01 2.344e-01 4.677e-01 2.308e-01 7.592e-01 5.593e-01 2.172e-02\n",
      " 7.874e-01 2.256e-01 4.709e-01 7.069e-01 8.906e-01 1.973e-01 9.588e-01\n",
      " 1.692e-01 9.284e-01 5.927e-01 6.665e-01 1.270e-01 8.420e-01 8.143e-01\n",
      " 2.422e-01 2.399e-01 2.502e-02 2.301e-01 2.004e-01 4.583e-01 5.152e-01\n",
      " 6.892e-01 1.177e-01 7.301e-01 1.119e-01 6.912e-01 2.647e-01 4.675e-01\n",
      " 2.948e-01 2.862e-01 3.316e-01 8.671e-01 4.415e-01 8.093e-01 3.851e-01\n",
      " 1.461e-03 5.905e-01 4.981e-01 1.949e-01 3.046e-01 7.830e-01 3.254e-01\n",
      " 9.908e-01 1.864e-01 7.888e-01 5.268e-02 1.635e-02 1.573e-01 2.032e-01\n",
      " 5.192e-01 9.517e-01 6.788e-01 5.594e-01 6.697e-01 2.423e-02 5.799e-01\n",
      " 6.988e-01 6.674e-01 4.606e-01 7.050e-01 2.127e-01 2.156e-01 3.118e-01\n",
      " 3.414e-01 5.825e-01 9.533e-01 5.025e-01 3.094e-01 5.974e-01 5.574e-01\n",
      " 6.585e-01 4.576e-01 5.978e-01 7.332e-01 3.218e-01 3.354e-01 8.752e-01\n",
      " 9.242e-01 2.176e-01 7.803e-01 1.876e-01 9.658e-01 5.676e-01 3.278e-01\n",
      " 1.924e-01 4.934e-01 1.425e-01 8.619e-01 3.641e-01 7.849e-01 3.407e-01\n",
      " 1.281e-01 5.420e-01 2.247e-01 5.125e-01 9.772e-01 6.712e-01 3.536e-01\n",
      " 9.081e-01 7.878e-01 3.961e-01 7.120e-01 2.340e-01 6.320e-01 1.163e-01\n",
      " 1.524e-01 1.764e-01 7.330e-01 3.571e-01 9.898e-01 7.230e-01 3.375e-01\n",
      " 7.692e-01 3.335e-01 2.062e-01 7.707e-02 2.408e-01 5.677e-01 1.466e-01\n",
      " 6.457e-01 2.882e-01 1.632e-01 9.854e-01 9.472e-01 4.053e-01 9.076e-01\n",
      " 2.706e-01 3.273e-01 6.533e-01 8.155e-01 9.746e-01 7.212e-01 9.409e-01\n",
      " 9.169e-01 6.506e-02 7.310e-01 3.038e-01 6.890e-01 2.572e-01 2.929e-01\n",
      " 8.518e-01 5.293e-01 3.725e-01 7.877e-01 7.022e-01 6.831e-01 5.604e-01\n",
      " 9.564e-01 8.450e-01 8.627e-01 3.090e-01 9.623e-01 7.775e-01 5.144e-01\n",
      " 6.171e-02 8.090e-01 1.173e-01 5.182e-01 9.149e-01 6.948e-01 9.440e-01\n",
      " 1.984e-01 8.766e-01 6.422e-01 6.434e-02 4.601e-01 7.304e-01 9.829e-01\n",
      " 2.125e-01 8.749e-01 4.157e-01 3.335e-01 1.083e-01 3.946e-01 5.258e-01\n",
      " 5.661e-01 6.994e-01 6.169e-01 5.380e-02 3.765e-01 7.497e-01 1.355e-01\n",
      " 4.531e-01 8.246e-01 8.686e-01 6.320e-01 5.852e-01 9.084e-01 8.162e-01\n",
      " 8.378e-01 9.183e-01 4.864e-01 4.626e-01 6.915e-01 7.157e-01 4.331e-01\n",
      " 8.263e-01 3.896e-02 7.410e-01 4.793e-01 2.023e-01 2.332e-01 4.594e-01\n",
      " 4.154e-01 9.578e-01 8.838e-01 2.004e-01 2.451e-01 2.929e-01 5.932e-01\n",
      " 3.375e-01 7.848e-01 6.284e-01 2.398e-01 8.468e-01 8.779e-01 8.339e-01\n",
      " 8.799e-01 8.069e-01 5.243e-01 4.057e-01 1.690e-01 8.663e-02 8.368e-01\n",
      " 1.325e-01 2.231e-01 9.994e-01 7.507e-02 3.633e-01 4.303e-01 9.245e-01\n",
      " 2.963e-02 2.014e-01 4.252e-01 5.232e-01 2.301e-02 5.705e-01 2.834e-01\n",
      " 4.111e-01 5.395e-01 2.814e-01 1.278e-01 7.443e-01 7.779e-01 8.359e-01\n",
      " 1.340e-01 6.036e-01 5.159e-01 3.320e-01 3.196e-01 6.202e-01 8.028e-01\n",
      " 4.590e-01 5.962e-01 4.801e-02 6.518e-01 4.370e-01 1.872e-01 8.613e-01\n",
      " 8.274e-01 3.458e-01 9.621e-01 6.295e-01 5.714e-01 8.326e-01 8.445e-01\n",
      " 5.724e-01 1.378e-01 1.438e-01 1.771e-01 7.489e-01 7.812e-01 2.890e-01\n",
      " 5.801e-01 2.601e-01 7.387e-01 3.567e-01 3.798e-02 4.956e-01 5.743e-01\n",
      " 6.324e-02 2.458e-01 7.498e-01 4.178e-01 9.119e-01 8.576e-01 6.338e-01\n",
      " 9.581e-01 2.574e-01 2.568e-01 9.097e-01 4.525e-01 8.839e-01 6.093e-01\n",
      " 9.225e-01 4.085e-01 7.474e-01 3.577e-01 7.799e-02 8.970e-01 4.696e-01\n",
      " 8.267e-01 2.706e-01 2.570e-02 8.754e-01 6.690e-01 5.407e-01 1.132e-01\n",
      " 4.692e-01 7.885e-01 3.062e-01 7.148e-01 5.027e-01 7.748e-01 8.213e-01\n",
      " 3.040e-01 9.347e-01 1.931e-01 4.840e-01 6.293e-01 3.894e-01 2.865e-01\n",
      " 8.187e-02 6.379e-01 9.338e-01 5.388e-01 3.731e-01 8.461e-02 3.798e-01\n",
      " 6.271e-01 2.242e-01 5.056e-01 3.820e-02 9.707e-02 4.219e-01 7.053e-01\n",
      " 1.240e-01 8.628e-01 2.413e-01 2.751e-01 8.632e-01 9.643e-01 4.435e-01\n",
      " 2.427e-01 9.110e-01 2.538e-02 4.311e-01 2.180e-01 9.475e-01 8.678e-01\n",
      " 5.535e-01 2.761e-01 3.188e-01 5.112e-01 6.535e-01 9.875e-01 5.504e-01\n",
      " 3.940e-01 1.210e-01 1.145e-01 6.105e-02 5.383e-01 9.493e-01 9.279e-01\n",
      " 5.795e-01 5.883e-01 5.294e-01 8.442e-01 5.508e-01 9.386e-01 3.559e-01\n",
      " 4.940e-01 9.204e-01 3.473e-01 1.755e-01 1.960e-01 3.820e-01 4.331e-01\n",
      " 9.597e-01 4.749e-01 9.690e-01 3.349e-01 8.392e-01 2.332e-01 6.540e-01\n",
      " 7.189e-02 9.927e-01 1.353e-01 6.376e-02 8.407e-01 5.587e-01 9.301e-01\n",
      " 5.019e-01 6.866e-01 8.697e-01 4.504e-01 7.414e-01 8.629e-01]\n",
      "Best Choice:  828 0.9994333359912076\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 1000\n",
    "num_ep  = 500\n",
    "num_iter= 1500\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T09:14:10.732565Z",
     "start_time": "2019-01-14T09:14:09.124059Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[2.963e-01 3.457e-01 3.940e-02 3.219e-01 9.101e-01 5.097e-01 6.107e-01\n",
      " 6.928e-01 4.142e-01 5.656e-01 2.767e-01 9.413e-02 5.395e-01 4.435e-01\n",
      " 6.662e-01 9.208e-02 1.530e-01 7.446e-02 5.772e-01 7.942e-02 9.051e-01\n",
      " 7.291e-01 9.097e-01 3.260e-01 1.912e-01 7.887e-01 4.240e-02 6.255e-01\n",
      " 2.291e-02 7.146e-01 1.360e-01 3.511e-01 7.320e-01 5.116e-01 4.448e-01\n",
      " 3.702e-01 6.859e-01 8.909e-01 5.329e-01 9.193e-02 8.489e-01 2.207e-01\n",
      " 2.050e-02 1.154e-01 8.374e-01 4.480e-01 2.933e-01 5.043e-01 3.156e-01\n",
      " 8.169e-01 4.842e-01 1.856e-01 9.017e-01 1.833e-01 7.597e-01 9.808e-01\n",
      " 7.148e-01 7.997e-02 5.469e-01 3.888e-01 6.136e-01 1.520e-01 1.349e-01\n",
      " 2.493e-01 3.017e-02 5.595e-01 1.263e-01 1.958e-01 8.585e-01 8.537e-01\n",
      " 7.748e-01 8.730e-01 5.480e-01 8.650e-01 4.029e-01 6.053e-01 3.376e-01\n",
      " 6.420e-01 3.129e-01 1.465e-01 4.518e-01 8.860e-01 5.715e-01 4.841e-01\n",
      " 7.920e-01 8.943e-01 2.852e-01 8.840e-01 6.107e-02 2.341e-01 6.369e-01\n",
      " 1.074e-01 5.746e-01 5.445e-02 8.488e-02 3.358e-01 2.169e-02 9.986e-01\n",
      " 5.361e-01 4.704e-01 5.451e-01 9.020e-01 3.656e-01 2.097e-01 2.589e-01\n",
      " 6.769e-01 9.010e-01 4.773e-01 8.704e-01 9.480e-01 5.742e-01 4.833e-02\n",
      " 9.665e-01 4.727e-01 6.751e-01 9.967e-01 6.914e-01 6.321e-01 5.686e-01\n",
      " 5.381e-01 3.089e-01 1.085e-01 6.605e-01 9.623e-01 8.525e-01 3.609e-01\n",
      " 6.048e-01 4.570e-01 7.889e-01 3.266e-01 7.398e-01 2.059e-01 4.612e-01\n",
      " 3.568e-01 6.624e-01 5.015e-01 8.172e-02 8.618e-01 6.501e-02 3.390e-01\n",
      " 6.979e-01 1.403e-01 5.724e-01 5.642e-01 7.521e-01 9.235e-01 8.854e-01\n",
      " 8.909e-02 3.882e-01 5.475e-01 6.446e-02 1.406e-01 2.869e-01 3.522e-01\n",
      " 2.688e-01 9.299e-01 3.654e-01 3.447e-01 2.979e-01 2.484e-01 4.413e-01\n",
      " 1.772e-01 1.027e-01 1.869e-01 1.059e-01 8.357e-02 4.996e-01 2.804e-01\n",
      " 1.460e-01 4.795e-01 4.816e-01 8.752e-03 3.746e-01 3.280e-02 2.134e-02\n",
      " 9.105e-01 1.684e-01 4.234e-02 8.948e-01 5.525e-02 4.698e-01 6.332e-02\n",
      " 1.526e-01 2.356e-01 9.736e-01 5.589e-01 8.604e-01 1.735e-01 6.778e-01\n",
      " 1.697e-02 3.206e-01 1.856e-01 5.431e-01 8.584e-01 1.459e-01 2.273e-01\n",
      " 5.137e-02 9.684e-02 4.577e-01 2.859e-01 2.609e-01 2.261e-01 9.071e-01\n",
      " 2.224e-01 9.128e-01 2.866e-01 9.211e-01 2.636e-02 5.991e-01 7.604e-01\n",
      " 8.545e-01 9.592e-01 7.419e-01 7.497e-01 9.743e-01 4.156e-01 9.913e-01\n",
      " 2.315e-01 6.228e-01 8.633e-01 5.826e-01 4.963e-02 7.568e-02 3.736e-01\n",
      " 5.580e-01 5.780e-01 8.907e-01 9.963e-01 4.373e-01 5.796e-01 8.228e-01\n",
      " 3.918e-01 2.587e-01 3.920e-01 1.823e-01 5.145e-01 7.134e-01 7.668e-01\n",
      " 9.606e-01 3.265e-01 1.324e-01 4.191e-01 2.681e-01 3.753e-01 6.177e-01\n",
      " 7.950e-01 5.246e-01 7.064e-01 1.599e-01 3.538e-01 6.610e-01 5.531e-01\n",
      " 5.927e-02 3.743e-01 6.029e-02 6.077e-01 7.910e-01 9.456e-03 2.712e-01\n",
      " 8.202e-01 2.700e-01 6.398e-01 3.210e-01 6.083e-02 1.585e-01 9.779e-01\n",
      " 9.596e-01 4.100e-01 3.910e-01 4.431e-01 1.999e-02 8.525e-01 8.987e-02\n",
      " 3.968e-01 7.653e-01 6.045e-01 2.524e-02 2.200e-01 1.829e-01 5.484e-01\n",
      " 9.245e-01 8.867e-01 4.657e-01 4.289e-01 4.432e-02 1.529e-01 6.248e-01\n",
      " 2.302e-02 3.446e-01 9.511e-01 8.881e-01 2.754e-01 7.790e-01 1.939e-01\n",
      " 3.284e-01 8.038e-02 7.964e-01 1.459e-01 4.676e-01 6.899e-01 9.580e-01\n",
      " 2.860e-01 8.658e-01 8.058e-01 7.729e-01 3.259e-01 5.799e-02 6.837e-01\n",
      " 2.373e-01 9.714e-01 2.923e-01 6.657e-01 4.624e-01 7.649e-01 1.466e-01\n",
      " 1.621e-01 1.688e-01 6.729e-01 5.435e-01 3.150e-01 9.118e-01 9.140e-01\n",
      " 6.234e-01 7.248e-01 3.895e-01 1.079e-02 4.918e-01 8.787e-01 1.936e-03\n",
      " 1.281e-01 9.705e-01 4.319e-01 3.975e-01 2.867e-01 5.189e-01 7.465e-01\n",
      " 9.344e-01 6.993e-02 5.320e-01 1.523e-01 9.521e-01 8.790e-01 2.659e-02\n",
      " 4.351e-01 3.571e-01 9.401e-02 1.377e-01 9.827e-02 8.691e-02 6.278e-01\n",
      " 8.647e-02 5.095e-01 9.642e-01 8.486e-01 7.473e-01 6.074e-01 1.896e-01\n",
      " 1.365e-01 2.842e-03 3.109e-01 1.192e-01 3.795e-01 3.668e-01 2.854e-01\n",
      " 3.452e-01 2.673e-01 2.933e-01 6.538e-01 1.396e-01 1.959e-01 1.342e-01\n",
      " 4.729e-01 9.142e-01 4.761e-01 3.592e-01 6.651e-01 3.454e-01 6.930e-01\n",
      " 5.749e-01 1.846e-01 9.246e-02 8.416e-01 7.133e-01 3.925e-01 7.497e-01\n",
      " 9.903e-01 6.178e-01 2.374e-01 5.281e-01 7.395e-01 4.255e-01 2.500e-01\n",
      " 9.374e-01 2.251e-01 8.334e-01 2.894e-01 2.126e-01 9.823e-01 5.825e-01\n",
      " 5.983e-01 9.824e-01 7.388e-01 8.850e-01 1.503e-02 9.387e-01 1.087e-01\n",
      " 2.686e-01 5.022e-01 2.729e-01 9.636e-01 4.910e-01 7.766e-01 1.192e-02\n",
      " 5.579e-01 9.920e-01 8.345e-01 9.015e-01 3.389e-01 6.070e-01 7.244e-01\n",
      " 9.342e-01 8.307e-01 9.638e-02 9.604e-01 6.616e-01 1.307e-01 9.897e-01\n",
      " 4.823e-01 6.415e-02 6.051e-01 5.816e-01 4.306e-01 1.517e-01 6.478e-01\n",
      " 2.432e-01 9.633e-01 1.090e-01 7.114e-01 2.658e-02 1.768e-01 6.354e-01\n",
      " 8.686e-04 3.810e-01 9.233e-01 1.142e-01 2.317e-01 8.112e-01 1.664e-01\n",
      " 7.637e-01 7.730e-01 9.462e-01 8.736e-02 3.047e-01 8.989e-01 9.223e-01\n",
      " 9.634e-01 2.640e-01 8.407e-01 8.535e-02 5.130e-01 3.576e-01 6.884e-01\n",
      " 1.577e-01 6.739e-01 9.351e-01 1.299e-01 2.174e-01 9.276e-01 1.420e-01\n",
      " 3.702e-01 1.864e-01 3.244e-01 9.900e-01 1.409e-01 3.073e-02 1.842e-01\n",
      " 6.282e-02 4.760e-01 2.725e-01 4.892e-01 9.533e-01 1.379e-01 3.438e-01\n",
      " 8.569e-01 2.764e-01 4.437e-01 9.919e-01 5.754e-01 7.821e-01 8.110e-01\n",
      " 8.207e-01 6.822e-02 8.038e-02 4.869e-01 3.147e-01 3.346e-01 8.909e-01\n",
      " 9.862e-01 3.797e-01 6.385e-01 9.082e-01 6.173e-01 5.253e-01 9.469e-03\n",
      " 5.216e-02 7.832e-01 5.119e-01 8.914e-01 8.649e-01 8.443e-01 1.368e-01\n",
      " 8.608e-01 7.176e-01 3.514e-01 2.925e-01 4.264e-01 1.152e-01 6.530e-01\n",
      " 1.727e-01 6.100e-01 5.846e-01 7.550e-01 4.472e-01 3.783e-01 5.566e-01\n",
      " 5.035e-01 3.751e-01 3.625e-01 2.221e-01 7.827e-01 3.876e-01 3.654e-01\n",
      " 9.430e-01 4.330e-01 5.518e-01 8.080e-01 3.522e-01 6.415e-01 2.588e-02\n",
      " 4.873e-01 8.269e-01 6.027e-01 7.839e-01 3.376e-01 6.923e-01 7.118e-01\n",
      " 3.074e-01 5.906e-01 6.306e-01 6.917e-01 5.127e-01 9.752e-01 1.071e-02\n",
      " 8.600e-01 1.637e-01 9.080e-01 9.611e-01 9.150e-01 3.285e-01 9.371e-01\n",
      " 4.450e-01 4.118e-01 7.795e-01 1.313e-01 1.343e-01 9.394e-01 7.982e-01\n",
      " 4.060e-01 5.635e-01 1.018e-01 4.738e-01 4.763e-01 6.173e-01 5.390e-01\n",
      " 5.577e-01 1.432e-01 1.782e-01 5.244e-01 7.026e-01 9.065e-01 9.446e-02\n",
      " 5.937e-01 5.153e-01 9.226e-01 4.770e-01 1.501e-01 8.875e-01 5.548e-01\n",
      " 2.562e-01 3.063e-01 7.289e-01 8.655e-01 6.458e-01 2.332e-01 9.076e-01\n",
      " 7.311e-01 2.344e-01 4.677e-01 2.308e-01 7.592e-01 5.593e-01 2.172e-02\n",
      " 7.874e-01 2.256e-01 4.709e-01 7.069e-01 8.906e-01 1.973e-01 9.588e-01\n",
      " 1.692e-01 9.284e-01 5.927e-01 6.665e-01 1.270e-01 8.420e-01 8.143e-01\n",
      " 2.422e-01 2.399e-01 2.502e-02 2.301e-01 2.004e-01 4.583e-01 5.152e-01\n",
      " 6.892e-01 1.177e-01 7.301e-01 1.119e-01 6.912e-01 2.647e-01 4.675e-01\n",
      " 2.948e-01 2.862e-01 3.316e-01 8.671e-01 4.415e-01 8.093e-01 3.851e-01\n",
      " 1.461e-03 5.905e-01 4.981e-01 1.949e-01 3.046e-01 7.830e-01 3.254e-01\n",
      " 9.908e-01 1.864e-01 7.888e-01 5.268e-02 1.635e-02 1.573e-01 2.032e-01\n",
      " 5.192e-01 9.517e-01 6.788e-01 5.594e-01 6.697e-01 2.423e-02 5.799e-01\n",
      " 6.988e-01 6.674e-01 4.606e-01 7.050e-01 2.127e-01 2.156e-01 3.118e-01\n",
      " 3.414e-01 5.825e-01 9.533e-01 5.025e-01 3.094e-01 5.974e-01 5.574e-01\n",
      " 6.585e-01 4.576e-01 5.978e-01 7.332e-01 3.218e-01 3.354e-01 8.752e-01\n",
      " 9.242e-01 2.176e-01 7.803e-01 1.876e-01 9.658e-01 5.676e-01 3.278e-01\n",
      " 1.924e-01 4.934e-01 1.425e-01 8.619e-01 3.641e-01 7.849e-01 3.407e-01\n",
      " 1.281e-01 5.420e-01 2.247e-01 5.125e-01 9.772e-01 6.712e-01 3.536e-01\n",
      " 9.081e-01 7.878e-01 3.961e-01 7.120e-01 2.340e-01 6.320e-01 1.163e-01\n",
      " 1.524e-01 1.764e-01 7.330e-01 3.571e-01 9.898e-01 7.230e-01 3.375e-01\n",
      " 7.692e-01 3.335e-01 2.062e-01 7.707e-02 2.408e-01 5.677e-01 1.466e-01\n",
      " 6.457e-01 2.882e-01 1.632e-01 9.854e-01 9.472e-01 4.053e-01 9.076e-01\n",
      " 2.706e-01 3.273e-01 6.533e-01 8.155e-01 9.746e-01 7.212e-01 9.409e-01\n",
      " 9.169e-01 6.506e-02 7.310e-01 3.038e-01 6.890e-01 2.572e-01 2.929e-01\n",
      " 8.518e-01 5.293e-01 3.725e-01 7.877e-01 7.022e-01 6.831e-01 5.604e-01\n",
      " 9.564e-01 8.450e-01 8.627e-01 3.090e-01 9.623e-01 7.775e-01 5.144e-01\n",
      " 6.171e-02 8.090e-01 1.173e-01 5.182e-01 9.149e-01 6.948e-01 9.440e-01\n",
      " 1.984e-01 8.766e-01 6.422e-01 6.434e-02 4.601e-01 7.304e-01 9.829e-01\n",
      " 2.125e-01 8.749e-01 4.157e-01 3.335e-01 1.083e-01 3.946e-01 5.258e-01\n",
      " 5.661e-01 6.994e-01 6.169e-01 5.380e-02 3.765e-01 7.497e-01 1.355e-01\n",
      " 4.531e-01 8.246e-01 8.686e-01 6.320e-01 5.852e-01 9.084e-01 8.162e-01\n",
      " 8.378e-01 9.183e-01 4.864e-01 4.626e-01 6.915e-01 7.157e-01 4.331e-01\n",
      " 8.263e-01 3.896e-02 7.410e-01 4.793e-01 2.023e-01 2.332e-01 4.594e-01\n",
      " 4.154e-01 9.578e-01 8.838e-01 2.004e-01 2.451e-01 2.929e-01 5.932e-01\n",
      " 3.375e-01 7.848e-01 6.284e-01 2.398e-01 8.468e-01 8.779e-01 8.339e-01\n",
      " 8.799e-01 8.069e-01 5.243e-01 4.057e-01 1.690e-01 8.663e-02 8.368e-01\n",
      " 1.325e-01 2.231e-01 9.994e-01 7.507e-02 3.633e-01 4.303e-01 9.245e-01\n",
      " 2.963e-02 2.014e-01 4.252e-01 5.232e-01 2.301e-02 5.705e-01 2.834e-01\n",
      " 4.111e-01 5.395e-01 2.814e-01 1.278e-01 7.443e-01 7.779e-01 8.359e-01\n",
      " 1.340e-01 6.036e-01 5.159e-01 3.320e-01 3.196e-01 6.202e-01 8.028e-01\n",
      " 4.590e-01 5.962e-01 4.801e-02 6.518e-01 4.370e-01 1.872e-01 8.613e-01\n",
      " 8.274e-01 3.458e-01 9.621e-01 6.295e-01 5.714e-01 8.326e-01 8.445e-01\n",
      " 5.724e-01 1.378e-01 1.438e-01 1.771e-01 7.489e-01 7.812e-01 2.890e-01\n",
      " 5.801e-01 2.601e-01 7.387e-01 3.567e-01 3.798e-02 4.956e-01 5.743e-01\n",
      " 6.324e-02 2.458e-01 7.498e-01 4.178e-01 9.119e-01 8.576e-01 6.338e-01\n",
      " 9.581e-01 2.574e-01 2.568e-01 9.097e-01 4.525e-01 8.839e-01 6.093e-01\n",
      " 9.225e-01 4.085e-01 7.474e-01 3.577e-01 7.799e-02 8.970e-01 4.696e-01\n",
      " 8.267e-01 2.706e-01 2.570e-02 8.754e-01 6.690e-01 5.407e-01 1.132e-01\n",
      " 4.692e-01 7.885e-01 3.062e-01 7.148e-01 5.027e-01 7.748e-01 8.213e-01\n",
      " 3.040e-01 9.347e-01 1.931e-01 4.840e-01 6.293e-01 3.894e-01 2.865e-01\n",
      " 8.187e-02 6.379e-01 9.338e-01 5.388e-01 3.731e-01 8.461e-02 3.798e-01\n",
      " 6.271e-01 2.242e-01 5.056e-01 3.820e-02 9.707e-02 4.219e-01 7.053e-01\n",
      " 1.240e-01 8.628e-01 2.413e-01 2.751e-01 8.632e-01 9.643e-01 4.435e-01\n",
      " 2.427e-01 9.110e-01 2.538e-02 4.311e-01 2.180e-01 9.475e-01 8.678e-01\n",
      " 5.535e-01 2.761e-01 3.188e-01 5.112e-01 6.535e-01 9.875e-01 5.504e-01\n",
      " 3.940e-01 1.210e-01 1.145e-01 6.105e-02 5.383e-01 9.493e-01 9.279e-01\n",
      " 5.795e-01 5.883e-01 5.294e-01 8.442e-01 5.508e-01 9.386e-01 3.559e-01\n",
      " 4.940e-01 9.204e-01 3.473e-01 1.755e-01 1.960e-01 3.820e-01 4.331e-01\n",
      " 9.597e-01 4.749e-01 9.690e-01 3.349e-01 8.392e-01 2.332e-01 6.540e-01\n",
      " 7.189e-02 9.927e-01 1.353e-01 6.376e-02 8.407e-01 5.587e-01 9.301e-01\n",
      " 5.019e-01 6.866e-01 8.697e-01 4.504e-01 7.414e-01 8.629e-01]\n",
      "Expected \n",
      "[0.297 0.35  0.039 0.323 0.91  0.506 0.611 0.693 0.416 0.564 0.276 0.096\n",
      " 0.538 0.445 0.667 0.092 0.152 0.075 0.578 0.081 0.907 0.73  0.908 0.327\n",
      " 0.192 0.789 0.042 0.624 0.023 0.716 0.134 0.351 0.731 0.509 0.445 0.371\n",
      " 0.688 0.89  0.534 0.091 0.849 0.22  0.02  0.116 0.837 0.449 0.291 0.503\n",
      " 0.312 0.816 0.484 0.186 0.9   0.181 0.76  0.982 0.717 0.08  0.544 0.388\n",
      " 0.616 0.151 0.132 0.25  0.031 0.557 0.13  0.197 0.859 0.854 0.775 0.874\n",
      " 0.55  0.866 0.404 0.604 0.34  0.643 0.312 0.15  0.449 0.886 0.573 0.484\n",
      " 0.793 0.894 0.286 0.884 0.06  0.235 0.636 0.109 0.571 0.054 0.085 0.336\n",
      " 0.022 0.999 0.537 0.47  0.544 0.902 0.365 0.211 0.261 0.676 0.9   0.477\n",
      " 0.872 0.947 0.575 0.05  0.966 0.472 0.675 0.997 0.693 0.631 0.569 0.539\n",
      " 0.309 0.108 0.661 0.961 0.851 0.359 0.604 0.458 0.79  0.324 0.742 0.204\n",
      " 0.464 0.356 0.66  0.502 0.082 0.86  0.065 0.339 0.698 0.14  0.575 0.566\n",
      " 0.75  0.924 0.886 0.091 0.386 0.547 0.065 0.144 0.286 0.351 0.269 0.931\n",
      " 0.366 0.345 0.297 0.249 0.441 0.18  0.102 0.187 0.105 0.083 0.499 0.28\n",
      " 0.144 0.48  0.483 0.009 0.375 0.032 0.021 0.911 0.169 0.041 0.894 0.055\n",
      " 0.469 0.064 0.153 0.234 0.975 0.56  0.86  0.173 0.679 0.017 0.321 0.187\n",
      " 0.541 0.858 0.145 0.226 0.05  0.096 0.458 0.287 0.26  0.226 0.908 0.219\n",
      " 0.911 0.29  0.922 0.027 0.598 0.759 0.856 0.959 0.741 0.748 0.974 0.414\n",
      " 0.991 0.233 0.624 0.863 0.581 0.049 0.075 0.375 0.555 0.577 0.89  0.996\n",
      " 0.441 0.58  0.822 0.393 0.26  0.39  0.181 0.517 0.716 0.765 0.96  0.325\n",
      " 0.134 0.419 0.267 0.375 0.616 0.795 0.523 0.709 0.159 0.353 0.662 0.553\n",
      " 0.059 0.376 0.059 0.608 0.793 0.009 0.271 0.821 0.267 0.636 0.32  0.061\n",
      " 0.158 0.978 0.959 0.409 0.39  0.443 0.019 0.852 0.092 0.394 0.766 0.606\n",
      " 0.026 0.218 0.184 0.549 0.924 0.886 0.465 0.429 0.045 0.152 0.623 0.023\n",
      " 0.344 0.953 0.888 0.274 0.78  0.193 0.327 0.079 0.797 0.148 0.468 0.689\n",
      " 0.958 0.285 0.866 0.805 0.771 0.326 0.058 0.687 0.237 0.971 0.292 0.668\n",
      " 0.462 0.764 0.149 0.16  0.17  0.672 0.541 0.315 0.91  0.915 0.62  0.725\n",
      " 0.389 0.011 0.493 0.878 0.002 0.128 0.97  0.431 0.399 0.286 0.52  0.747\n",
      " 0.935 0.07  0.536 0.152 0.951 0.881 0.026 0.436 0.356 0.094 0.14  0.097\n",
      " 0.087 0.627 0.088 0.511 0.963 0.849 0.747 0.606 0.192 0.135 0.003 0.31\n",
      " 0.122 0.38  0.366 0.284 0.344 0.265 0.295 0.652 0.14  0.194 0.134 0.471\n",
      " 0.916 0.475 0.355 0.668 0.346 0.694 0.574 0.186 0.091 0.842 0.715 0.395\n",
      " 0.75  0.99  0.616 0.238 0.525 0.738 0.426 0.248 0.939 0.224 0.834 0.29\n",
      " 0.214 0.982 0.583 0.594 0.982 0.738 0.885 0.015 0.937 0.109 0.268 0.5\n",
      " 0.271 0.963 0.492 0.775 0.012 0.553 0.992 0.835 0.903 0.337 0.606 0.724\n",
      " 0.933 0.828 0.096 0.959 0.661 0.129 0.99  0.481 0.064 0.607 0.579 0.43\n",
      " 0.151 0.649 0.242 0.963 0.109 0.713 0.027 0.179 0.634 0.001 0.38  0.921\n",
      " 0.113 0.233 0.812 0.167 0.764 0.773 0.945 0.086 0.305 0.899 0.922 0.961\n",
      " 0.263 0.842 0.085 0.513 0.357 0.688 0.158 0.673 0.936 0.13  0.219 0.928\n",
      " 0.142 0.37  0.187 0.323 0.99  0.142 0.031 0.185 0.063 0.474 0.271 0.487\n",
      " 0.953 0.138 0.342 0.857 0.278 0.445 0.992 0.574 0.781 0.811 0.82  0.067\n",
      " 0.081 0.486 0.316 0.337 0.891 0.987 0.379 0.637 0.91  0.617 0.526 0.01\n",
      " 0.051 0.786 0.511 0.893 0.864 0.841 0.136 0.858 0.719 0.351 0.293 0.429\n",
      " 0.113 0.651 0.175 0.612 0.585 0.753 0.448 0.38  0.558 0.501 0.377 0.361\n",
      " 0.223 0.785 0.387 0.365 0.945 0.433 0.552 0.809 0.351 0.641 0.025 0.49\n",
      " 0.829 0.601 0.784 0.337 0.692 0.711 0.308 0.586 0.63  0.693 0.516 0.975\n",
      " 0.011 0.861 0.163 0.91  0.961 0.915 0.329 0.937 0.45  0.412 0.779 0.133\n",
      " 0.135 0.939 0.798 0.405 0.563 0.102 0.471 0.477 0.617 0.538 0.556 0.143\n",
      " 0.18  0.525 0.701 0.907 0.094 0.593 0.514 0.924 0.475 0.15  0.887 0.555\n",
      " 0.254 0.306 0.728 0.866 0.647 0.231 0.909 0.733 0.236 0.469 0.23  0.763\n",
      " 0.557 0.022 0.788 0.227 0.468 0.706 0.89  0.196 0.959 0.174 0.93  0.593\n",
      " 0.665 0.127 0.842 0.813 0.242 0.24  0.026 0.228 0.202 0.456 0.515 0.689\n",
      " 0.117 0.73  0.111 0.691 0.265 0.467 0.292 0.286 0.337 0.866 0.442 0.809\n",
      " 0.389 0.002 0.591 0.501 0.196 0.305 0.783 0.324 0.991 0.185 0.79  0.053\n",
      " 0.016 0.159 0.204 0.518 0.951 0.677 0.559 0.67  0.025 0.577 0.699 0.667\n",
      " 0.462 0.704 0.212 0.216 0.309 0.34  0.583 0.953 0.504 0.31  0.597 0.555\n",
      " 0.659 0.456 0.598 0.733 0.322 0.337 0.873 0.925 0.218 0.778 0.187 0.965\n",
      " 0.567 0.327 0.191 0.496 0.144 0.862 0.365 0.785 0.344 0.129 0.541 0.227\n",
      " 0.512 0.977 0.675 0.352 0.909 0.786 0.393 0.708 0.236 0.633 0.115 0.151\n",
      " 0.178 0.731 0.358 0.99  0.721 0.339 0.771 0.334 0.208 0.077 0.243 0.568\n",
      " 0.146 0.648 0.288 0.163 0.985 0.946 0.401 0.907 0.274 0.324 0.655 0.817\n",
      " 0.973 0.72  0.942 0.918 0.064 0.731 0.304 0.685 0.256 0.29  0.851 0.53\n",
      " 0.374 0.788 0.702 0.683 0.559 0.957 0.846 0.862 0.312 0.962 0.778 0.514\n",
      " 0.061 0.809 0.116 0.517 0.916 0.696 0.945 0.201 0.877 0.643 0.065 0.464\n",
      " 0.73  0.982 0.21  0.874 0.413 0.332 0.108 0.396 0.527 0.571 0.699 0.616\n",
      " 0.054 0.379 0.751 0.136 0.455 0.824 0.87  0.631 0.587 0.908 0.815 0.839\n",
      " 0.918 0.486 0.465 0.691 0.716 0.433 0.828 0.038 0.739 0.48  0.202 0.232\n",
      " 0.459 0.415 0.957 0.881 0.198 0.243 0.292 0.593 0.338 0.784 0.628 0.24\n",
      " 0.846 0.877 0.834 0.879 0.805 0.526 0.406 0.167 0.088 0.838 0.131 0.221\n",
      " 1.    0.075 0.364 0.432 0.922 0.029 0.2   0.423 0.52  0.022 0.571 0.284\n",
      " 0.413 0.54  0.281 0.131 0.744 0.78  0.835 0.135 0.601 0.513 0.331 0.319\n",
      " 0.621 0.804 0.458 0.596 0.049 0.654 0.44  0.184 0.86  0.829 0.346 0.962\n",
      " 0.63  0.572 0.836 0.841 0.572 0.139 0.142 0.177 0.749 0.782 0.291 0.58\n",
      " 0.262 0.741 0.36  0.037 0.495 0.576 0.063 0.246 0.75  0.419 0.913 0.857\n",
      " 0.633 0.957 0.256 0.256 0.91  0.455 0.886 0.609 0.922 0.408 0.747 0.359\n",
      " 0.079 0.897 0.469 0.824 0.272 0.025 0.874 0.666 0.543 0.114 0.467 0.787\n",
      " 0.303 0.716 0.503 0.776 0.821 0.304 0.934 0.192 0.482 0.63  0.391 0.286\n",
      " 0.083 0.636 0.934 0.541 0.37  0.084 0.378 0.629 0.226 0.501 0.038 0.097\n",
      " 0.423 0.705 0.125 0.862 0.241 0.276 0.863 0.964 0.442 0.242 0.909 0.026\n",
      " 0.433 0.219 0.948 0.867 0.555 0.275 0.316 0.51  0.656 0.988 0.551 0.395\n",
      " 0.119 0.115 0.063 0.542 0.949 0.928 0.578 0.589 0.531 0.843 0.551 0.938\n",
      " 0.355 0.499 0.92  0.348 0.176 0.195 0.381 0.433 0.96  0.477 0.97  0.333\n",
      " 0.838 0.231 0.656 0.074 0.993 0.135 0.062 0.841 0.561 0.93  0.502 0.69\n",
      " 0.869 0.448 0.743 0.86 ]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1,num_bandit) < gt_prob\n",
    "        temp_expect    = temp_expect + current_reward\n",
    "\n",
    "    a_expect[eps,:] = temp_expect/temp_choice\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:08.777Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "b_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    b_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:08.980Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "c_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "  \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    c_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:09.182Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "d_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.99 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    d_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:09.376Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "e_regret_total = np.zeros((num_ep,num_iter))\n",
    "      \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.1\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "            \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    e_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(np.around(e_estimation.mean(0),3))\n",
    "print('Expected Normalized')\n",
    "print(\n",
    "    (gt_prob.max()-gt_prob.min())*(e_estimation.mean(0)-e_estimation.mean(0).min())/(e_estimation.mean(0).max()-e_estimation.mean(0).min()) + gt_prob.min()\n",
    ")\n",
    "e_estimation = (gt_prob.max()-gt_prob.min())*(e_estimation.mean(0)-e_estimation.mean(0).min())/(e_estimation.mean(0).max()-e_estimation.mean(0).min()) + gt_prob.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:09.583Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "f_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.01\n",
    "    beta  = 0.001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    f_regret_total[eps,:] = temp_regret\n",
    "    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(\n",
    "    (gt_prob.max()-gt_prob.min())*(f_estimation.mean(0)-f_estimation.mean(0).min())/(f_estimation.mean(0).max()-f_estimation.mean(0).min()) + gt_prob.min()\n",
    ")\n",
    "f_estimation = (gt_prob.max()-gt_prob.min())*(e_estimation.mean(0)-e_estimation.mean(0).min())/(e_estimation.mean(0).max()-e_estimation.mean(0).min()) + gt_prob.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:09.785Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# g UBC 1\n",
    "g_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "g_estimation   = np.zeros((num_ep,num_bandit))\n",
    "g_reward       = np.zeros((num_ep,num_iter))\n",
    "g_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "g_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(0.5*np.log(iter+1)/(temp_pull_count+1)))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    g_pull_count[eps,:]   = temp_pull_count\n",
    "    g_estimation[eps,:]   = temp_estimation\n",
    "    g_reward[eps,:]       = temp_reward\n",
    "    g_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    g_regret_total[eps,:] = temp_regret\n",
    "  \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(g_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:09.988Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# h UBC 1 Tuned\n",
    "h_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "h_estimation   = np.zeros((num_ep,num_bandit))\n",
    "h_reward       = np.zeros((num_ep,num_iter))\n",
    "h_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "h_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit) \n",
    "    temp_estimation   = np.zeros(num_bandit) \n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_sumof_squares= np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        v_temp = temp_sumof_squares + np.sqrt(2*np.log(iter+1)/(temp_pull_count+1))\n",
    "        current_min_value = np.minimum(v_temp,np.ones_like(v_temp)*0.25)\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(np.log(iter+1)/(temp_pull_count+1)*current_min_value))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        temp_sumof_squares[current_choice] = temp_sumof_squares[current_choice] + current_reward ** 2\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    h_pull_count[eps,:]   = temp_pull_count\n",
    "    h_estimation[eps,:]   = temp_estimation\n",
    "    h_reward[eps,:]       = temp_reward\n",
    "    h_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    h_regret_total[eps,:] = temp_regret\n",
    "    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(h_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:10.191Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # i Thompson Sampling (beta) (slow)\n",
    "# i_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "# i_estimation   = np.zeros((num_ep,num_bandit))\n",
    "# i_reward       = np.zeros((num_ep,num_iter))\n",
    "# i_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "# i_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "# for eps in range(num_ep):\n",
    "\n",
    "#     temp_pull_count   = np.zeros(num_bandit)\n",
    "#     temp_estimation   = np.zeros(num_bandit)\n",
    "#     temp_reward       = np.zeros(num_iter)\n",
    "#     temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "#     for iter in range(num_iter):\n",
    "        \n",
    "#         theta_samples = [stats.beta(a=1+w,b=1+t-w).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "#         # select bandit / get reward /increase count / update estimate\n",
    "#         current_choice = np.argmax(theta_samples)\n",
    "#         current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "#         temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "#         temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "#         # update reward and optimal choice\n",
    "#         temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "#         temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "#         temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "#     i_pull_count[eps,:]   = temp_pull_count\n",
    "#     i_estimation[eps,:]   = theta_samples\n",
    "#     i_reward[eps,:]       = temp_reward\n",
    "#     i_optimal_pull[eps,:] = temp_optimal_pull\n",
    "#     i_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "# print('Ground Truth')\n",
    "# print(gt_prob)\n",
    "# print('Expected ')\n",
    "# print(i_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:10.419Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # j Thompson Sampling (uniform) (slow)\n",
    "# j_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "# j_estimation   = np.zeros((num_ep,num_bandit))\n",
    "# j_reward       = np.zeros((num_ep,num_iter))\n",
    "# j_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "# j_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "# for eps in range(num_ep):\n",
    "\n",
    "#     temp_pull_count   = np.zeros(num_bandit)\n",
    "#     temp_estimation   = np.zeros(num_bandit)\n",
    "#     temp_reward       = np.zeros(num_iter)\n",
    "#     temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "#     for iter in range(num_iter):\n",
    "        \n",
    "#         theta_samples = [stats.uniform(w/(t+0.000000001),1-w/(t+0.000000001)).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "#         # select bandit / get reward /increase count / update estimate\n",
    "#         current_choice = np.argmax(theta_samples)\n",
    "#         current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "#         temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "#         temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "#         # update reward and optimal choice\n",
    "#         temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "#         temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "#         temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "#     j_pull_count[eps,:]   = temp_pull_count\n",
    "#     j_estimation[eps,:]   = theta_samples\n",
    "#     j_reward[eps,:]       = temp_reward\n",
    "#     j_optimal_pull[eps,:] = temp_optimal_pull\n",
    "#     j_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "# print('Ground Truth')\n",
    "# print(gt_prob)\n",
    "# print('Expected ')\n",
    "# print(j_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:10.660Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# k neural network (with adam)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "k_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def d_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    weights = np.random.randn(num_bandit,1)\n",
    "    moment  = np.zeros_like(weights); \n",
    "    velocity = np.zeros_like(weights);\n",
    "    epsilon  = 0.3\n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        if np.random.uniform(0,1)>epsilon:\n",
    "            current_choice = np.argmax(weights)\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "        else:\n",
    "            current_choice = np.random.choice(np.arange(num_bandit))\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "\n",
    "        layer1 = current_input @ weights\n",
    "        layer1a= sigmoid(layer1)\n",
    "\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        # KL Divergence https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "        grad3 = np.log(layer1a+0.0000001) - np.log(temp_estimation[current_choice]/(temp_pull_count[current_choice])+0.0000001)\n",
    "        grad2 = d_sigmoid(layer1)\n",
    "        grad1 = current_input\n",
    "        grad  = grad1.T @ (grad3 * grad2)\n",
    "        \n",
    "        moment   = 0.9*moment + (1-0.9) * grad\n",
    "        velocity = 0.999*velocity + (1-0.999) * grad**2\n",
    "        moment_hat   = moment/(1-0.9)\n",
    "        velocity_hat = velocity/(1-0.999)\n",
    "        weights  = weights - 0.08 * (moment_hat/(np.sqrt(velocity_hat)+1e-8))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # Decay the learning rate\n",
    "        epsilon = epsilon * 0.999\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = np.squeeze(sigmoid(weights))\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    k_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:10.900Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# l softmax\n",
    "l_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "l_estimation   = np.zeros((num_ep,num_bandit))\n",
    "l_reward       = np.zeros((num_ep,num_iter))\n",
    "l_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "l_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    tempture = 30\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation/tempture) / np.sum(np.exp(temp_estimation/tempture))\n",
    "        current_choice = np.random.choice(num_bandit,p=pi)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # decay the temp\n",
    "        tempture = tempture * 0.99\n",
    "        \n",
    "    l_pull_count[eps,:]   = temp_pull_count\n",
    "    l_estimation[eps,:]   = temp_estimation\n",
    "    l_reward[eps,:]       = temp_reward\n",
    "    l_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    l_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(l_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:11.174Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# m gradient base\n",
    "m_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "m_estimation   = np.zeros((num_ep,num_bandit))\n",
    "m_reward       = np.zeros((num_ep,num_iter))\n",
    "m_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "m_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    temp_mean_reward = 0\n",
    "    alpha = 0.8\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation) / np.sum(np.exp(temp_estimation))\n",
    "        current_choice = np.random.choice(num_bandit,p=pi)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        temp_mean_reward = temp_mean_reward + ((current_reward-temp_mean_reward))/(iter) if not iter==0 else ((current_reward-temp_mean_reward))\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1\n",
    "        \n",
    "        temp_estimation = (mask)   * (temp_estimation+alpha*(current_reward-temp_mean_reward)*(1-pi)) + \\\n",
    "                          (1-mask) * (temp_estimation-alpha*(current_reward-temp_mean_reward)*(pi))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    m_pull_count[eps,:]   = temp_pull_count\n",
    "    m_estimation[eps,:]   = temp_estimation\n",
    "    m_reward[eps,:]       = temp_reward\n",
    "    m_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    m_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(np.around(m_estimation.mean(0),2))\n",
    "print('Expected Normalized')\n",
    "print(\n",
    "    (gt_prob.max()-gt_prob.min())*(m_estimation.mean(0)-m_estimation.mean(0).min())/(m_estimation.mean(0).max()-m_estimation.mean(0).min()) + gt_prob.min()\n",
    ")\n",
    "m_estimation = (gt_prob.max()-gt_prob.min())*(m_estimation.mean(0)-m_estimation.mean(0).min())/(m_estimation.mean(0).max()-m_estimation.mean(0).min()) + gt_prob.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:11.451Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the regret\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(b_regret_total.mean(0),c='red',    label='greedy')\n",
    "plt.plot(c_regret_total.mean(0),c='blue',   label='e greedy')\n",
    "plt.plot(d_regret_total.mean(0),c='green',  label='decay e greedy')\n",
    "plt.plot(e_regret_total.mean(0),c='yellow', label='Linear Reward Inaction')\n",
    "plt.plot(f_regret_total.mean(0),c='purple', label='Linear Reward Penalty')\n",
    "plt.plot(g_regret_total.mean(0),c='black',  label='UBC')\n",
    "plt.plot(h_regret_total.mean(0),c='gold',   label='UBC tuned')\n",
    "# plt.plot(i_regret_total.mean(0),c='pink',   label='beta')\n",
    "# plt.plot(j_regret_total.mean(0),c='grey',   label='uniform')\n",
    "plt.plot(k_regret_total.mean(0),c='skyblue',label='NN')\n",
    "plt.plot(l_regret_total.mean(0),c='cyan',   label='softmax')\n",
    "plt.plot(m_regret_total.mean(0),c='magenta',label='Grad')\n",
    "plt.legend(prop={'size': 30})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:11.876Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the reward\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(b_reward.mean(0),c='red',    label='greedy')\n",
    "plt.plot(c_reward.mean(0),c='blue',   label='e greedy')\n",
    "plt.plot(d_reward.mean(0),c='green',  label='decay e greedy')\n",
    "plt.plot(e_reward.mean(0),c='yellow', label='Linear Reward Inaction')\n",
    "plt.plot(f_reward.mean(0),c='purple', label='Linear Reward Penalty')\n",
    "plt.plot(g_reward.mean(0),c='black',  label='UBC')\n",
    "plt.plot(h_reward.mean(0),c='gold',   label='UBC tuned')\n",
    "# plt.plot(i_reward.mean(0),c='pink',   label='beta')\n",
    "# plt.plot(j_reward.mean(0),c='grey',   label='uniform')\n",
    "plt.plot(k_reward.mean(0),c='skyblue',label='NN')\n",
    "plt.plot(l_reward.mean(0),c='cyan',   label='softmax')\n",
    "plt.plot(m_reward.mean(0),c='magenta',label='Grad')\n",
    "plt.legend(prop={'size': 30})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:12.873Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the reward\n",
    "print(gt_prob)\n",
    "print(b_estimation.mean(0))\n",
    "print(c_estimation.mean(0))\n",
    "print(d_estimation.mean(0))\n",
    "print(e_estimation)\n",
    "print(f_estimation)\n",
    "print(g_estimation.mean(0))\n",
    "print(h_estimation.mean(0))\n",
    "# print(i_estimation.mean(0))\n",
    "# print(j_estimation.mean(0))\n",
    "print(k_estimation.mean(0))\n",
    "print(l_estimation.mean(0))\n",
    "print(m_estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T09:14:13.220Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# correaltion\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "data = pd.DataFrame({\n",
    "        'GT':gt_prob,\n",
    "        'Vector': a_expect.mean(0),\n",
    "        'greedy':b_estimation.mean(0),\n",
    "        'e-greedy':c_estimation.mean(0),\n",
    "        'decay e-greedy':d_estimation.mean(0),\n",
    "        'Linear Reward Inaction':e_estimation,\n",
    "        'Linear Reward Penalty':f_estimation,\n",
    "        'UCB1':g_estimation.mean(0),\n",
    "        'UCB1 Tuned':h_estimation.mean(0),\n",
    "        #'beta':i_estimation.mean(0),\n",
    "        #'uniform':j_estimation.mean(0),\n",
    "        'NN':k_estimation.mean(0),\n",
    "        'softmax':l_estimation.mean(0),\n",
    "        'grad':m_estimation\n",
    "    })\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(data.corr(),annot=True,linewidths=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions  NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "2. [ Archived Post ] Random Note about Multi-Arm Bandit Problem 2. (2019). Medium. Retrieved 13 January 2019, from https://medium.com/@SeoJaeDuk/archived-post-random-note-about-multi-arm-bandit-problem-2-5c522d1dfbdc\n",
    "3. Vieira, T. (2014). KL-divergence as an objective function  Graduate Descent. Timvieira.github.io. Retrieved 13 January 2019, from https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "4. Some Reinforcement Learning: The Greedy and Explore-Exploit Algorithms for the Multi-Armed Bandit Framework in Python. (2019). Datasciencecentral.com. Retrieved 13 January 2019, from https://www.datasciencecentral.com/profiles/blogs/some-reinforcement-learning-the-greedy-and-explore-exploit\n",
    "5. (2019). Cs.mcgill.ca. Retrieved 13 January 2019, from https://www.cs.mcgill.ca/~vkules/bandits.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
