{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:04:15.917453Z",
     "start_time": "2019-01-09T23:04:15.913470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12798052 0.05257987 0.04168536 0.1013075  0.13220688 0.07774843\n",
      " 0.18022149 0.1258417  0.08837421 0.07205402]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "\n",
    "expected_action_value  = np.random.uniform(0 ,1 , 10)\n",
    "expected_action_value = expected_action_value/expected_action_value.sum()\n",
    "num_epoch = 10000\n",
    "alpha = 0.2\n",
    "beta  = 0.8\n",
    "print(expected_action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:04:16.527038Z",
     "start_time": "2019-01-09T23:04:16.264140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.128 0.053 0.042 0.101 0.132 0.078 0.18  0.126 0.088 0.072]\n",
      "[0.111 0.11  0.111 0.111 0.095 0.107 0.111 0.111 0.111 0.022]\n",
      "[1026.  961.  947.  994. 1030.  978. 1092. 1026.  971.  975.]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Linear Reward Inaction \n",
    "estimated = np.zeros(10)+0.1\n",
    "pull_count= np.zeros(10)\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    current_choice = np.argmax(estimated) if np.random.uniform(0,1) > 0.3 else np.random.choice(10, p=estimated)\n",
    "    pull_count[current_choice] = pull_count[current_choice] + 1\n",
    "    current_reward = 1 if expected_action_value[current_choice]>np.random.uniform(0,1) else 0\n",
    "    \n",
    "    mask = np.zeros(10) \n",
    "    mask[current_choice] = 1\n",
    "\n",
    "    if current_reward == 1:\n",
    "        estimated = (mask) * (estimated + alpha*(1-estimated)) + (1-mask) * ((1-alpha) *estimated)\n",
    "    else:\n",
    "        estimated = (mask) * ((1-beta)*estimated) + (1-mask) * (beta/9.0 + (1-beta)*estimated)\n",
    "\n",
    "print(np.around(expected_action_value,3))\n",
    "print(np.around(estimated,3))\n",
    "print(np.around(pull_count,3))\n",
    "print(estimated.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:05:04.946689Z",
     "start_time": "2019-01-09T23:05:04.514141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.128 0.053 0.042 0.101 0.132 0.078 0.18  0.126 0.088 0.072]\n",
      "[0.1   0.1   0.099 0.1   0.1   0.1   0.102 0.1   0.1   0.1  ]\n",
      "[ 879.  949.  914.  970.  953.  916. 1732.  933.  879.  875.]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Linear Reward Inaction \n",
    "alpha = 0.00001\n",
    "estimated = np.zeros(10)+0.1\n",
    "pull_count= np.zeros(10)\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    current_choice = np.argmax(estimated) if np.random.uniform(0,1) > 0.9 else np.random.choice(10, p=estimated)\n",
    "    pull_count[current_choice] = pull_count[current_choice] + 1\n",
    "    current_reward = 1 if expected_action_value[current_choice]>np.random.uniform(0,1) else 0\n",
    "    \n",
    "    mask = np.zeros(10) \n",
    "    mask[current_choice] = 1\n",
    "    \n",
    "    if current_reward == 1:\n",
    "        estimated = (mask) * (estimated + alpha*(1-estimated)) + (1-mask) * ((1-alpha) *estimated)\n",
    "        \n",
    "    estimated = estimated/estimated.sum()\n",
    "        \n",
    "print(np.around(expected_action_value,3))\n",
    "print(np.around(estimated,3))\n",
    "print(np.around(pull_count,3))\n",
    "print(estimated.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:01:38.519267Z",
     "start_time": "2019-01-09T23:01:38.513811Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:58:01.752170Z",
     "start_time": "2019-01-09T23:57:56.023353Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting LRI - Linear Reward Inaction Algorithm\n",
      "\n",
      "\n",
      "Run: 1, Optimal Decision: q3 with chance: 0.970689 chosen 8481 times. Average reward: 0.95\n",
      "[0.67  0.462 0.971 0.291 0.299 0.074 0.764 0.412 0.699 0.934]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[50, 32, 8481, 19, 33, 13, 92, 43, 88, 1149]\n",
      "Run: 2, Optimal Decision: q5 with chance: 0.990591 chosen 9455 times. Average reward: 0.97\n",
      "[0.095 0.238 0.148 0.784 0.991 0.47  0.342 0.66  0.39  0.096]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[23, 13, 20, 227, 9455, 22, 54, 111, 61, 14]\n",
      "Run: 3, Optimal Decision: q1 with chance: 0.961163 chosen 2871 times. Average reward: 0.94\n",
      "[0.961 0.262 0.46  0.659 0.304 0.951 0.832 0.268 0.279 0.463]\n",
      "[0.61 0.   0.   0.   0.   0.39 0.   0.   0.   0.  ]\n",
      "[2871, 24, 48, 20, 18, 6763, 178, 23, 23, 32]\n",
      "Run: 4, Optimal Decision: q3 with chance: 0.951237 chosen 5558 times. Average reward: 0.91\n",
      "[0.244 0.418 0.951 0.902 0.589 0.837 0.837 0.676 0.315 0.277]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[25, 23, 5558, 3651, 111, 114, 419, 45, 36, 18]\n",
      "Run: 5, Optimal Decision: q6 with chance: 0.942106 chosen 4388 times. Average reward: 0.92\n",
      "[0.56  0.863 0.186 0.195 0.774 0.942 0.355 0.921 0.917 0.295]\n",
      "[0.    0.    0.    0.    0.    0.725 0.    0.    0.275 0.   ]\n",
      "[50, 410, 16, 10, 160, 4388, 22, 11, 4915, 18]\n",
      "Run: 6, Optimal Decision: q2 with chance: 0.992821 chosen 7139 times. Average reward: 0.96\n",
      "[0.108 0.993 0.919 0.28  0.205 0.866 0.435 0.451 0.846 0.919]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[24, 7139, 1782, 24, 23, 259, 12, 29, 97, 611]\n",
      "Run: 7, Optimal Decision: q8 with chance: 0.877937 chosen 9221 times. Average reward: 0.86\n",
      "[0.26  0.338 0.042 0.346 0.706 0.628 0.508 0.878 0.506 0.512]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[33, 86, 25, 49, 230, 172, 93, 9221, 31, 60]\n",
      "Run: 8, Optimal Decision: q8 with chance: 0.889393 chosen 4605 times. Average reward: 0.88\n",
      "[0.887 0.059 0.793 0.721 0.874 0.56  0.05  0.889 0.523 0.643]\n",
      "[0.383 0.    0.    0.    0.    0.    0.    0.617 0.    0.   ]\n",
      "[4168, 21, 170, 36, 844, 61, 16, 4605, 37, 42]\n",
      "Run: 9, Optimal Decision: q10 with chance: 0.902754 chosen 9441 times. Average reward: 0.88\n",
      "[0.513 0.305 0.552 0.444 0.114 0.583 0.691 0.114 0.29  0.903]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[120, 23, 64, 48, 20, 157, 89, 16, 22, 9441]\n",
      "Run: 10, Optimal Decision: q1 with chance: 0.882216 chosen 7606 times. Average reward: 0.85\n",
      "[0.882 0.613 0.758 0.474 0.503 0.367 0.854 0.2   0.229 0.195]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[7606, 138, 473, 100, 120, 30, 1451, 26, 24, 32]\n",
      "\n",
      "\n",
      "Starting LRI - Linear Reward Inaction Algorithm\n",
      "\n",
      "\n",
      "Run: 1, Optimal Decision: q6 with chance: 0.731245 chosen 9370 times / 10000\n",
      "[0.217 0.037 0.156 0.613 0.147 0.731 0.006 0.509 0.517 0.283]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[48, 32, 17, 99, 36, 9370, 32, 124, 187, 55]\n",
      "Run: 2, Optimal Decision: q4 with chance: 0.874762 chosen 3725 times / 10000\n",
      "[0.156 0.372 0.198 0.875 0.626 0.505 0.358 0.111 0.788 0.868]\n",
      "[0.    0.    0.    0.348 0.    0.    0.    0.    0.    0.652]\n",
      "[19, 32, 25, 3725, 49, 42, 65, 12, 110, 5921]\n",
      "Run: 3, Optimal Decision: q2 with chance: 0.753764 chosen 1160 times / 10000\n",
      "[0.549 0.754 0.236 0.441 0.08  0.748 0.028 0.312 0.139 0.582]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[210, 1160, 21, 198, 26, 8063, 23, 48, 62, 189]\n",
      "Run: 4, Optimal Decision: q7 with chance: 0.925873 chosen 7576 times / 10000\n",
      "[0.147 0.464 0.254 0.067 0.86  0.217 0.926 0.044 0.008 0.245]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[33, 32, 65, 27, 2154, 43, 7576, 23, 20, 27]\n",
      "Run: 5, Optimal Decision: q3 with chance: 0.883017 chosen 8249 times / 10000\n",
      "[0.831 0.546 0.883 0.487 0.037 0.622 0.368 0.061 0.434 0.438]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1415, 107, 8249, 23, 16, 44, 32, 19, 68, 27]\n",
      "Run: 6, Optimal Decision: q1 with chance: 0.988221 chosen 9545 times / 10000\n",
      "[0.988 0.553 0.559 0.332 0.767 0.114 0.681 0.123 0.281 0.219]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[9545, 107, 58, 27, 69, 22, 67, 50, 27, 28]\n",
      "Run: 7, Optimal Decision: q5 with chance: 0.957531 chosen 2835 times / 10000\n",
      "[0.661 0.194 0.084 0.956 0.958 0.085 0.817 0.431 0.385 0.69 ]\n",
      "[0.   0.   0.   0.46 0.54 0.   0.   0.   0.   0.  ]\n",
      "[118, 35, 17, 6687, 2835, 17, 190, 30, 41, 30]\n",
      "Run: 8, Optimal Decision: q2 with chance: 0.889651 chosen 8008 times / 10000\n",
      "[0.377 0.89  0.732 0.069 0.594 0.758 0.713 0.764 0.206 0.351]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[18, 8008, 819, 26, 56, 290, 353, 394, 13, 23]\n",
      "Run: 9, Optimal Decision: q1 with chance: 0.996130 chosen 7076 times / 10000\n",
      "[0.996 0.874 0.941 0.555 0.052 0.799 0.358 0.623 0.973 0.358]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[7076, 116, 1354, 16, 20, 77, 37, 32, 1262, 10]\n",
      "Run: 10, Optimal Decision: q7 with chance: 0.945287 chosen 9274 times / 10000\n",
      "[0.253 0.663 0.478 0.501 0.521 0.106 0.945 0.047 0.812 0.575]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[36, 275, 75, 41, 43, 16, 9274, 20, 133, 87]\n"
     ]
    }
   ],
   "source": [
    "import random, datetime, numpy as np, math\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def lrI(alpha):\n",
    "\tprint(\"\\n\\nStarting LRI - Linear Reward Inaction Algorithm\\n\\n\")\n",
    "\tfor j in range(1):\n",
    "\t\tlevers = [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\"]\n",
    "\t\treward_percent = [random.random() for i in range(10)]\n",
    "\t\tlever_count, reward_count  = ([0 for i in range(10)] for i in range(2))\n",
    "\t\t#probability of choosing a lever - sum equal to 1\n",
    "\t\tlever_probability = [0.1 for i in range(10)]\n",
    "\t\toptimal_decision = 0\n",
    "\t\t#randomly choose a lever and set alpha/beta values\n",
    "\t\tlever = random.randint(0, 9)\n",
    "\t\tbeta = 0\n",
    "\t\t#gets lever that is the most optimal (has the highest chance of giving us a reward)\n",
    "\t\toptimal_decision = reward_percent.index(max(reward_percent))\n",
    "\t\ti = 1\n",
    "\t\twhile i <= 10000:\n",
    "\t\t# if reward = 1\n",
    "\t\t\tif random.random() < reward_percent[lever]:\n",
    "\t\t\t\treward_count[lever]+=1\n",
    "\t\t\t\t#update the current lever\n",
    "\t\t\t\tlever_probability[lever] = lever_probability[lever] + alpha * (1- lever_probability[lever])\n",
    "\t\t\t\t#update other levers\n",
    "\t\t\t\tfor x in range(0, len(lever_probability)):\n",
    "\t\t\t\t\tif x is not lever:\n",
    "\t\t\t\t\t\tlever_probability[x] = (1 - alpha) * lever_probability[x]\n",
    "\t\t\tlever_count[lever] += 1\n",
    "\t\t\ti+=1\n",
    "\t\t\t#choose the lever according to the probability distribution lever_probability\n",
    "\t\t\tlever = np.random.choice(10, p=lever_probability)\n",
    "\n",
    "\t\tprint(\"Run: %d, Optimal Decision: %s with chance: %f chosen %d times. Average reward: %.2f\" % (j+1, levers[optimal_decision], reward_percent[optimal_decision], lever_count[optimal_decision], sum(reward_count)/i))\n",
    "\t\tprint(np.around(reward_percent,3))\n",
    "\t\tprint(np.around(lever_probability,3))\n",
    "\t\tprint(lever_count)\n",
    "        \n",
    "def lrI2(alpha):\n",
    "    print(\"\\n\\nStarting LRI - Linear Reward Inaction Algorithm\\n\\n\")\n",
    "    for j in range(1):\n",
    "        levers = [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\"]\n",
    "        reward_percent = [random.random() for i in range(10)]\n",
    "        lever_count, reward_count  = ([0 for i in range(10)] for i in range(2))\n",
    "        lever_probability = np.zeros(10) + 0.1\n",
    "        optimal_decision = 0\n",
    "        lever = random.randint(0, 9)\n",
    "        optimal_decision = reward_percent.index(max(reward_percent))\n",
    "        i = 1\n",
    "        while i <= 10000:\n",
    "            if random.random() < reward_percent[lever]:\n",
    "                reward_count[lever] = reward_count[lever] + 1\n",
    "                mask = np.zeros(10)\n",
    "                mask[lever] = mask[lever] + 1\n",
    "                lever_probability = (mask) * (lever_probability + alpha*(1-lever_probability)) + (1-mask) *((1 - alpha) * lever_probability)\n",
    "\n",
    "            lever_count[lever] += 1; i+=1\n",
    "            lever = np.random.choice(10, p=lever_probability)\n",
    "\n",
    "        print(\"Run: %d, Optimal Decision: %s with chance: %f chosen %d times / 10000\"% \n",
    "              (j+1, levers[optimal_decision], reward_percent[optimal_decision], lever_count[optimal_decision])  )\n",
    "        print(np.around(reward_percent,3))\n",
    "        print(np.around(lever_probability,3))\n",
    "        print(lever_count)\n",
    "        \n",
    "alpha2 =  0.01\n",
    "lrI(alpha2)\n",
    "lrI2(alpha2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Michael Pacheco. (2019). Michaelpacheco.net. Retrieved 9 January 2019, from https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2\n",
    "2. numpy.random.choice â€” NumPy v1.15 Manual. (2019). Docs.scipy.org. Retrieved 9 January 2019, from https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:28:19.119522Z",
     "start_time": "2019-01-09T23:28:19.114103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "lever_probability = np.random.uniform(0,1,10)\n",
    "lever_probability = lever_probability/lever_probability.sum()\n",
    "print(np.random.choice(10, p=lever_probability))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
