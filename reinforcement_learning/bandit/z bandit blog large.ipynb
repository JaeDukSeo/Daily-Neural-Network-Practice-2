{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<ol>\n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction (Pursuit Methods)</li>\n",
    "<li>f: Linear Reward Penalty (Pursuit Methods)</li>\n",
    "<li>g: UBC 1</li>\n",
    "<li>h: UCB 1-Tuned</li>\n",
    "<li>i: Thompson Sampling (beta)</li>\n",
    "<li>j: Thompson Sampling (uniform)</li>\n",
    "<li>k: Neural Network</li>\n",
    "<li>l: softmax </li>\n",
    "<li>m: Gradient Bandits</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:51:57.945429Z",
     "start_time": "2019-01-14T08:51:54.995121Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy,time,sys\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)\n",
    "tf.set_random_seed(678)\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:52:00.598470Z",
     "start_time": "2019-01-14T08:52:00.593459Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Best Choice:  48 0.9943540985119588\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 100\n",
    "num_ep  = 20\n",
    "num_iter= 1000\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:54:01.014617Z",
     "start_time": "2019-01-14T08:54:00.988693Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Expected \n",
      "[0.491 0.048 0.373 0.538 0.614 0.416 0.186 0.279 0.077 0.181 0.09  0.73\n",
      " 0.194 0.757 0.629 0.791 0.795 0.412 0.792 0.102 0.96  0.141 0.493 0.889\n",
      " 0.765 0.021 0.296 0.643 0.984 0.933 0.634 0.264 0.624 0.132 0.797 0.661\n",
      " 0.245 0.138 0.633 0.293 0.455 0.471 0.419 0.882 0.074 0.35  0.819 0.86\n",
      " 0.993 0.213 0.297 0.614 0.392 0.438 0.993 0.472 0.905 0.115 0.263 0.751\n",
      " 0.277 0.892 0.582 0.413 0.279 0.732 0.932 0.851 0.503 0.104 0.145 0.615\n",
      " 0.321 0.244 0.389 0.588 0.125 0.062 0.047 0.02  0.812 0.812 0.016 0.812\n",
      " 0.609 0.741 0.628 0.502 0.63  0.76  0.707 0.951 0.414 0.948 0.236 0.925\n",
      " 0.661 0.139 0.799 0.11 ]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1,num_bandit) < gt_prob\n",
    "        temp_expect    = temp_expect + current_reward\n",
    "\n",
    "    a_expect[eps,:] = temp_expect/temp_choice\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:54:01.361689Z",
     "start_time": "2019-01-14T08:54:01.146266Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Expected \n",
      "[0.488 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "b_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    b_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:54:01.667249Z",
     "start_time": "2019-01-14T08:54:01.400585Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Expected \n",
      "[0.313 0.042 0.311 0.332 0.437 0.473 0.114 0.192 0.043 0.162 0.084 0.507\n",
      " 0.116 0.564 0.544 0.562 0.565 0.289 0.592 0.075 0.679 0.057 0.407 0.662\n",
      " 0.557 0.01  0.189 0.468 0.719 0.703 0.453 0.203 0.489 0.128 0.543 0.475\n",
      " 0.185 0.167 0.45  0.171 0.259 0.359 0.268 0.683 0.06  0.26  0.573 0.661\n",
      " 0.747 0.142 0.265 0.445 0.328 0.366 0.985 0.461 0.647 0.065 0.125 0.537\n",
      " 0.254 0.659 0.352 0.309 0.148 0.566 0.634 0.613 0.314 0.104 0.124 0.395\n",
      " 0.272 0.222 0.338 0.559 0.141 0.055 0.052 0.    0.624 0.614 0.    0.662\n",
      " 0.426 0.534 0.39  0.361 0.415 0.519 0.533 0.685 0.371 0.728 0.151 0.743\n",
      " 0.487 0.079 0.61  0.105]\n"
     ]
    }
   ],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "c_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "  \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    c_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:54:01.925845Z",
     "start_time": "2019-01-14T08:54:01.712090Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Expected \n",
      "[0.175 0.013 0.178 0.108 0.188 0.25  0.075 0.05  0.075 0.037 0.013 0.154\n",
      " 0.117 0.255 0.287 0.242 0.225 0.15  0.237 0.067 0.367 0.058 0.195 0.362\n",
      " 0.283 0.    0.167 0.217 0.396 0.242 0.175 0.171 0.258 0.062 0.325 0.125\n",
      " 0.11  0.025 0.18  0.05  0.196 0.085 0.133 0.342 0.05  0.113 0.217 0.258\n",
      " 0.411 0.067 0.083 0.092 0.113 0.129 0.991 0.243 0.412 0.079 0.085 0.25\n",
      " 0.083 0.338 0.25  0.105 0.108 0.325 0.271 0.317 0.212 0.    0.042 0.237\n",
      " 0.139 0.175 0.079 0.208 0.075 0.    0.05  0.007 0.234 0.275 0.    0.394\n",
      " 0.283 0.229 0.163 0.196 0.233 0.317 0.225 0.396 0.172 0.275 0.125 0.438\n",
      " 0.161 0.05  0.3   0.087]\n"
     ]
    }
   ],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "d_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.99 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    d_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:54:02.874339Z",
     "start_time": "2019-01-14T08:54:01.986682Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Expected \n",
      "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.05 0.   0.   0.   0.1  0.   0.   0.1  0.   0.   0.   0.\n",
      " 0.   0.05 0.   0.   0.   0.   0.1  0.05 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.1  0.05 0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.05 0.05 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.05\n",
      " 0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.1  0.   0.05 0.   0.\n",
      " 0.   0.  ]\n",
      "Expected Normalized\n",
      "[0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012\n",
      " 0.012 0.012 0.012 0.012 0.503 0.012 0.012 0.012 0.994 0.012 0.012 0.994\n",
      " 0.012 0.012 0.012 0.012 0.012 0.503 0.012 0.012 0.012 0.012 0.994 0.503\n",
      " 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.994\n",
      " 0.503 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012\n",
      " 0.012 0.503 0.012 0.012 0.012 0.503 0.503 0.012 0.012 0.012 0.012 0.012\n",
      " 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.503\n",
      " 0.012 0.503 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.994 0.012 0.503\n",
      " 0.012 0.012 0.012 0.012]\n"
     ]
    }
   ],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "e_regret_total = np.zeros((num_ep,num_iter))\n",
    "      \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.1\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "            \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    e_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(np.around(e_estimation.mean(0),3))\n",
    "print('Expected Normalized')\n",
    "print(\n",
    "    (gt_prob.max()-gt_prob.min())*(e_estimation.mean(0)-e_estimation.mean(0).min())/(e_estimation.mean(0).max()-e_estimation.mean(0).min()) + gt_prob.min()\n",
    ")\n",
    "e_estimation = (gt_prob.max()-gt_prob.min())*(e_estimation.mean(0)-e_estimation.mean(0).min())/(e_estimation.mean(0).max()-e_estimation.mean(0).min()) + gt_prob.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:54:03.837052Z",
     "start_time": "2019-01-14T08:54:02.926190Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Expected \n",
      "[0.001 0.    0.    0.001 0.002 0.001 0.    0.    0.    0.    0.    0.007\n",
      " 0.    0.011 0.004 0.013 0.019 0.001 0.018 0.    0.053 0.    0.001 0.042\n",
      " 0.011 0.    0.    0.002 0.048 0.055 0.004 0.    0.005 0.    0.015 0.004\n",
      " 0.    0.    0.004 0.    0.001 0.001 0.002 0.015 0.    0.    0.01  0.031\n",
      " 0.086 0.    0.    0.003 0.    0.    0.07  0.002 0.041 0.    0.    0.008\n",
      " 0.001 0.038 0.002 0.001 0.    0.008 0.062 0.023 0.002 0.    0.    0.006\n",
      " 0.001 0.001 0.002 0.001 0.    0.    0.    0.    0.008 0.023 0.    0.017\n",
      " 0.003 0.002 0.004 0.001 0.008 0.005 0.006 0.049 0.    0.046 0.001 0.055\n",
      " 0.002 0.    0.021 0.   ]\n",
      "Expected Normalized\n",
      "[0.018 0.012 0.014 0.027 0.03  0.022 0.013 0.012 0.012 0.012 0.012 0.093\n",
      " 0.012 0.139 0.06  0.159 0.231 0.019 0.215 0.012 0.617 0.015 0.017 0.495\n",
      " 0.138 0.012 0.013 0.029 0.563 0.639 0.061 0.012 0.063 0.012 0.184 0.055\n",
      " 0.012 0.012 0.053 0.013 0.025 0.02  0.032 0.182 0.012 0.013 0.129 0.371\n",
      " 0.994 0.012 0.012 0.04  0.013 0.015 0.818 0.033 0.486 0.012 0.013 0.107\n",
      " 0.016 0.446 0.037 0.024 0.013 0.108 0.724 0.275 0.034 0.012 0.013 0.076\n",
      " 0.025 0.019 0.037 0.024 0.014 0.012 0.014 0.012 0.099 0.271 0.012 0.205\n",
      " 0.049 0.038 0.052 0.022 0.105 0.066 0.082 0.568 0.012 0.533 0.019 0.645\n",
      " 0.033 0.012 0.246 0.012]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "f_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.01\n",
    "    beta  = 0.001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    f_regret_total[eps,:] = temp_regret\n",
    "    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(\n",
    "    (gt_prob.max()-gt_prob.min())*(f_estimation.mean(0)-f_estimation.mean(0).min())/(f_estimation.mean(0).max()-f_estimation.mean(0).min()) + gt_prob.min()\n",
    ")\n",
    "f_estimation = (gt_prob.max()-gt_prob.min())*(e_estimation.mean(0)-e_estimation.mean(0).min())/(e_estimation.mean(0).max()-e_estimation.mean(0).min()) + gt_prob.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T08:54:04.216126Z",
     "start_time": "2019-01-14T08:54:03.886001Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Expected \n",
      "[0.35  0.013 0.212 0.37  0.403 0.265 0.067 0.12  0.029 0.113 0.042 0.597\n",
      " 0.057 0.659 0.429 0.638 0.622 0.268 0.666 0.037 0.951 0.09  0.31  0.834\n",
      " 0.676 0.    0.207 0.467 0.956 0.864 0.483 0.134 0.494 0.05  0.642 0.497\n",
      " 0.199 0.071 0.513 0.13  0.262 0.283 0.315 0.808 0.05  0.198 0.71  0.652\n",
      " 0.971 0.1   0.144 0.44  0.174 0.189 0.978 0.177 0.85  0.034 0.157 0.634\n",
      " 0.105 0.758 0.277 0.263 0.187 0.529 0.875 0.69  0.297 0.05  0.149 0.418\n",
      " 0.203 0.146 0.217 0.429 0.08  0.071 0.    0.    0.715 0.666 0.025 0.691\n",
      " 0.433 0.633 0.368 0.254 0.508 0.639 0.591 0.919 0.311 0.859 0.16  0.788\n",
      " 0.438 0.102 0.602 0.062]\n"
     ]
    }
   ],
   "source": [
    "# g UBC 1\n",
    "g_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "g_estimation   = np.zeros((num_ep,num_bandit))\n",
    "g_reward       = np.zeros((num_ep,num_iter))\n",
    "g_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "g_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(0.5*np.log(iter+1)/(temp_pull_count+1)))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    g_pull_count[eps,:]   = temp_pull_count\n",
    "    g_estimation[eps,:]   = temp_estimation\n",
    "    g_reward[eps,:]       = temp_reward\n",
    "    g_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    g_regret_total[eps,:] = temp_regret\n",
    "  \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(g_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:01.905Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736\n",
      " 0.196 0.768 0.638 0.796 0.788 0.417 0.789 0.103 0.964 0.137 0.497 0.895\n",
      " 0.766 0.019 0.309 0.631 0.984 0.932 0.619 0.275 0.633 0.14  0.8   0.659\n",
      " 0.254 0.143 0.643 0.272 0.45  0.462 0.428 0.883 0.074 0.342 0.805 0.867\n",
      " 0.994 0.207 0.315 0.598 0.396 0.437 0.993 0.48  0.91  0.117 0.262 0.765\n",
      " 0.265 0.894 0.553 0.412 0.283 0.743 0.926 0.85  0.503 0.104 0.134 0.612\n",
      " 0.317 0.25  0.387 0.605 0.141 0.069 0.044 0.019 0.8   0.805 0.012 0.818\n",
      " 0.612 0.737 0.625 0.482 0.624 0.745 0.716 0.95  0.418 0.94  0.252 0.913\n",
      " 0.664 0.153 0.804 0.122]\n",
      "Expected \n",
      "[0.251 0.    0.137 0.265 0.251 0.168 0.058 0.07  0.083 0.112 0.075 0.523\n",
      " 0.067 0.57  0.4   0.459 0.416 0.216 0.597 0.033 0.914 0.02  0.208 0.675\n",
      " 0.349 0.017 0.129 0.371 0.951 0.69  0.343 0.1   0.312 0.104 0.475 0.249\n",
      " 0.117 0.05  0.428 0.07  0.169 0.188 0.245 0.6   0.033 0.092 0.389 0.558\n",
      " 0.986 0.09  0.185 0.346 0.128 0.131 0.977 0.203 0.692 0.05  0.115 0.32\n",
      " 0.083 0.608 0.201 0.186 0.127 0.539 0.785 0.552 0.206 0.    0.053 0.217\n",
      " 0.16  0.087 0.137 0.257 0.053 0.02  0.017 0.017 0.58  0.504 0.017 0.556\n",
      " 0.193 0.449 0.393 0.233 0.257 0.361 0.322 0.771 0.175 0.822 0.123 0.809\n",
      " 0.398 0.037 0.552 0.   ]\n"
     ]
    }
   ],
   "source": [
    "# h UBC 1 Tuned\n",
    "h_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "h_estimation   = np.zeros((num_ep,num_bandit))\n",
    "h_reward       = np.zeros((num_ep,num_iter))\n",
    "h_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "h_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit) \n",
    "    temp_estimation   = np.zeros(num_bandit) \n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_sumof_squares= np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        v_temp = temp_sumof_squares + np.sqrt(2*np.log(iter+1)/(temp_pull_count+1))\n",
    "        current_min_value = np.minimum(v_temp,np.ones_like(v_temp)*0.25)\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(np.log(iter+1)/(temp_pull_count+1)*current_min_value))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        temp_sumof_squares[current_choice] = temp_sumof_squares[current_choice] + current_reward ** 2\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    h_pull_count[eps,:]   = temp_pull_count\n",
    "    h_estimation[eps,:]   = temp_estimation\n",
    "    h_reward[eps,:]       = temp_reward\n",
    "    h_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    h_regret_total[eps,:] = temp_regret\n",
    "    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(h_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:02.086Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# i Thompson Sampling (beta) (slow)\n",
    "i_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "i_estimation   = np.zeros((num_ep,num_bandit))\n",
    "i_reward       = np.zeros((num_ep,num_iter))\n",
    "i_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "i_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.beta(a=1+w,b=1+t-w).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    i_pull_count[eps,:]   = temp_pull_count\n",
    "    i_estimation[eps,:]   = theta_samples\n",
    "    i_reward[eps,:]       = temp_reward\n",
    "    i_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    i_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(i_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:02.313Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# j Thompson Sampling (uniform) (slow)\n",
    "j_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "j_estimation   = np.zeros((num_ep,num_bandit))\n",
    "j_reward       = np.zeros((num_ep,num_iter))\n",
    "j_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "j_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.uniform(w/(t+0.000000001),1-w/(t+0.000000001)).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    j_pull_count[eps,:]   = temp_pull_count\n",
    "    j_estimation[eps,:]   = theta_samples\n",
    "    j_reward[eps,:]       = temp_reward\n",
    "    j_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    j_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(j_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:02.596Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# k neural network (with adam)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "k_regret_total = np.zeros((num_ep,num_iter))\n",
    "\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def d_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    weights = np.random.randn(num_bandit,1)\n",
    "    moment  = np.zeros_like(weights); \n",
    "    velocity = np.zeros_like(weights);\n",
    "    epsilon  = 0.3\n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        if np.random.uniform(0,1)>epsilon:\n",
    "            current_choice = np.argmax(weights)\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "        else:\n",
    "            current_choice = np.random.choice(np.arange(num_bandit))\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "\n",
    "        layer1 = current_input @ weights\n",
    "        layer1a= sigmoid(layer1)\n",
    "\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        # KL Divergence https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "        grad3 = np.log(layer1a+0.0000001) - np.log(temp_estimation[current_choice]/(temp_pull_count[current_choice])+0.0000001)\n",
    "        grad2 = d_sigmoid(layer1)\n",
    "        grad1 = current_input\n",
    "        grad  = grad1.T @ (grad3 * grad2)\n",
    "        \n",
    "        moment   = 0.9*moment + (1-0.9) * grad\n",
    "        velocity = 0.999*velocity + (1-0.999) * grad**2\n",
    "        moment_hat   = moment/(1-0.9)\n",
    "        velocity_hat = velocity/(1-0.999)\n",
    "        weights  = weights - 0.08 * (moment_hat/(np.sqrt(velocity_hat)+1e-8))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # Decay the learning rate\n",
    "        epsilon = epsilon * 0.999\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = np.squeeze(sigmoid(weights))\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    k_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:02.821Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# l softmax\n",
    "l_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "l_estimation   = np.zeros((num_ep,num_bandit))\n",
    "l_reward       = np.zeros((num_ep,num_iter))\n",
    "l_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "l_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    tempture = 30\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation/tempture) / np.sum(np.exp(temp_estimation/tempture))\n",
    "        current_choice = np.random.choice(num_bandit,p=pi)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        # decay the temp\n",
    "        tempture = tempture * 0.99\n",
    "        \n",
    "    l_pull_count[eps,:]   = temp_pull_count\n",
    "    l_estimation[eps,:]   = temp_estimation\n",
    "    l_reward[eps,:]       = temp_reward\n",
    "    l_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    l_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(l_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:03.281Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# m gradient base\n",
    "m_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "m_estimation   = np.zeros((num_ep,num_bandit))\n",
    "m_reward       = np.zeros((num_ep,num_iter))\n",
    "m_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "m_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    temp_mean_reward = 0\n",
    "    alpha = 0.8\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation) / np.sum(np.exp(temp_estimation))\n",
    "        current_choice = np.random.choice(num_bandit,p=pi)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        temp_mean_reward = temp_mean_reward + ((current_reward-temp_mean_reward))/(iter) if not iter==0 else ((current_reward-temp_mean_reward))\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1\n",
    "        \n",
    "        temp_estimation = (mask)   * (temp_estimation+alpha*(current_reward-temp_mean_reward)*(1-pi)) + \\\n",
    "                          (1-mask) * (temp_estimation-alpha*(current_reward-temp_mean_reward)*(pi))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    m_pull_count[eps,:]   = temp_pull_count\n",
    "    m_estimation[eps,:]   = temp_estimation\n",
    "    m_reward[eps,:]       = temp_reward\n",
    "    m_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    m_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(np.around(m_estimation.mean(0),2))\n",
    "print('Expected Normalized')\n",
    "print(\n",
    "    (gt_prob.max()-gt_prob.min())*(m_estimation.mean(0)-m_estimation.mean(0).min())/(m_estimation.mean(0).max()-m_estimation.mean(0).min()) + gt_prob.min()\n",
    ")\n",
    "m_estimation = (gt_prob.max()-gt_prob.min())*(m_estimation.mean(0)-m_estimation.mean(0).min())/(m_estimation.mean(0).max()-m_estimation.mean(0).min()) + gt_prob.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:03.787Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the regret\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(b_regret_total.mean(0),c='red',    label='greedy')\n",
    "plt.plot(c_regret_total.mean(0),c='blue',   label='e greedy')\n",
    "plt.plot(d_regret_total.mean(0),c='green',  label='decay e greedy')\n",
    "plt.plot(e_regret_total.mean(0),c='yellow', label='Linear Reward Inaction')\n",
    "plt.plot(f_regret_total.mean(0),c='purple', label='Linear Reward Penalty')\n",
    "plt.plot(g_regret_total.mean(0),c='black',  label='UBC')\n",
    "plt.plot(h_regret_total.mean(0),c='gold',   label='UBC tuned')\n",
    "plt.plot(i_regret_total.mean(0),c='pink',   label='beta')\n",
    "plt.plot(j_regret_total.mean(0),c='grey',   label='uniform')\n",
    "plt.plot(k_regret_total.mean(0),c='skyblue',label='NN')\n",
    "plt.plot(l_regret_total.mean(0),c='cyan',   label='softmax')\n",
    "plt.plot(m_regret_total.mean(0),c='magenta',label='Grad')\n",
    "plt.legend(prop={'size': 30})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:04.130Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the reward\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(b_reward.mean(0),c='red',    label='greedy')\n",
    "plt.plot(c_reward.mean(0),c='blue',   label='e greedy')\n",
    "plt.plot(d_reward.mean(0),c='green',  label='decay e greedy')\n",
    "plt.plot(e_reward.mean(0),c='yellow', label='Linear Reward Inaction')\n",
    "plt.plot(f_reward.mean(0),c='purple', label='Linear Reward Penalty')\n",
    "plt.plot(g_reward.mean(0),c='black',  label='UBC')\n",
    "plt.plot(h_reward.mean(0),c='gold',   label='UBC tuned')\n",
    "plt.plot(i_reward.mean(0),c='pink',   label='beta')\n",
    "plt.plot(j_reward.mean(0),c='grey',   label='uniform')\n",
    "plt.plot(k_reward.mean(0),c='skyblue',label='NN')\n",
    "plt.plot(l_reward.mean(0),c='cyan',   label='softmax')\n",
    "plt.plot(m_reward.mean(0),c='magenta',label='Grad')\n",
    "plt.legend(prop={'size': 30})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:04.715Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the reward\n",
    "print(gt_prob)\n",
    "print(b_estimation.mean(0))\n",
    "print(c_estimation.mean(0))\n",
    "print(d_estimation.mean(0))\n",
    "print(e_estimation)\n",
    "print(f_estimation)\n",
    "print(g_estimation.mean(0))\n",
    "print(h_estimation.mean(0))\n",
    "print(i_estimation.mean(0))\n",
    "print(j_estimation.mean(0))\n",
    "print(k_estimation.mean(0))\n",
    "print(l_estimation.mean(0))\n",
    "print(m_estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T08:54:05.224Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# correaltion\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "data = pd.DataFrame({\n",
    "        'GT':gt_prob,\n",
    "        'Vector': a_expect.mean(0),\n",
    "        'greedy':b_estimation.mean(0),\n",
    "        'e-greedy':c_estimation.mean(0),\n",
    "        'decay e-greedy':d_estimation.mean(0),\n",
    "        'Linear Reward Inaction':e_estimation,\n",
    "        'Linear Reward Penalty':f_estimation,\n",
    "        'UCB1':g_estimation.mean(0),\n",
    "        'UCB1 Tuned':h_estimation.mean(0),\n",
    "        'beta':i_estimation.mean(0),\n",
    "        'uniform':j_estimation.mean(0),\n",
    "        'NN':k_estimation.mean(0),\n",
    "        'softmax':l_estimation.mean(0),\n",
    "        'grad':m_estimation\n",
    "    })\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(data.corr(),annot=True,linewidths=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions  NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "2. [ Archived Post ] Random Note about Multi-Arm Bandit Problem 2. (2019). Medium. Retrieved 13 January 2019, from https://medium.com/@SeoJaeDuk/archived-post-random-note-about-multi-arm-bandit-problem-2-5c522d1dfbdc\n",
    "3. Vieira, T. (2014). KL-divergence as an objective function  Graduate Descent. Timvieira.github.io. Retrieved 13 January 2019, from https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "4. Some Reinforcement Learning: The Greedy and Explore-Exploit Algorithms for the Multi-Armed Bandit Framework in Python. (2019). Datasciencecentral.com. Retrieved 13 January 2019, from https://www.datasciencecentral.com/profiles/blogs/some-reinforcement-learning-the-greedy-and-explore-exploit\n",
    "5. (2019). Cs.mcgill.ca. Retrieved 13 January 2019, from https://www.cs.mcgill.ca/~vkules/bandits.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
