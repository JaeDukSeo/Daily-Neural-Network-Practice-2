{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction</li>\n",
    "<li>f: Linear Reward Penalty</li>\n",
    "<li>g: UBC</li>\n",
    "<li>h: BayesianUCB</li>\n",
    "<li>i: Thompson Sampling</li>\n",
    "<li>j: Neural Network</li>\n",
    "<li>k: Non Stationary</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T02:15:42.633947Z",
     "start_time": "2019-01-13T02:15:42.629482Z"
    }
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy,time,sys\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T02:39:02.697751Z",
     "start_time": "2019-01-13T02:39:02.691827Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77  0.29  0.795 0.553 0.671 0.403 0.445 0.47  0.225 0.195 0.567 0.347]\n",
      "Best Choice:  2 0.7947818672937569\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 12\n",
    "num_ep  = 200\n",
    "num_iter= 2000\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T02:39:03.736717Z",
     "start_time": "2019-01-13T02:39:03.419281Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.77  0.29  0.795 0.553 0.671 0.403 0.445 0.47  0.225 0.195 0.567 0.347]\n",
      "Expected \n",
      "[0.762 0.285 0.788 0.546 0.664 0.397 0.44  0.464 0.221 0.193 0.559 0.342]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1) < gt_prob\n",
    "        temp_expect    = temp_expect + (1/(temp_choice+1)) * (current_reward - temp_expect)\n",
    "    a_expect[eps,:] = temp_expect\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T02:39:07.722582Z",
     "start_time": "2019-01-13T02:39:05.317979Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.77  0.29  0.795 0.553 0.671 0.403 0.445 0.47  0.225 0.195 0.567 0.347]\n",
      "Expected \n",
      "[0.    0.    0.795 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T02:46:38.677687Z",
     "start_time": "2019-01-13T02:46:34.657624Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.77  0.29  0.795 0.553 0.671 0.403 0.445 0.47  0.225 0.195 0.567 0.347]\n",
      "Expected \n",
      "[0.743 0.288 0.793 0.537 0.643 0.387 0.425 0.437 0.216 0.189 0.543 0.333]\n"
     ]
    }
   ],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T02:46:45.602639Z",
     "start_time": "2019-01-13T02:46:41.318723Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.77  0.29  0.795 0.553 0.671 0.403 0.445 0.47  0.225 0.195 0.567 0.347]\n",
      "Expected \n",
      "[0.757 0.286 0.794 0.546 0.658 0.397 0.44  0.463 0.221 0.191 0.55  0.35 ]\n"
     ]
    }
   ],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.999 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:03:12.764805Z",
     "start_time": "2019-01-13T04:02:57.202334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.77  0.29  0.795 0.553 0.671 0.403 0.445 0.47  0.225 0.195 0.567 0.347]\n",
      "Expected \n",
      "[0.138 0.053 0.146 0.09  0.116 0.066 0.073 0.076 0.046 0.044 0.092 0.059]\n",
      "Expected Normalized\n",
      "[0.793 0.303 0.835 0.515 0.663 0.379 0.416 0.438 0.265 0.253 0.529 0.339]\n"
     ]
    }
   ],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(e_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(e_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:18:06.913869Z",
     "start_time": "2019-01-13T04:17:50.374281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.77  0.29  0.795 0.553 0.671 0.403 0.445 0.47  0.225 0.195 0.567 0.347]\n",
      "Expected \n",
      "[0.173 0.044 0.279 0.062 0.088 0.05  0.057 0.054 0.043 0.041 0.063 0.047]\n",
      "Expected Normalized\n",
      "[0.992 0.251 1.597 0.354 0.503 0.286 0.328 0.307 0.248 0.237 0.358 0.267]\n"
     ]
    }
   ],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.001\n",
    "    beta  = 0.0001\n",
    "    \n",
    "    alpha = np.random.uniform(0,1)\n",
    "    beta  = 1-alpha\n",
    "    alpha = alpha/num_bandit\n",
    "    beta = beta/num_bandit\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(f_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:17:06.379555Z",
     "start_time": "2019-01-13T04:17:06.373602Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-13T01:55:15.467Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-13T01:54:57.308Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:50.978044Z",
     "start_time": "2019-01-13T01:50:50.742466Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions â€” NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
