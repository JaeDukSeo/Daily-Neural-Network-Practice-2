{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction</li>\n",
    "<li>f: Linear Reward Penalty</li>\n",
    "<li>g: UBC</li>\n",
    "<li>h: BayesianUCB</li>\n",
    "<li>i: Thompson Sampling</li>\n",
    "<li>j: Neural Network</li>\n",
    "<li>k: Non Stationary</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:27:29.554957Z",
     "start_time": "2019-01-13T04:27:29.549999Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy,time,sys\n",
    "import scipy.stats as stats\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:27:29.758084Z",
     "start_time": "2019-01-13T04:27:29.750110Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Best Choice:  11 0.7364685816073836\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 12\n",
    "num_ep  = 20\n",
    "num_iter= 2000\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:27:30.117361Z",
     "start_time": "2019-01-13T04:27:30.058312Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.489 0.056 0.37  0.519 0.598 0.434 0.176 0.284 0.069 0.182 0.085 0.734]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1) < gt_prob\n",
    "        temp_expect    = temp_expect + (1/(temp_choice+1)) * (current_reward - temp_expect)\n",
    "    a_expect[eps,:] = temp_expect\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:27:30.582584Z",
     "start_time": "2019-01-13T04:27:30.310316Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.739]\n"
     ]
    }
   ],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:27:31.198161Z",
     "start_time": "2019-01-13T04:27:30.714555Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.489 0.066 0.339 0.524 0.559 0.393 0.175 0.292 0.074 0.198 0.09  0.737]\n"
     ]
    }
   ],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:27:31.799271Z",
     "start_time": "2019-01-13T04:27:31.331046Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.472 0.067 0.37  0.515 0.579 0.422 0.183 0.285 0.057 0.188 0.082 0.731]\n"
     ]
    }
   ],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.999 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:27:33.533281Z",
     "start_time": "2019-01-13T04:27:31.961485Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.104 0.045 0.081 0.108 0.129 0.092 0.055 0.07  0.046 0.057 0.046 0.166]\n",
      "Expected Normalized\n",
      "[0.419 0.181 0.325 0.433 0.516 0.369 0.221 0.279 0.184 0.23  0.185 0.665]\n"
     ]
    }
   ],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(e_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(e_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:27:35.483055Z",
     "start_time": "2019-01-13T04:27:33.723250Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.105 0.046 0.08  0.109 0.126 0.093 0.055 0.073 0.047 0.057 0.048 0.16 ]\n",
      "Expected Normalized\n",
      "[0.422 0.186 0.32  0.435 0.506 0.374 0.222 0.291 0.188 0.228 0.194 0.64 ]\n"
     ]
    }
   ],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.001\n",
    "    beta  = 0.0001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(f_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T04:17:06.379555Z",
     "start_time": "2019-01-13T04:17:06.373602Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-13T04:28:53.326Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Thompson Sampling (beta)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.beta(a=1+w,b=1+t-w).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-13T01:55:15.467Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-13T01:54:57.308Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:50.978044Z",
     "start_time": "2019-01-13T01:50:50.742466Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions â€” NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "2. [ Archived Post ] Random Note about Multi-Arm Bandit Problem 2. (2019). Medium. Retrieved 13 January 2019, from https://medium.com/@SeoJaeDuk/archived-post-random-note-about-multi-arm-bandit-problem-2-5c522d1dfbdc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
