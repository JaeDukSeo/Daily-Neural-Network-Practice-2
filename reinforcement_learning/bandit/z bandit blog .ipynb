{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:29:27.640901Z",
     "start_time": "2019-01-13T01:29:26.638907Z"
    }
   },
   "source": [
    "### Compare Listing \n",
    "<ol>\n",
    "<li>a: vector uniform</li>\n",
    "<li>b: greedy</li>\n",
    "<li>c: e - greedy</li>\n",
    "<li>d: decay e - greedy</li>\n",
    "<li>e: Linear Reward Inaction (Pursuit Methods)</li>\n",
    "<li>f: Linear Reward Penalty (Pursuit Methods)</li>\n",
    "<li>g: UBC 1</li>\n",
    "<li>h: UCB 1-Tuned</li>\n",
    "<li>i: Thompson Sampling (beta)</li>\n",
    "<li>j: Thompson Sampling (uniform)</li>\n",
    "<li>k: Neural Network</li>\n",
    "<li>l: softmax </li>\n",
    "<li>m: Gradient Bandits</li>\n",
    "<li>n: Non Stationary</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.383974Z",
     "start_time": "2019-01-14T05:31:26.552073Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy,time,sys\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "np.random.seed(5678)\n",
    "np.set_printoptions(3)\n",
    "tf.set_random_seed(678)\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:31:39.403920Z",
     "start_time": "2019-01-14T05:31:39.394945Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Best Choice:  11 0.7364685816073836\n"
     ]
    }
   ],
   "source": [
    "# setting the ground truth\n",
    "num_bandit = 12\n",
    "num_ep  = 20\n",
    "num_iter= 1000\n",
    "gt_prob = np.random.uniform(0,1,num_bandit)\n",
    "optimal_choice = np.argmax(gt_prob)\n",
    "print(gt_prob)\n",
    "print('Best Choice: ',optimal_choice,gt_prob[optimal_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T05:32:08.136712Z",
     "start_time": "2019-01-14T05:32:08.110745Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.481 0.057 0.354 0.528 0.599 0.423 0.17  0.275 0.084 0.182 0.084 0.758]\n"
     ]
    }
   ],
   "source": [
    "# a vectorized\n",
    "a_expect = np.zeros((num_ep,num_bandit))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_expect = np.zeros(num_bandit)\n",
    "    temp_choice = np.zeros(num_bandit)\n",
    "                    \n",
    "    for iter in range(num_iter//10):\n",
    "        temp_choice    = temp_choice + 1\n",
    "        current_reward = np.random.uniform(0,1,num_bandit) < gt_prob\n",
    "        temp_expect    = temp_expect + current_reward\n",
    "\n",
    "    a_expect[eps,:] = temp_expect/temp_choice\n",
    "                    \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(a_expect.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:36.813205Z",
     "start_time": "2019-01-14T06:25:36.653641Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.484 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# b greedy\n",
    "b_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "b_estimation   = np.zeros((num_ep,num_bandit))\n",
    "b_reward       = np.zeros((num_ep,num_iter))\n",
    "b_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "b_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    b_pull_count[eps,:]   = temp_pull_count\n",
    "    b_estimation[eps,:]   = temp_estimation\n",
    "    b_reward[eps,:]       = temp_reward\n",
    "    b_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    b_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(b_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:25:37.435275Z",
     "start_time": "2019-01-14T06:25:37.184025Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.461 0.049 0.316 0.513 0.577 0.401 0.164 0.244 0.071 0.184 0.074 0.734]\n"
     ]
    }
   ],
   "source": [
    "# c e greedy \n",
    "c_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "c_estimation   = np.zeros((num_ep,num_bandit))\n",
    "c_reward       = np.zeros((num_ep,num_iter))\n",
    "c_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "c_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = np.random.uniform(0,1)\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "  \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "    c_pull_count[eps,:]   = temp_pull_count\n",
    "    c_estimation[eps,:]   = temp_estimation\n",
    "    c_reward[eps,:]       = temp_reward\n",
    "    c_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    c_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(c_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:26:06.981650Z",
     "start_time": "2019-01-14T06:26:06.850003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(b_regret_total.mean(0),label='greedy')\n",
    "plt.plot(c_regret_total.mean(0),label='e greedy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:03.343630Z",
     "start_time": "2019-01-13T08:53:02.983500Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.471 0.066 0.344 0.512 0.575 0.43  0.183 0.279 0.066 0.179 0.101 0.732]\n"
     ]
    }
   ],
   "source": [
    "# d decy e greedy \n",
    "d_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "d_estimation   = np.zeros((num_ep,num_bandit))\n",
    "d_reward       = np.zeros((num_ep,num_iter))\n",
    "d_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    epsilon = 1.0\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_expect) if epsilon < np.random.uniform(0,1) else np.random.choice(np.arange(num_bandit))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # decay the eps\n",
    "        epsilon = 0.999 * epsilon\n",
    "        \n",
    "    d_pull_count[eps,:]   = temp_pull_count\n",
    "    d_estimation[eps,:]   = temp_estimation\n",
    "    d_reward[eps,:]       = temp_reward\n",
    "    d_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(d_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:04.790954Z",
     "start_time": "2019-01-13T08:53:03.877787Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.095 0.062 0.086 0.098 0.104 0.09  0.069 0.078 0.063 0.07  0.063 0.123]\n",
      "Expected Normalized\n",
      "[0.379 0.247 0.343 0.391 0.416 0.359 0.278 0.312 0.251 0.282 0.254 0.494]\n"
     ]
    }
   ],
   "source": [
    "# e Linear Reward Inaction\n",
    "e_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "e_estimation   = np.zeros((num_ep,num_bandit))\n",
    "e_reward       = np.zeros((num_ep,num_iter))\n",
    "e_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    learning_rate = 0.001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + learning_rate * (1-temp_estimation)) + (1-mask) * ( (1-learning_rate) * temp_estimation)\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    e_pull_count[eps,:]   = temp_pull_count\n",
    "    e_estimation[eps,:]   = temp_estimation\n",
    "    e_reward[eps,:]       = temp_reward\n",
    "    e_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(e_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(e_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:53:06.356294Z",
     "start_time": "2019-01-13T08:53:05.320152Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.093 0.063 0.085 0.099 0.105 0.088 0.071 0.078 0.063 0.07  0.065 0.12 ]\n",
      "Expected Normalized\n",
      "[0.374 0.251 0.34  0.396 0.423 0.353 0.283 0.313 0.254 0.28  0.259 0.483]\n"
     ]
    }
   ],
   "source": [
    "# f Linear Reward Penalty\n",
    "f_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "f_estimation   = np.zeros((num_ep,num_bandit))\n",
    "f_reward       = np.zeros((num_ep,num_iter))\n",
    "f_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    alpha = 0.001\n",
    "    beta  = 0.0001\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit) + 1.0/num_bandit\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.random.choice(num_bandit, p=temp_estimation)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "\n",
    "        mask = np.zeros(num_bandit)\n",
    "        mask[current_choice] = 1.0\n",
    "        \n",
    "        if current_reward == 1.0:\n",
    "            temp_estimation = (mask) * (temp_estimation + alpha * (1-temp_estimation)) + (1-mask) * ( (1-alpha) * temp_estimation)\n",
    "        else: \n",
    "            temp_estimation = (mask) * ((1-beta) * temp_estimation) + (1-mask) * ( beta/(num_bandit-1) + (1-beta) * temp_estimation )\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    f_pull_count[eps,:]   = temp_pull_count\n",
    "    f_estimation[eps,:]   = temp_estimation\n",
    "    f_reward[eps,:]       = temp_reward\n",
    "    f_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(f_estimation.mean(0))\n",
    "print('Expected Normalized')\n",
    "print(f_estimation.mean(0) * gt_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:17:20.818768Z",
     "start_time": "2019-01-14T06:17:20.548499Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.497 0.058 0.34  0.492 0.564 0.427 0.134 0.274 0.057 0.139 0.075 0.737]\n"
     ]
    }
   ],
   "source": [
    "# g UBC\n",
    "g_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "g_estimation   = np.zeros((num_ep,num_bandit))\n",
    "g_reward       = np.zeros((num_ep,num_iter))\n",
    "g_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(2*np.log(iter+1)/(temp_pull_count+1)))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    g_pull_count[eps,:]   = temp_pull_count\n",
    "    g_estimation[eps,:]   = temp_estimation\n",
    "    g_reward[eps,:]       = temp_reward\n",
    "    g_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(g_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:21:50.280119Z",
     "start_time": "2019-01-14T06:21:49.983217Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.46  0.047 0.34  0.514 0.568 0.428 0.142 0.244 0.058 0.145 0.066 0.732]\n"
     ]
    }
   ],
   "source": [
    "# h UBC Tuned\n",
    "h_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "h_estimation   = np.zeros((num_ep,num_bandit))\n",
    "h_reward       = np.zeros((num_ep,num_iter))\n",
    "h_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit) \n",
    "    temp_estimation   = np.zeros(num_bandit) \n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        current_min_value = 1\n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(temp_estimation + np.sqrt(np.log(iter+1)/(temp_pull_count+1)*current_min_value))\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    h_pull_count[eps,:]   = temp_pull_count\n",
    "    h_estimation[eps,:]   = temp_estimation\n",
    "    h_reward[eps,:]       = temp_reward\n",
    "    h_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(h_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:56:39.592302Z",
     "start_time": "2019-01-13T08:53:08.806546Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.412 0.141 0.367 0.409 0.588 0.402 0.194 0.325 0.194 0.233 0.214 0.743]\n"
     ]
    }
   ],
   "source": [
    "# i Thompson Sampling (beta) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.beta(a=1+w,b=1+t-w).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:43.572283Z",
     "start_time": "2019-01-13T08:56:40.286696Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.702 0.469 0.715 0.747 0.768 0.701 0.648 0.616 0.503 0.516 0.603 0.885]\n"
     ]
    }
   ],
   "source": [
    "# j Thompson Sampling (uniform) (slow)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        theta_samples = [stats.uniform(w/(t+0.000000001),1-w/(t+0.000000001)).rvs(size=1) for t, w in zip(temp_pull_count, temp_estimation)]\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        current_choice = np.argmax(theta_samples)\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        \n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = theta_samples\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:45.319236Z",
     "start_time": "2019-01-13T08:59:44.170002Z"
    },
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.468 0.073 0.311 0.505 0.543 0.373 0.13  0.27  0.071 0.164 0.08  0.7  ]\n",
      "Scaled \n",
      "[0.487 0.061 0.318 0.527 0.568 0.384 0.123 0.273 0.059 0.159 0.069 0.736]\n"
     ]
    }
   ],
   "source": [
    "# k neural network (with adam)\n",
    "k_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "k_estimation   = np.zeros((num_ep,num_bandit))\n",
    "k_reward       = np.zeros((num_ep,num_iter))\n",
    "k_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "            \n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def d_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    \n",
    "    weights = np.random.randn(num_bandit,1)\n",
    "    moment  = np.zeros_like(weights); \n",
    "    velocity = np.zeros_like(weights);\n",
    "    epsilon  = 1.0 \n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        if np.random.uniform(0,1)>epsilon:\n",
    "            current_choice = np.argmax(weights)\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "        else:\n",
    "            current_choice = np.random.choice(np.arange(num_bandit))\n",
    "            current_input  = np.zeros((1,num_bandit))\n",
    "            current_input[0,current_choice] = 1\n",
    "\n",
    "        layer1 = current_input @ weights\n",
    "        layer1a= sigmoid(layer1)\n",
    "\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + current_reward\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        \n",
    "        # KL Divergence https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "        grad3 = np.log(layer1a+0.0000001) - np.log(temp_estimation[current_choice]/(temp_pull_count[current_choice])+0.0000001)\n",
    "        grad2 = d_sigmoid(layer1)\n",
    "        grad1 = current_input\n",
    "        grad  = grad1.T @ (grad3 * grad2)\n",
    "        \n",
    "        moment   = 0.9*moment + (1-0.9) * grad\n",
    "        velocity = 0.999*velocity + (1-0.999) * grad**2\n",
    "        moment_hat   = moment/(1-0.9)\n",
    "        velocity_hat = velocity/(1-0.999)\n",
    "        weights  = weights - 0.08 * (moment_hat/(np.sqrt(velocity_hat)+1e-8))\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = temp_reward[iter] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        \n",
    "        # Decay the learning rate\n",
    "        epsilon = epsilon * 0.999\n",
    "        \n",
    "    k_pull_count[eps,:]   = temp_pull_count\n",
    "    k_estimation[eps,:]   = np.squeeze(sigmoid(weights))\n",
    "    k_reward[eps,:]       = temp_reward\n",
    "    k_optimal_pull[eps,:] = temp_optimal_pull\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(k_estimation.mean(0))\n",
    "print('Scaled ')\n",
    "print((gt_prob.max()-gt_prob.min())*(k_estimation.mean(0)-k_estimation.mean(0).min())/(k_estimation.mean(0).max()-k_estimation.mean(0).min()) + gt_prob.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:46.606806Z",
     "start_time": "2019-01-13T08:59:46.039910Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "[0.489 0.059 0.366 0.519 0.598 0.431 0.179 0.285 0.071 0.185 0.088 0.736]\n",
      "Expected \n",
      "[0.498 0.062 0.367 0.515 0.609 0.416 0.173 0.272 0.077 0.189 0.088 0.738]\n"
     ]
    }
   ],
   "source": [
    "# l softmax\n",
    "l_pull_count   = np.zeros((num_ep,num_bandit))\n",
    "l_estimation   = np.zeros((num_ep,num_bandit))\n",
    "l_reward       = np.zeros((num_ep,num_iter))\n",
    "l_optimal_pull = np.zeros((num_ep,num_iter))\n",
    "l_regret_total = np.zeros((num_ep,num_iter))\n",
    "                    \n",
    "for eps in range(num_ep):\n",
    "    temp_pull_count   = np.zeros(num_bandit)\n",
    "    temp_estimation   = np.zeros(num_bandit)\n",
    "    temp_reward       = np.zeros(num_iter)\n",
    "    temp_optimal_pull = np.zeros(num_iter)\n",
    "    temp_regret = np.zeros(num_iter)\n",
    "    tempture = 300\n",
    "                    \n",
    "    for iter in range(num_iter):\n",
    "        \n",
    "        # select bandit / get reward /increase count / update estimate\n",
    "        pi  = np.exp(temp_estimation/tempture) / np.sum(np.exp(temp_estimation/tempture))\n",
    "        cdf = np.cumsum(pi)\n",
    "        current_choice = np.where(np.random.uniform(0,1) < cdf)[0][0]\n",
    "        current_reward = 1 if np.random.uniform(0,1) < gt_prob[current_choice] else 0\n",
    "        temp_pull_count[current_choice] = temp_pull_count[current_choice] + 1\n",
    "        temp_estimation[current_choice] = temp_estimation[current_choice] + (1/(temp_pull_count[current_choice]+1)) * (current_reward-temp_estimation[current_choice])\n",
    "\n",
    "        # update reward and optimal choice\n",
    "        temp_reward[iter] = current_reward if iter == 0 else temp_reward[iter-1] + current_reward\n",
    "        temp_optimal_pull[iter] = 1 if current_choice == optimal_choice else 0\n",
    "        temp_regret[iter] = gt_prob[optimal_choice] - gt_prob[current_choice] if iter == 0 else temp_regret[iter-1] + (gt_prob[optimal_choice] - gt_prob[current_choice])\n",
    "        \n",
    "        tempture = tempture * 0.999999\n",
    "        \n",
    "    l_pull_count[eps,:]   = temp_pull_count\n",
    "    l_estimation[eps,:]   = temp_estimation\n",
    "    l_reward[eps,:]       = temp_reward\n",
    "    l_optimal_pull[eps,:] = temp_optimal_pull\n",
    "    l_regret_total[eps,:] = temp_regret\n",
    "        \n",
    "print('Ground Truth')\n",
    "print(gt_prob)\n",
    "print('Expected ')\n",
    "print(l_estimation.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T06:10:36.712012Z",
     "start_time": "2019-01-14T06:10:36.708024Z"
    }
   },
   "outputs": [],
   "source": [
    "# m gradient base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n non stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T08:59:47.235732Z",
     "start_time": "2019-01-13T08:59:47.225357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"e3ef950a-4475-4c9a-8746-7db3cbb41e8d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:50:52.230664Z",
     "start_time": "2019-01-13T01:50:51.987646Z"
    }
   },
   "source": [
    "# Reference \n",
    "1. numpy.set_printoptions  NumPy v1.14 Manual. (2019). Docs.scipy.org. Retrieved 13 January 2019, from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html\n",
    "2. [ Archived Post ] Random Note about Multi-Arm Bandit Problem 2. (2019). Medium. Retrieved 13 January 2019, from https://medium.com/@SeoJaeDuk/archived-post-random-note-about-multi-arm-bandit-problem-2-5c522d1dfbdc\n",
    "3. Vieira, T. (2014). KL-divergence as an objective function  Graduate Descent. Timvieira.github.io. Retrieved 13 January 2019, from https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/\n",
    "4. Some Reinforcement Learning: The Greedy and Explore-Exploit Algorithms for the Multi-Armed Bandit Framework in Python. (2019). Datasciencecentral.com. Retrieved 13 January 2019, from https://www.datasciencecentral.com/profiles/blogs/some-reinforcement-learning-the-greedy-and-explore-exploit\n",
    "5. (2019). Cs.mcgill.ca. Retrieved 13 January 2019, from https://www.cs.mcgill.ca/~vkules/bandits.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
