{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:04:15.917453Z",
     "start_time": "2019-01-09T23:04:15.913470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12798052 0.05257987 0.04168536 0.1013075  0.13220688 0.07774843\n",
      " 0.18022149 0.1258417  0.08837421 0.07205402]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "\n",
    "expected_action_value  = np.random.uniform(0 ,1 , 10)\n",
    "expected_action_value = expected_action_value/expected_action_value.sum()\n",
    "num_epoch = 10000\n",
    "alpha = 0.2\n",
    "beta  = 0.8\n",
    "print(expected_action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:04:16.527038Z",
     "start_time": "2019-01-09T23:04:16.264140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.128 0.053 0.042 0.101 0.132 0.078 0.18  0.126 0.088 0.072]\n",
      "[0.111 0.11  0.111 0.111 0.095 0.107 0.111 0.111 0.111 0.022]\n",
      "[1026.  961.  947.  994. 1030.  978. 1092. 1026.  971.  975.]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Linear Reward Inaction \n",
    "estimated = np.zeros(10)+0.1\n",
    "pull_count= np.zeros(10)\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    current_choice = np.argmax(estimated) if np.random.uniform(0,1) > 0.3 else np.random.choice(10, p=estimated)\n",
    "    pull_count[current_choice] = pull_count[current_choice] + 1\n",
    "    current_reward = 1 if expected_action_value[current_choice]>np.random.uniform(0,1) else 0\n",
    "    \n",
    "    mask = np.zeros(10) \n",
    "    mask[current_choice] = 1\n",
    "\n",
    "    if current_reward == 1:\n",
    "        estimated = (mask) * (estimated + alpha*(1-estimated)) + (1-mask) * ((1-alpha) *estimated)\n",
    "    else:\n",
    "        estimated = (mask) * ((1-beta)*estimated) + (1-mask) * (beta/9.0 + (1-beta)*estimated)\n",
    "\n",
    "print(np.around(expected_action_value,3))\n",
    "print(np.around(estimated,3))\n",
    "print(np.around(pull_count,3))\n",
    "print(estimated.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:05:04.946689Z",
     "start_time": "2019-01-09T23:05:04.514141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.128 0.053 0.042 0.101 0.132 0.078 0.18  0.126 0.088 0.072]\n",
      "[0.1   0.1   0.099 0.1   0.1   0.1   0.102 0.1   0.1   0.1  ]\n",
      "[ 879.  949.  914.  970.  953.  916. 1732.  933.  879.  875.]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Linear Reward Inaction \n",
    "alpha = 0.00001\n",
    "estimated = np.zeros(10)+0.1\n",
    "pull_count= np.zeros(10)\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    current_choice = np.argmax(estimated) if np.random.uniform(0,1) > 0.9 else np.random.choice(10, p=estimated)\n",
    "    pull_count[current_choice] = pull_count[current_choice] + 1\n",
    "    current_reward = 1 if expected_action_value[current_choice]>np.random.uniform(0,1) else 0\n",
    "    \n",
    "    mask = np.zeros(10) \n",
    "    mask[current_choice] = 1\n",
    "    \n",
    "    if current_reward == 1:\n",
    "        estimated = (mask) * (estimated + alpha*(1-estimated)) + (1-mask) * ((1-alpha) *estimated)\n",
    "        \n",
    "    estimated = estimated/estimated.sum()\n",
    "        \n",
    "print(np.around(expected_action_value,3))\n",
    "print(np.around(estimated,3))\n",
    "print(np.around(pull_count,3))\n",
    "print(estimated.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:01:38.519267Z",
     "start_time": "2019-01-09T23:01:38.513811Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:58:08.701888Z",
     "start_time": "2019-01-09T23:58:08.105191Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting LRI - Linear Reward Inaction Algorithm\n",
      "\n",
      "\n",
      "Run: 1, Optimal Decision: q8 with chance: 0.829062 chosen 9164 times. Average reward: 0.82\n",
      "[0.581 0.787 0.709 0.379 0.54  0.607 0.287 0.829 0.78  0.105]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[109, 96, 193, 63, 79, 88, 26, 9164, 156, 26]\n",
      "\n",
      "\n",
      "Starting LRI - Linear Reward Inaction Algorithm\n",
      "\n",
      "\n",
      "Run: 1, Optimal Decision: q7 with chance: 0.931256 chosen 9595 times / 10000\n",
      "[0.724 0.43  0.344 0.877 0.466 0.545 0.931 0.351 0.096 0.249]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[139, 43, 18, 66, 40, 43, 9595, 17, 17, 22]\n"
     ]
    }
   ],
   "source": [
    "import random, datetime, numpy as np, math\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def lrI(alpha):\n",
    "\tprint(\"\\n\\nStarting LRI - Linear Reward Inaction Algorithm\\n\\n\")\n",
    "\tfor j in range(1):\n",
    "\t\tlevers = [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\"]\n",
    "\t\treward_percent = [random.random() for i in range(10)]\n",
    "\t\tlever_count, reward_count  = ([0 for i in range(10)] for i in range(2))\n",
    "\t\t#probability of choosing a lever - sum equal to 1\n",
    "\t\tlever_probability = [0.1 for i in range(10)]\n",
    "\t\toptimal_decision = 0\n",
    "\t\t#randomly choose a lever and set alpha/beta values\n",
    "\t\tlever = random.randint(0, 9)\n",
    "\t\tbeta = 0\n",
    "\t\t#gets lever that is the most optimal (has the highest chance of giving us a reward)\n",
    "\t\toptimal_decision = reward_percent.index(max(reward_percent))\n",
    "\t\ti = 1\n",
    "\t\twhile i <= 10000:\n",
    "\t\t# if reward = 1\n",
    "\t\t\tif random.random() < reward_percent[lever]:\n",
    "\t\t\t\treward_count[lever]+=1\n",
    "\t\t\t\t#update the current lever\n",
    "\t\t\t\tlever_probability[lever] = lever_probability[lever] + alpha * (1- lever_probability[lever])\n",
    "\t\t\t\t#update other levers\n",
    "\t\t\t\tfor x in range(0, len(lever_probability)):\n",
    "\t\t\t\t\tif x is not lever:\n",
    "\t\t\t\t\t\tlever_probability[x] = (1 - alpha) * lever_probability[x]\n",
    "\t\t\tlever_count[lever] += 1\n",
    "\t\t\ti+=1\n",
    "\t\t\t#choose the lever according to the probability distribution lever_probability\n",
    "\t\t\tlever = np.random.choice(10, p=lever_probability)\n",
    "\n",
    "\t\tprint(\"Run: %d, Optimal Decision: %s with chance: %f chosen %d times. Average reward: %.2f\" % (j+1, levers[optimal_decision], reward_percent[optimal_decision], lever_count[optimal_decision], sum(reward_count)/i))\n",
    "\t\tprint(np.around(reward_percent,3))\n",
    "\t\tprint(np.around(lever_probability,3))\n",
    "\t\tprint(lever_count)\n",
    "        \n",
    "def lrI2(alpha):\n",
    "    print(\"\\n\\nStarting LRI - Linear Reward Inaction Algorithm\\n\\n\")\n",
    "    for j in range(1):\n",
    "        levers = [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\"]\n",
    "        reward_percent = [random.random() for i in range(10)]\n",
    "        lever_count, reward_count  = ([0 for i in range(10)] for i in range(2))\n",
    "        lever_probability = np.zeros(10) + 0.1\n",
    "        optimal_decision = 0\n",
    "        lever = random.randint(0, 9)\n",
    "        optimal_decision = reward_percent.index(max(reward_percent))\n",
    "        i = 1\n",
    "        while i <= 10000:\n",
    "            if random.random() < reward_percent[lever]:\n",
    "                reward_count[lever] = reward_count[lever] + 1\n",
    "                mask = np.zeros(10)\n",
    "                mask[lever] = mask[lever] + 1\n",
    "                lever_probability = (mask) * (lever_probability + alpha*(1-lever_probability)) + (1-mask) *((1 - alpha) * lever_probability)\n",
    "\n",
    "            lever_count[lever] += 1; i+=1\n",
    "            lever = np.random.choice(10, p=lever_probability)\n",
    "\n",
    "        print(\"Run: %d, Optimal Decision: %s with chance: %f chosen %d times / 10000\"% \n",
    "              (j+1, levers[optimal_decision], reward_percent[optimal_decision], lever_count[optimal_decision])  )\n",
    "        print(np.around(reward_percent,3))\n",
    "        print(np.around(lever_probability,3))\n",
    "        print(lever_count)\n",
    "        \n",
    "alpha2 =  0.01\n",
    "lrI(alpha2)\n",
    "lrI2(alpha2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Michael Pacheco. (2019). Michaelpacheco.net. Retrieved 9 January 2019, from https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2\n",
    "2. numpy.random.choice â€” NumPy v1.15 Manual. (2019). Docs.scipy.org. Retrieved 9 January 2019, from https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T23:28:19.119522Z",
     "start_time": "2019-01-09T23:28:19.114103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "lever_probability = np.random.uniform(0,1,10)\n",
    "lever_probability = lever_probability/lever_probability.sum()\n",
    "print(np.random.choice(10, p=lever_probability))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
