{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:20:42.901157Z",
     "start_time": "2018-12-10T14:20:39.433450Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread,imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:20:44.640534Z",
     "start_time": "2018-12-10T14:20:42.948026Z"
    },
    "code_folding": [
     0,
     1,
     28
    ]
   },
   "outputs": [],
   "source": [
    "# read all of the data\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "train_images = read_all_images(\"../../../DataSet/STL10/stl10_binary/train_X.bin\") / 255.0\n",
    "train_labels = read_labels    (\"../../../DataSet/STL10/stl10_binary/train_Y.bin\")\n",
    "test_images  = read_all_images(\"../../../DataSet/STL10/stl10_binary/test_X.bin\")  / 255.0\n",
    "test_labels  = read_labels    (\"../../../DataSet/STL10/stl10_binary/test_y.bin\")\n",
    "\n",
    "label_encoder= OneHotEncoder(sparse=False,categories='auto')\n",
    "train_labels = label_encoder.fit_transform(train_labels.reshape((-1,1)))\n",
    "test_labels  = label_encoder.fit_transform(test_labels.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:22:53.320039Z",
     "start_time": "2018-12-10T16:22:38.909546Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.] [0. 0. 0.] [0.44671062 0.43980984 0.40664645] [0.26034098 0.25657727 0.27126738]\n",
      "(5000, 96, 96, 3)\n",
      "(5000, 10)\n",
      "[1. 1. 1.] [0. 0. 0.] [0.44723063 0.43964247 0.40495725] [0.2605645  0.25666146 0.26997382]\n",
      "(8000, 96, 96, 3)\n",
      "(8000, 10)\n"
     ]
    }
   ],
   "source": [
    "# some basic statistic of train and test image // hyper\n",
    "print(train_images.max((0,1,2)),train_images.min((0,1,2)),train_images.mean((0,1,2)),train_images.std((0,1,2)) )\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.max((0,1,2)),test_images.min((0,1,2)),test_images.mean((0,1,2)),test_images.std((0,1,2)) )\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "num_epoch = 50 ; learning_rate = 0.0008; batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:22:53.799730Z",
     "start_time": "2018-12-10T16:22:53.768814Z"
    },
    "code_folding": [
     25,
     60
    ]
   },
   "outputs": [],
   "source": [
    "# import layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_elu(x):     return tf.nn.elu(x)\n",
    "def d_tf_elu(x):   return tf.cast(tf.greater(x,0),tf.float32)  + (tf_elu(tf.cast(tf.less_equal(x,0),tf.float32) * x) + 1.0)\n",
    "def tf_relu(x):    return tf.nn.relu(x)\n",
    "def d_tf_relu(x):  return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_elu,d_act=d_tf_elu):\n",
    "        self.w          = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.b          = tf.Variable(tf.zeros(out,dtype=tf.float32))\n",
    "        self.m,self.v   = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.mb,self.vb = tf.Variable(tf.zeros_like(self.b)),tf.Variable(tf.zeros_like(self.b))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.which_reg  = which_reg\n",
    "        \n",
    "    def getw(self): return [self.w,self.b]\n",
    "\n",
    "    def feedforward(self,input,stride=1,padding='VALID'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "    def backprop(self,gradient,stride=1,padding='VALID'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad_b      = tf.reduce_mean(grad_middle,(0,1,2))/batch_size\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.which_reg == 0:   grad = grad\n",
    "        if self.which_reg == 0.5: grad = grad + lamda * (tf.sqrt(tf.abs(self.w))) * (1.0/tf.sqrt(tf.abs(self.w)+ 10e-5)) * tf.sign(self.w)\n",
    "        if self.which_reg == 1:   grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.which_reg == 1.5: grad = grad + lamda * 1.0/(tf.sqrt(tf.square(self.w) + 10e-5)) * self.w\n",
    "        if self.which_reg == 2:   grad = grad + lamda * (1.0/tf.sqrt(tf.square(tf.abs(self.w))+ 10e-5)) * tf.abs(self.w) * tf.sign(self.w)\n",
    "        if self.which_reg == 2.5: grad = grad + lamda * 2.0 * self.w\n",
    "        if self.which_reg == 3:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),3)+ 10e-5,-0.66) * tf.pow(tf.abs(self.w),2) * tf.sign(self.w)\n",
    "        if self.which_reg == 4:   grad = grad + lamda * tf.pow(tf.pow(tf.abs(self.w),4)+ 10e-5,-0.75) * tf.pow(tf.abs(self.w),3) * tf.sign(self.w)\n",
    "\n",
    "        update_w = []\n",
    "        \n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        \n",
    "        update_w.append(tf.assign( self.mb,self.mb*beta1 + (1-beta1) * (grad_b)   ))\n",
    "        update_w.append(tf.assign( self.vb,self.vb*beta2 + (1-beta2) * (grad_b ** 2)   ))\n",
    "        m_hatb = self.mb / (1-beta1) ; v_hatb = self.vb / (1-beta2)\n",
    "        adam_middleb = m_hatb * learning_rate/(tf.sqrt(v_hatb) + adam_e)\n",
    "        update_w.append(tf.assign(self.b,tf.subtract(self.b,adam_middleb  )))\n",
    "        \n",
    "        return grad_pass,update_w\n",
    "    \n",
    "def tf_pca_svd(X,mmax=0.8,mmin=0.0): \n",
    "    s,U,V  = tf.svd(X,full_matrices=False)\n",
    "    smin = tf.reduce_min(s,0)\n",
    "    smax = tf.reduce_max(s,0)\n",
    "    ScaledS= (mmax-mmin)*((s-smin)/(smax-smin)) + mmin\n",
    "    recon_data = U @ tf.diag(s) @ tf.transpose(V) * tf.reduce_mean(tf.abs(V) * ScaledS[None,:],0,keepdims =True)\n",
    "    return recon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:51:55.277696Z",
     "start_time": "2018-12-10T16:51:54.925511Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# restart the graph \n",
    "# sess.close()\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "num_epoch = 50 ; learning_rate = 0.0008; batch_size = 50 \n",
    "\n",
    "l1 = CNN(3,3, 16)\n",
    "l2 = CNN(3,16,16)\n",
    "l3 = CNN(3,16,16)\n",
    "\n",
    "l4 = CNN(3,16,32)\n",
    "l5 = CNN(3,32,32)\n",
    "l6 = CNN(3,32,32)\n",
    "\n",
    "l7 = CNN(3,32,64)\n",
    "l8 = CNN(1,64,64)\n",
    "l9 = CNN(1,64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:51:56.484521Z",
     "start_time": "2018-12-10T16:51:55.706309Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# build graph \n",
    "def do(layer9,n):\n",
    "    layer9_t = tf.transpose(layer9,(1,2,0,3))\n",
    "    with tf.device('/cpu:0'):\n",
    "        s,U,V = tf.svd(layer9_t)\n",
    "    S         = tf.matrix_diag(s)\n",
    "    layer9_2  = U[:,:,:,:n] @ S[:,:,:n,:n] @ tf.transpose(V,(0,1,3,2))[:,:,:n,:]\n",
    "    layer9_3  = tf.transpose(layer9_2,(2,0,1,3)) \n",
    "    return layer9_3\n",
    "\n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "n1 = tf.placeholder(shape=(),dtype=tf.int32)\n",
    "n2 = tf.placeholder(shape=(),dtype=tf.int32)\n",
    "is_training = tf.placeholder_with_default(input=False,shape=())\n",
    "\n",
    "layer1 = l1.feedforward(x       ,padding='SAME',stride=2)\n",
    "layer2 = l2.feedforward(layer1  ,padding='SAME',stride=1)\n",
    "layer3 = l3.feedforward(layer2  ,padding='SAME',stride=2)\n",
    "layer3 = do(layer3,n1)\n",
    "\n",
    "layer4 = l4.feedforward(layer3  ,padding='SAME',stride=2)\n",
    "layer5 = l5.feedforward(layer4  ,padding='SAME',stride=1)\n",
    "layer6 = l6.feedforward(layer5  ,padding='SAME',stride=2)\n",
    "layer6 = do(layer6,n2)\n",
    " \n",
    "layer7 = l7.feedforward(layer6  ,padding='VALID',stride=1)\n",
    "layer8 = l8.feedforward(layer7  ,padding='VALID',stride=1)\n",
    "layer9 = l9.feedforward(layer8  ,padding='VALID',stride=1)\n",
    "\n",
    "final_layer = tf.reduce_sum(layer9,axis=(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-10T16:51:55.920Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter : 0/50 batch : 7950/8000 acc : 0.46\n",
      " Current : 0 Acc : 0.18219999928027392 Test Acc : 0.2574999997857958\n",
      "\n",
      "Current Iter : 1/50 batch : 7950/8000 acc : 0.22\n",
      " Current : 1 Acc : 0.2907999996095896 Test Acc : 0.30412499979138374\n",
      "\n",
      "Current Iter : 2/50 batch : 7950/8000 acc : 0.36\n",
      " Current : 2 Acc : 0.32439999997615815 Test Acc : 0.32850000113248823\n",
      "\n",
      "Current Iter : 3/50 batch : 7950/8000 acc : 0.26\n",
      " Current : 3 Acc : 0.3651999990642071 Test Acc : 0.35850000139325855\n",
      "\n",
      "Current Iter : 4/50 batch : 7950/8000 acc : 0.34\n",
      " Current : 4 Acc : 0.38400000020861624 Test Acc : 0.37750000124797223\n",
      "\n",
      "Current Iter : 5/50 batch : 7950/8000 acc : 0.34\n",
      " Current : 5 Acc : 0.4131999996304512 Test Acc : 0.39187500104308126\n",
      "\n",
      "Current Iter : 6/50 batch : 7950/8000 acc : 0.38\n",
      " Current : 6 Acc : 0.4328000000119209 Test Acc : 0.39737499998882414\n",
      "\n",
      "Current Iter : 7/50 batch : 7950/8000 acc : 0.42\n",
      " Current : 7 Acc : 0.4467999966442585 Test Acc : 0.41487500024959445\n",
      "\n",
      "Current Iter : 8/50 batch : 7950/8000 acc : 0.42\n",
      " Current : 8 Acc : 0.465799997150898 Test Acc : 0.41275000022724273\n",
      "\n",
      "Current Iter : 9/50 batch : 7950/8000 acc : 0.36\n",
      " Current : 9 Acc : 0.48759999722242353 Test Acc : 0.4187499997206032\n",
      "\n",
      "Current Iter : 10/50 batch : 7950/8000 acc : 0.36\n",
      " Current : 10 Acc : 0.5048000013828278 Test Acc : 0.42299999967217444\n",
      "\n",
      "Current Iter : 11/50 batch : 5300/8000 acc : 0.42\r"
     ]
    }
   ],
   "source": [
    "# start the training \n",
    "# sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "avg_acc_train = 0; avg_acc_test  = 0\n",
    "\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    train_images,train_labels = shuffle(train_images,train_labels); test_images,test_labels   = shuffle(test_images,test_labels)\n",
    "    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label,n1:8,n2:16})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images), batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,n1:16,n2:32})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 ; avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# build graph (batch norm)\n",
    "def do(layer9,num=10):\n",
    "    with tf.device('/cpu:0'):\n",
    "        s,U,V = tf.svd(layer9)\n",
    "    S      = tf.matrix_diag(s)\n",
    "    layer9 = U @ S @ tf.transpose(V,(0,1,3,2))[:,:,:,:num] \n",
    "    return layer9\n",
    "\n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "is_training = tf.placeholder_with_default(input=False,shape=())\n",
    "\n",
    "layer1 = l1.feedforward(x,     padding='SAME',stride=1)\n",
    "layer2 = l2.feedforward(layer1,padding='SAME',stride=1)\n",
    "layer3 = l3.feedforward(layer2,padding='SAME',stride=2)\n",
    "# layer3 = tf.layers.batch_normalization(layer3,training=is_training)\n",
    "\n",
    "layer4 = l4.feedforward(layer3,padding='SAME',stride=1)\n",
    "layer5 = l5.feedforward(layer4,padding='SAME',stride=1)\n",
    "layer6 = l6.feedforward(layer5,padding='SAME',stride=2)\n",
    "# layer6 = tf.layers.batch_normalization(layer6,training=is_training)\n",
    "\n",
    "# layer6_t = tf.transpose(layer6,(1,2,0,3))\n",
    "# with tf.device('/cpu:0'):\n",
    "#     s,U,V = tf.svd(layer6_t)\n",
    "# S        = tf.matrix_diag(s)\n",
    "# layer6_2 = U @ S @ tf.transpose(V,(0,1,3,2))[:,:,:,:64]\n",
    "# layer6_3 = tf.transpose(layer6_2,(2,0,1,3))\n",
    "# print(U,S,V)\n",
    "# print(layer6_2)\n",
    "# print(layer6_3)\n",
    "\n",
    "layer7 = l7.feedforward(layer6,padding='VALID',stride=1)\n",
    "layer8 = l8.feedforward(layer7,padding='VALID',stride=1)\n",
    "layer9 = l9.feedforward(layer8,padding='VALID',stride=1)\n",
    "\n",
    "final_layer = tf.reduce_mean(layer9,axis=(1,2))\n",
    "cost        = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "# update_ops  = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "auto_train  = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:17:25.431311Z",
     "start_time": "2018-12-10T14:17:14.977Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# build graph \n",
    "x = tf.placeholder(shape=(batch_size,96,96,3),dtype=tf.float32)\n",
    "y = tf.placeholder(shape=(batch_size,10),dtype=tf.float32)\n",
    "is_training = tf.placeholder_with_default(input=False,shape=())\n",
    "\n",
    "layer1 = l1.feedforward(x,padding='VALID',stride=2)\n",
    "layer2 = l2.feedforward(layer1,padding='VALID',stride=2)\n",
    "layer3 = l3.feedforward(layer2,padding='VALID',stride=2)\n",
    "\n",
    "layer4 = l4.feedforward(layer3,padding='VALID',stride=2)\n",
    "layer5 = l5.feedforward(layer4,padding='VALID')\n",
    "layer6 = l6.feedforward(layer5,padding='VALID',stride=2)\n",
    "\n",
    "layer7 = l7.feedforward(layer6,padding='VALID')\n",
    "layer8 = l8.feedforward(layer7,padding='VALID')\n",
    "layer9 = l9.feedforward(layer8,padding='VALID')\n",
    "final_layer   = tf.reduce_mean(layer9,axis=(1,2))\n",
    "\n",
    "cost       = tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer,labels=y)\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "final_softmax      = tf_softmax(final_layer)\n",
    "correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T14:17:25.433341Z",
     "start_time": "2018-12-10T14:17:14.981Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start the training \n",
    "# sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "avg_acc_train = 0\n",
    "avg_acc_test  = 0\n",
    "for iter in range(num_epoch):\n",
    "    \n",
    "    train_images,train_labels = shuffle(train_images,train_labels)\n",
    "    test_images,test_labels   = shuffle(test_images,test_labels)\n",
    "    \n",
    "    for current_batch_index in range(0,len(train_images),batch_size):\n",
    "        current_data  = train_images[current_batch_index:current_batch_index+batch_size]\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size]\n",
    "        sess_results  = sess.run([accuracy,auto_train],feed_dict={x:current_data,y:current_label,is_training:True})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  +\n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + \n",
    "                         ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "        avg_acc_train = avg_acc_train + sess_results[0]\n",
    "        \n",
    "    for current_batch_index in range(0,len(test_images),batch_size):\n",
    "        current_data  = test_images[current_batch_index:current_batch_index+batch_size]\n",
    "        current_label = test_labels[current_batch_index:current_batch_index+batch_size]\n",
    "        sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_training:False})\n",
    "        sys.stdout.write('Current Iter : ' + str(iter) + '/'+ str(num_epoch)  +\n",
    "                         ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + \n",
    "                         ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "        avg_acc_test = avg_acc_test + sess_results[0]        \n",
    "        \n",
    "    print(\"\\n Current : \"+ str(iter) + \" Acc : \" + str(avg_acc_train/(len(train_images)/batch_size)) + \" Test Acc : \" + str(avg_acc_test/(len(test_images)/batch_size)) + '\\n')\n",
    "    avg_acc_train = 0 \n",
    "    avg_acc_test  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T16:56:07.568679Z",
     "start_time": "2018-12-09T16:56:07.550727Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "1. Brownlee, J. (2017). How to One Hot Encode Sequence Data in Python. Machine Learning Mastery. Retrieved 9 December 2018, from https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "2. tf.placeholder_with_default | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default\n",
    "3. tf.nn.softmax_cross_entropy_with_logits | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "4. line, O. (2018). Output without new line. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2623470/output-without-new-line\n",
    "5. shell?, H. (2018). How to tell if tensorflow is using gpu acceleration from inside python shell?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell\n",
    "6. GPU?, H. (2018). How to use TensorFlow GPU?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/51306862/how-to-use-tensorflow-gpu\n",
    "7. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "8. Batch normalization: theory and how to use it with Tensorflow. (2018). Towards Data Science. Retrieved 9 December 2018, from https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "9. tf.reset_default_graph | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/reset_default_graph\n",
    "10. tf.Session | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/Session\n",
    "11. tf.nn.moments | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "12. CMD?, H. (2018). How do I run two commands in one line in Windows CMD?. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/8055371/how-do-i-run-two-commands-in-one-line-in-windows-cmd\n",
    "13. loop, B. (2018). Batch script loop. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/2591758/batch-script-loop\n",
    "14. tf.train.MomentumOptimizer | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\n",
    "15. Test if two numpy arrays are (close to) equal, i. (2018). Test if two numpy arrays are (close to) equal, including shape. Stack Overflow. Retrieved 9 December 2018, from https://stackoverflow.com/questions/32874840/test-if-two-numpy-arrays-are-close-to-equal-including-shape\n",
    "16. tf.linalg.diag | TensorFlow. (2018). TensorFlow. Retrieved 9 December 2018, from https://www.tensorflow.org/api_docs/python/tf/linalg/diag\n",
    "17. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "18. tf.layers.batch_normalization | TensorFlow. (2018). TensorFlow. Retrieved 10 December 2018, from https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "19. error, t. (2018). tf.layers.batch_normalization large test error. Stack Overflow. Retrieved 10 December 2018, from https://stackoverflow.com/questions/43234667/tf-layers-batch-normalization-large-test-error\n",
    "20. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T15:52:23.075289Z",
     "start_time": "2018-12-10T15:52:21.780293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatrixDiag_54:0\", shape=(100, 32, 3, 3), dtype=float64)\n",
      "Tensor(\"Svd_54:1\", shape=(100, 32, 32, 3), dtype=float64) Tensor(\"Svd_54:0\", shape=(100, 32, 3), dtype=float64) Tensor(\"Svd_54:2\", shape=(100, 32, 3, 3), dtype=float64)\n",
      "Tensor(\"Diag:0\", shape=(100, 32, 3, 100, 32, 3), dtype=float64)\n",
      "[[[[6.55823893 0.         0.        ]\n",
      "   [0.         5.56346808 0.        ]\n",
      "   [0.         0.         4.8413144 ]]\n",
      "\n",
      "  [[7.25037232 0.         0.        ]\n",
      "   [0.         5.49028313 0.        ]\n",
      "   [0.         0.         4.46036365]]\n",
      "\n",
      "  [[7.15437884 0.         0.        ]\n",
      "   [0.         5.90717244 0.        ]\n",
      "   [0.         0.         4.85589987]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.54711788 0.         0.        ]\n",
      "   [0.         5.23255879 0.        ]\n",
      "   [0.         0.         4.90220369]]\n",
      "\n",
      "  [[6.50449194 0.         0.        ]\n",
      "   [0.         5.96788598 0.        ]\n",
      "   [0.         0.         4.59075861]]\n",
      "\n",
      "  [[6.98222117 0.         0.        ]\n",
      "   [0.         5.18923976 0.        ]\n",
      "   [0.         0.         4.57867129]]]\n",
      "\n",
      "\n",
      " [[[6.65563915 0.         0.        ]\n",
      "   [0.         5.41452507 0.        ]\n",
      "   [0.         0.         4.19351541]]\n",
      "\n",
      "  [[7.04990175 0.         0.        ]\n",
      "   [0.         5.73566958 0.        ]\n",
      "   [0.         0.         4.94013733]]\n",
      "\n",
      "  [[6.59232613 0.         0.        ]\n",
      "   [0.         5.1820523  0.        ]\n",
      "   [0.         0.         4.56623391]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.99361756 0.         0.        ]\n",
      "   [0.         4.76220651 0.        ]\n",
      "   [0.         0.         3.78501403]]\n",
      "\n",
      "  [[6.70257066 0.         0.        ]\n",
      "   [0.         5.04106367 0.        ]\n",
      "   [0.         0.         3.62371423]]\n",
      "\n",
      "  [[6.44436126 0.         0.        ]\n",
      "   [0.         6.09610488 0.        ]\n",
      "   [0.         0.         4.23362188]]]\n",
      "\n",
      "\n",
      " [[[6.0584267  0.         0.        ]\n",
      "   [0.         5.21255356 0.        ]\n",
      "   [0.         0.         4.84853997]]\n",
      "\n",
      "  [[6.4555343  0.         0.        ]\n",
      "   [0.         5.5466113  0.        ]\n",
      "   [0.         0.         5.37977099]]\n",
      "\n",
      "  [[6.57745782 0.         0.        ]\n",
      "   [0.         4.88388699 0.        ]\n",
      "   [0.         0.         4.52899489]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.71141229 0.         0.        ]\n",
      "   [0.         5.08802657 0.        ]\n",
      "   [0.         0.         4.46674932]]\n",
      "\n",
      "  [[7.4053751  0.         0.        ]\n",
      "   [0.         5.47949032 0.        ]\n",
      "   [0.         0.         4.62222148]]\n",
      "\n",
      "  [[6.88954481 0.         0.        ]\n",
      "   [0.         5.75814449 0.        ]\n",
      "   [0.         0.         5.22155675]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[6.3636869  0.         0.        ]\n",
      "   [0.         5.42808712 0.        ]\n",
      "   [0.         0.         4.36875603]]\n",
      "\n",
      "  [[7.64153839 0.         0.        ]\n",
      "   [0.         5.85689381 0.        ]\n",
      "   [0.         0.         4.79103845]]\n",
      "\n",
      "  [[7.38098974 0.         0.        ]\n",
      "   [0.         5.62608207 0.        ]\n",
      "   [0.         0.         4.92903158]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.05271701 0.         0.        ]\n",
      "   [0.         5.67676784 0.        ]\n",
      "   [0.         0.         4.09923391]]\n",
      "\n",
      "  [[7.40547838 0.         0.        ]\n",
      "   [0.         5.87293664 0.        ]\n",
      "   [0.         0.         5.51400779]]\n",
      "\n",
      "  [[6.24778885 0.         0.        ]\n",
      "   [0.         5.38525575 0.        ]\n",
      "   [0.         0.         4.24792237]]]\n",
      "\n",
      "\n",
      " [[[6.68074436 0.         0.        ]\n",
      "   [0.         5.43293724 0.        ]\n",
      "   [0.         0.         4.46435496]]\n",
      "\n",
      "  [[6.89766143 0.         0.        ]\n",
      "   [0.         6.08170519 0.        ]\n",
      "   [0.         0.         4.68046135]]\n",
      "\n",
      "  [[6.26705532 0.         0.        ]\n",
      "   [0.         5.73080523 0.        ]\n",
      "   [0.         0.         4.73913252]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.33614622 0.         0.        ]\n",
      "   [0.         5.08085514 0.        ]\n",
      "   [0.         0.         4.70218318]]\n",
      "\n",
      "  [[6.63657168 0.         0.        ]\n",
      "   [0.         5.23492603 0.        ]\n",
      "   [0.         0.         4.80648337]]\n",
      "\n",
      "  [[5.73857265 0.         0.        ]\n",
      "   [0.         5.27168151 0.        ]\n",
      "   [0.         0.         4.16750616]]]\n",
      "\n",
      "\n",
      " [[[6.1818294  0.         0.        ]\n",
      "   [0.         5.42844189 0.        ]\n",
      "   [0.         0.         5.01108706]]\n",
      "\n",
      "  [[6.32672751 0.         0.        ]\n",
      "   [0.         5.93742272 0.        ]\n",
      "   [0.         0.         5.25498651]]\n",
      "\n",
      "  [[6.41676669 0.         0.        ]\n",
      "   [0.         5.65869776 0.        ]\n",
      "   [0.         0.         4.61444676]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.05626462 0.         0.        ]\n",
      "   [0.         5.43656193 0.        ]\n",
      "   [0.         0.         3.97818298]]\n",
      "\n",
      "  [[7.0781321  0.         0.        ]\n",
      "   [0.         4.84326143 0.        ]\n",
      "   [0.         0.         4.31987294]]\n",
      "\n",
      "  [[6.65490105 0.         0.        ]\n",
      "   [0.         5.76119643 0.        ]\n",
      "   [0.         0.         5.55471995]]]]\n",
      "None\n",
      "True\n",
      "19489\n",
      "307200\n"
     ]
    }
   ],
   "source": [
    "temp   = np.random.randn(100,32,32,3)\n",
    "s,U,V  = tf.svd(temp)\n",
    "S = tf.matrix_diag(s)\n",
    "print(S)\n",
    "print(U,s,V)\n",
    "print(tf.diag(s))\n",
    "\n",
    "result = U @ S @ tf.transpose(V,(0,1,3,2))\n",
    "print(S.eval())\n",
    "result = result.eval()\n",
    "\n",
    "print(np.testing.assert_allclose(temp, result))\n",
    "print(np.allclose(temp,result))\n",
    "print(np.equal(temp,result).sum())\n",
    "print(100*32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
