{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T16:59:41.361497Z",
     "start_time": "2019-01-24T16:59:37.705002Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Things to compare</h1>\n",
    "<div>\n",
    "<ol>\n",
    "  <li>Z: Baseline </li>\n",
    "  <li>A: abs(Theta)  </li>\n",
    "  <li>B: Theta ^ 2</li>\n",
    "  <li>C: sqrt(Theta ^ 2) </li>\n",
    "  <li>D: - log(1+Theta ^ 2)</li>\n",
    "  <li>E: - tanh(Theta) </li>\n",
    "  <li>F: - tanh(Theta^2)</li>\n",
    "  <li>G: - tanh(abs(Theta))</li>\n",
    "  <li>H: - tanh(abs(Theta)^2)</li>\n",
    "  <li>I:   sin(Theta)</li>\n",
    "  <li>J:   abs(sin(Theta))</li>\n",
    "  <li>K:   log(Theta^2)</li>\n",
    "  <li>L:   Theta * log(Theta^2)</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<ol>\n",
    "  <li>CNN - no batch </li>\n",
    "  <li>CNN - batch </li>\n",
    "  <li>FNN - no batch </li>\n",
    "  <li>FNN - batch </li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T16:59:41.378372Z",
     "start_time": "2019-01-24T16:59:41.364610Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "# CIFAR\n",
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T16:59:42.454186Z",
     "start_time": "2019-01-24T16:59:41.380346Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) 1.0 0.0\n",
      "(50000, 10) 1.0 0.0\n",
      "(10000, 32, 32, 3) 1.0 0.0\n",
      "(10000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "PathDicom = \"../../../Dataset/cifar-10-batches-py/\"\n",
    "lstFilesDCM = []  # create an empty list\n",
    "for dirName, subdirList, fileList in os.walk(PathDicom):\n",
    "    for filename in fileList:\n",
    "        if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
    "            lstFilesDCM.append(os.path.join(dirName,filename))\n",
    "\n",
    "# Read the data traind and Test\n",
    "batch0 = unpickle(lstFilesDCM[0])\n",
    "batch1 = unpickle(lstFilesDCM[1])\n",
    "batch2 = unpickle(lstFilesDCM[2])\n",
    "batch3 = unpickle(lstFilesDCM[3])\n",
    "batch4 = unpickle(lstFilesDCM[4])\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=True)\n",
    "train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
    "train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float32)\n",
    "train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float32)\n",
    "\n",
    "test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
    "test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float32)\n",
    "test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float32)\n",
    "\n",
    "# reshape data\n",
    "train_batch = np.reshape(train_batch,(len(train_batch),3,32,32))\n",
    "test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
    "\n",
    "# rotate data\n",
    "train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float32)\n",
    "test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float32)\n",
    "\n",
    "train_images = train_batch/255.0\n",
    "train_labels = train_label\n",
    "test_images  = test_batch/255.0\n",
    "test_labels  = test_label\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T16:59:42.595096Z",
     "start_time": "2019-01-24T16:59:42.457658Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.regularizer    = which_reg\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer, self.layerA\n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.regularizer == 'A': grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.regularizer == 'B': grad = grad + lamda * 2.0 * self.w\n",
    "        if self.regularizer == 'C': grad = grad + lamda * (1.0/tf.sqrt(tf.square(self.w)+ 10e-8)) * self.w\n",
    "        if self.regularizer == 'D': grad = grad + lamda * -(2*self.w)/(1 + self.w**2)\n",
    "        if self.regularizer == 'E': grad = grad + lamda * -(1-tf.tanh(self.w) ** 2)\n",
    "        if self.regularizer == 'F': grad = grad + lamda * -(1-tf.tanh(self.w** 2) ** 2) * 2.0 * self.w \n",
    "        if self.regularizer == 'G': grad = grad + lamda * -(1-tf.tanh(tf.abs(self.w)) ** 2) * tf.sign(self.w)\n",
    "        if self.regularizer == 'H': grad = grad + lamda * -(1-tf.tanh(tf.abs(self.w)** 2) ** 2) * 2.0 * tf.abs(self.w) *  tf.sign(self.w)\n",
    "        if self.regularizer == 'I': grad = grad + lamda * tf.cos(self.w)\n",
    "        if self.regularizer == 'J': grad = grad + lamda * tf.sign(tf.sin(self.w)) * tf.cos(self.w)\n",
    "        if self.regularizer == 'K': grad = grad + lamda * (2)/(self.w + 10e-8)\n",
    "        if self.regularizer == 'L': grad = grad + lamda * (tf.log(self.w**2) + 2.0)\n",
    "        \n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "def save_to_image(data,name):\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = data\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = np.asarray(l1g),np.asarray(l2g),np.asarray(l3g),np.asarray(l4g),np.asarray(l5g),np.asarray(l6g)\n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1g.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2g.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3g.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4g.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5g.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6g.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig(name + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')     \n",
    "def append_stat(current_list,data,number):\n",
    "    current_list[0].append(data[number].mean())\n",
    "    current_list[1].append(data[number].std())\n",
    "    current_list[2].append(skew    (data[number].ravel()))\n",
    "    current_list[3].append(kurtosis(data[number].ravel()))\n",
    "    current_list[4].append(np.count_nonzero(data[number]))\n",
    "    return current_list\n",
    "def transform_to_2d(data):\n",
    "    batch,width,height,chan = data.shape\n",
    "    return data.reshape((batch*width,height*chan))\n",
    "def save_to_image(main_data,one,two,three,four,five,six,experiment_name,tran_acc,test_acc,current_exp,iter):\n",
    "    plt.figure(figsize=(20,40))\n",
    "    G = gridspec.GridSpec(8, 6)\n",
    "\n",
    "    plt.figtext(0.5,1.0,\"Iter: \" + str(iter) + \" Histogram Per \" + experiment_name,ha=\"center\", va=\"top\", fontsize=35, color=\"black\")\n",
    "    plt.subplot(G[0, 0]).hist(main_data[0].ravel(),50,color='red');       plt.subplot(G[0, 0]).set_title(experiment_name+' 1')\n",
    "    plt.subplot(G[0, 1]).hist(main_data[1].ravel(),50,color='orange');    plt.subplot(G[0, 1]).set_title(experiment_name+' 2')\n",
    "    plt.subplot(G[0, 2]).hist(main_data[2].ravel(),50,color='yellow');  plt.subplot(G[0, 2]).set_title(experiment_name+' 3')\n",
    "    plt.subplot(G[0, 3]).hist(main_data[3].ravel(),50,color='green');    plt.subplot(G[0, 3]).set_title(experiment_name+' 4')\n",
    "    plt.subplot(G[0, 4]).hist(main_data[4].ravel(),50,color='blue');     plt.subplot(G[0, 4]).set_title(experiment_name+' 5')\n",
    "    plt.subplot(G[0, 5]).hist(main_data[5].ravel(),50,color='black');     plt.subplot(G[0, 5]).set_title(experiment_name+' 6')\n",
    "\n",
    "    plt.subplot(G[1, :]).set_title(\"Mean Per \"+ experiment_name)\n",
    "    plt.subplot(G[1, :]).plot(one[0]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[1, :]).plot(two[0]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[1, :]).plot(three[0],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[1, :]).plot(four[0],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[1, :]).plot(five[0],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[1, :]).plot(six[0],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[2, :]).set_title(\"Standard Deviation Per \"+ experiment_name)\n",
    "    plt.subplot(G[2, :]).plot(one[1]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[2, :]).plot(two[1]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[2, :]).plot(three[1],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[2, :]).plot(four[1],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[2, :]).plot(five[1],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[2, :]).plot(six[1],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[3, :]).set_title(\"Skewness Per \"+ experiment_name)\n",
    "    plt.subplot(G[3, :]).plot(one[2]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[3, :]).plot(two[2]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[3, :]).plot(three[2],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[3, :]).plot(four[2],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[3, :]).plot(five[2],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[3, :]).plot(six[2],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[4, :]).set_title(\"Kurtosis Per \"+ experiment_name)\n",
    "    plt.subplot(G[4, :]).plot(one[3]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[4, :]).plot(two[3]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[4, :]).plot(three[3],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[4, :]).plot(four[3],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[4, :]).plot(five[3],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[4, :]).plot(six[3],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[5, :]).set_title(\"# Non-Zero Per \"+ experiment_name)\n",
    "    plt.subplot(G[5, :]).plot(one[4]  ,c='red',alpha=0.9   ,label='1')\n",
    "    plt.subplot(G[5, :]).plot(two[4]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[5, :]).plot(three[4],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[5, :]).plot(four[4],c='green',alpha=0.9  ,label='4')\n",
    "    plt.subplot(G[5, :]).plot(five[4],c='blue',alpha=0.9   ,label='5')\n",
    "    plt.subplot(G[5, :]).plot(six[4],c='black',alpha=0.9   ,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[6, :]).set_title(\"Train/Test accuracy\")\n",
    "    plt.subplot(G[6, :]).plot(train_acc  ,c='red',alpha=0.9, label='Train')\n",
    "    plt.subplot(G[6, :]).plot(test_acc   ,c='blue',alpha=0.9,label='Test')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figtext(0.5,0,\"Correlation Matrix Per \"+ experiment_name,ha=\"center\", va=\"bottom\", fontsize=30, color=\"black\")\n",
    "    plt.subplot(G[7, 0]).imshow(np.corrcoef(transform_to_2d(main_data[0])),cmap='gray')\n",
    "    plt.subplot(G[7, 1]).imshow(np.corrcoef(transform_to_2d(main_data[1])),cmap='gray')\n",
    "    plt.subplot(G[7, 2]).imshow(np.corrcoef(transform_to_2d(main_data[2])),cmap='gray')\n",
    "    plt.subplot(G[7, 3]).imshow(np.corrcoef(transform_to_2d(main_data[3])),cmap='gray')\n",
    "    plt.subplot(G[7, 4]).imshow(np.corrcoef(transform_to_2d(main_data[4])),cmap='gray')\n",
    "    plt.subplot(G[7, 5]).imshow(np.corrcoef(transform_to_2d(main_data[5])),cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(current_exp + '/' + experiment_name + '/' + str(iter) + '.png')\n",
    "    plt.close('all')\n",
    "def plot_rotation_weight(current_layers,current_layer_number,current_batch_norm_type,current_exp_name): \n",
    "\n",
    "    def rotate(angle):ax.view_init(azim=angle)\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    colors = ['red','orange','yellow','green','blue','purple','black','gold','silver','cyan','pink']\n",
    "    number_of_eps = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax  = fig.add_subplot(111, projection='3d')\n",
    "    count = 0\n",
    "    for episode in range(num_eps+1):\n",
    "        if episode == num_eps:\n",
    "            ys   = current_layers.mean(0).flatten()\n",
    "            xmin = ys.min(); xmax = ys.max(); step = 0.005\n",
    "            hist,bins = np.histogram(ys, bins=np.linspace(xmin, xmax, (xmax-xmin)/step))\n",
    "            ax.bar(bins[:-1], hist, width=0.01,zs=episode, zdir='y', color=colors[episode], alpha=0.8)\n",
    "        else:\n",
    "            ys   = current_layers[episode].flatten()\n",
    "            xmin = ys.min(); xmax = ys.max(); step = 0.005\n",
    "            hist,bins = np.histogram(ys, bins=np.linspace(xmin, xmax, (xmax-xmin)/step))\n",
    "            ax.bar(bins[:-1], hist, width=0.01,zs=episode, zdir='y', color=colors[episode], alpha=0.4)\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Episode')\n",
    "    ax.get_yaxis().set_ticks(np.arange(num_eps+1))\n",
    "    ax.set_zlabel('Histogram')\n",
    "\n",
    "    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "    # make the grid lines transparent\n",
    "    ax.xaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    ax.yaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    # ax.zaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    animation.FuncAnimation(fig, rotate, frames=np.arange(0,362,2),interval=100) \\\n",
    "    .save(str(current_batch_norm_type)+'/'+str(current_exp_name)+'/weight_'+str(current_layer_number)+'.gif', dpi=80, writer='imagemagick')\n",
    "    plt.close('all')\n",
    "def plot_image_weight(current_layers,current_layer_number,current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy):\n",
    "    colors = ['red','orange','yellow','green','blue','purple','black','gold','silver','cyan','pink']\n",
    "    def rt(number): return np.around(number,4)\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    plt.rcParams.update({'font.size': 50})\n",
    "\n",
    "    fig.add_subplot(341); plt.hist(current_layers[0].flatten(),50,color=colors[0],alpha=0.8);plt.title('EPS: 1 Mean: '+str(rt(current_layers[0].mean())) +' STD: '+str(rt(current_layers[0].std())))\n",
    "    fig.add_subplot(342); plt.hist(current_layers[1].flatten(),50,color=colors[1],alpha=0.8);plt.title('EPS: 2 Mean: '+str(rt(current_layers[1].mean())) +' STD: '+str(rt(current_layers[1].std())))\n",
    "    fig.add_subplot(343); plt.hist(current_layers[2].flatten(),50,color=colors[2],alpha=0.8);plt.title('EPS: 3 Mean: '+str(rt(current_layers[2].mean())) +' STD: '+str(rt(current_layers[2].std())))\n",
    "    fig.add_subplot(344); plt.hist(current_layers[3].flatten(),50,color=colors[3],alpha=0.8);plt.title('EPS: 4 Mean: '+str(rt(current_layers[3].mean())) +' STD: '+str(rt(current_layers[3].std())))\n",
    "    \n",
    "    fig.add_subplot(345); plt.hist(current_layers[4].flatten(),50,color=colors[4],alpha=0.8);plt.title('EPS: 5 Mean: '+str(rt(current_layers[4].mean())) +' STD: '+str(rt(current_layers[4].std())))\n",
    "    fig.add_subplot(346); plt.hist(current_layers[5].flatten(),50,color=colors[5],alpha=0.8);plt.title('EPS: 6 Mean: '+str(rt(current_layers[5].mean())) +' STD: '+str(rt(current_layers[5].std())))\n",
    "    fig.add_subplot(347); plt.hist(current_layers[6].flatten(),50,color=colors[6],alpha=0.8);plt.title('EPS: 7 Mean: '+str(rt(current_layers[6].mean())) +' STD: '+str(rt(current_layers[6].std())))\n",
    "    fig.add_subplot(348); plt.hist(current_layers[7].flatten(),50,color=colors[7],alpha=0.8);plt.title('EPS: 8 Mean: '+str(rt(current_layers[7].mean())) +' STD: '+str(rt(current_layers[7].std())))\n",
    "    \n",
    "    fig.add_subplot(3,4,9);  plt.hist(current_layers[8].flatten(),50,color=colors[8],alpha=0.8);plt.title('EPS: 9 Mean: '+str(rt(current_layers[8].mean())) +' STD: '+str(rt(current_layers[8].std())))\n",
    "    fig.add_subplot(3,4,10); plt.hist(current_layers[9].flatten(),50,color=colors[9],alpha=0.8);plt.title('EPS: 10 Mean: '+str(rt(current_layers[9].mean())) +' STD: '+str(rt(current_layers[9].std())))\n",
    "    fig.add_subplot(3,4,11); plt.hist(current_layers.mean(0).flatten(),50,color=colors[10],alpha=0.8);plt.title('EPS: All Mean: '+str(rt(current_layers.mean(0).mean())) +' STD: '+str(rt(current_layers.mean(0).std())))\n",
    "    fig.add_subplot(3,4,12); \n",
    "    plt.plot(current_exp_train_accuracy.max(0),  ' ' ,color='red')\n",
    "    plt.plot(current_exp_train_accuracy.mean(0), '-' ,color='red',label='train mean')\n",
    "    plt.plot(current_exp_train_accuracy.min(0) , ' ' ,color='red')\n",
    "    plt.fill_between(range(num_epoch),current_exp_train_accuracy.max(0),current_exp_train_accuracy.min(0),facecolor='red', alpha=0.2)\n",
    "\n",
    "    plt.plot(current_exp_test_accuracy.max(0),  ' ' ,color='blue')\n",
    "    plt.plot(current_exp_test_accuracy.mean(0), '-' ,color='blue',label='test mean')\n",
    "    plt.plot(current_exp_test_accuracy.min(0) , ' ' ,color='blue')\n",
    "    plt.fill_between(range(num_epoch),current_exp_test_accuracy.max(0),current_exp_test_accuracy.min(0),facecolor='blue', alpha=0.2)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(current_batch_norm_type)+'/'+str(current_exp_name)+'/weight_'+str(current_layer_number)+'.png')\n",
    "    plt.close('all')\n",
    "def plot_full_weight(current_weights_layer1,current_weights_layer2,current_weights_layer3,\n",
    "                     current_weights_layer4,current_weights_layer5,current_weights_layer6,\n",
    "                     current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy):\n",
    "    \n",
    "    colors = ['red','orange','yellow','green','blue','purple','black','gold','silver','cyan','pink']\n",
    "    def rt(number): return np.around(number,4)\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    plt.rcParams.update({'font.size': 50})\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    \n",
    "    fig.add_subplot(331); plt.hist(current_weights_layer1.mean(0).flatten(),50,color=colors[0],alpha=0.8); plt.title('Layer 1 Histogram Mean :'+str(rt(current_weights_layer1.mean(0).mean()))+' STD: '+str(rt(current_weights_layer1.mean(0).std())))\n",
    "    fig.add_subplot(332); plt.hist(current_weights_layer2.mean(0).flatten(),50,color=colors[1],alpha=0.8); plt.title('Layer 2 Histogram Mean :'+str(rt(current_weights_layer2.mean(0).mean()))+' STD: '+str(rt(current_weights_layer2.mean(0).std())))\n",
    "    fig.add_subplot(333); plt.hist(current_weights_layer3.mean(0).flatten(),50,color=colors[2],alpha=0.8); plt.title('Layer 3 Histogram Mean :'+str(rt(current_weights_layer3.mean(0).mean()))+' STD: '+str(rt(current_weights_layer3.mean(0).std())))\n",
    "    \n",
    "    fig.add_subplot(334); plt.hist(current_weights_layer4.mean(0).flatten(),50,color=colors[3],alpha=0.8); plt.title('Layer 4 Histogram Mean :'+str(rt(current_weights_layer4.mean(0).mean()))+' STD: '+str(rt(current_weights_layer4.mean(0).std())))\n",
    "    fig.add_subplot(335); plt.hist(current_weights_layer5.mean(0).flatten(),50,color=colors[4],alpha=0.8); plt.title('Layer 5 Histogram Mean :'+str(rt(current_weights_layer5.mean(0).mean()))+' STD: '+str(rt(current_weights_layer5.mean(0).std())))\n",
    "    fig.add_subplot(336); plt.hist(current_weights_layer6.mean(0).flatten(),50,color=colors[5],alpha=0.8); plt.title('Layer 6 Histogram Mean :'+str(rt(current_weights_layer6.mean(0).mean()))+' STD: '+str(rt(current_weights_layer6.mean(0).std())))\n",
    "    \n",
    "    fig.add_subplot(gs[2,:]); \n",
    "    plt.plot(current_exp_train_accuracy.max(0),  ' ' ,color='red')\n",
    "    plt.plot(current_exp_train_accuracy.mean(0), '-' ,color='red',label='train mean')\n",
    "    plt.plot(current_exp_train_accuracy.min(0) , ' ' ,color='red')\n",
    "    plt.fill_between(range(num_epoch),current_exp_train_accuracy.max(0),current_exp_train_accuracy.min(0),facecolor='red', alpha=0.2)\n",
    "\n",
    "    plt.plot(current_exp_test_accuracy.max(0),  ' ' ,color='blue')\n",
    "    plt.plot(current_exp_test_accuracy.mean(0), '-' ,color='blue',label='test mean')\n",
    "    plt.plot(current_exp_test_accuracy.min(0) , ' ' ,color='blue')\n",
    "    plt.fill_between(range(num_epoch),current_exp_test_accuracy.max(0),current_exp_test_accuracy.min(0),facecolor='blue', alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.savefig(str(current_batch_norm_type)+'/'+str(current_exp_name)+'/z_all.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "def send_notification_email(letter,episode):\n",
    "    import smtplib, ssl\n",
    "\n",
    "    port = 587  # For starttls\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    sender_email = \"sendresultsforme@gmail.com\"\n",
    "    receiver_email = \"jae.duk.seo@gmail.com\"\n",
    "    password = \"Password123*\"\n",
    "    message = \"Subject: \" + str(letter) + \" : \"+str(episode)+\" is done!\"\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP(smtp_server, port) as server:\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.starttls(context=context)\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.904Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "num_eps   = 10; num_epoch = 100; learning_rate = 0.0008; batch_size = 100; beta1,beta2,adam_e = 0.9,0.999,1e-9; lamda = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.909Z"
    },
    "code_folding": [
     2
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ================================================\n",
      "                    Starting Episode: 0\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.216\t Test Acc : 0.272\t0.3520.15\n",
      "Current : 50\t Train Acc : 0.508\t Test Acc : 0.479\t0.379\n",
      "Z  Current Iter : 99/100 batch : 9900/10000 acc : 0.388\n",
      "        ================================================\n",
      "                    Starting Episode: 1\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.213\t Test Acc : 0.282\t0.2830.15\n",
      "Current : 50\t Train Acc : 0.514\t Test Acc : 0.489\t0.466\n",
      "Z  Current Iter : 99/100 batch : 9900/10000 acc : 0.483\n",
      "        ================================================\n",
      "                    Starting Episode: 2\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.205\t Test Acc : 0.261\t0.2430.15\n",
      "Current : 50\t Train Acc : 0.476\t Test Acc : 0.451\t0.454\n",
      "Z  Current Iter : 99/100 batch : 9900/10000 acc : 0.469\n",
      "        ================================================\n",
      "                    Starting Episode: 3\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.225\t Test Acc : 0.309\t0.2970.15\n",
      "Current : 50\t Train Acc : 0.561\t Test Acc : 0.537\t0.594\n",
      "Z  Current Iter : 99/100 batch : 9900/10000 acc : 0.556\n",
      "        ================================================\n",
      "                    Starting Episode: 4\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.228\t Test Acc : 0.301\t0.2960.15\n",
      "Current : 50\t Train Acc : 0.513\t Test Acc : 0.491\t0.524\n",
      "Z  Current Iter : 99/100 batch : 9900/10000 acc : 0.525\n",
      "        ================================================\n",
      "                    Starting Episode: 5\n",
      "        ================================================\n",
      "\n",
      "Current : 0\t Train Acc : 0.213\t Test Acc : 0.266\t0.2730.15\n",
      "Z  Current Iter : 48/100 batch : 13700/50000 acc : 0.47\r"
     ]
    }
   ],
   "source": [
    "# all the experiemnt (no batch)\n",
    "current_batch_norm_type = 'no_batch_cnn_CIFAR'\n",
    "all_the_exp = ['Z','A','B','C','D','E','F','G','H','I','J','K','L']\n",
    "for letter in all_the_exp:\n",
    "    current_exp_name = letter\n",
    "    sess = tf.InteractiveSession()\n",
    "    current_exp_train_accuracy = np.zeros((num_eps,num_epoch))\n",
    "    current_exp_test_accuracy  = np.zeros((num_eps,num_epoch))\n",
    "    current_weights_layer1 = np.zeros((num_eps,3,3,3,16))\n",
    "    current_weights_layer2 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer3 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer4 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer5 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer6 = np.zeros((num_eps,3,3,16,10))\n",
    "\n",
    "    for episode in range(num_eps):\n",
    "        sys.stdout.write(\"\"\"\n",
    "        ================================================\n",
    "                    Starting Episode: \"\"\" + str(episode) +  \"\"\"\n",
    "        ================================================\\n\n",
    "        \"\"\");sys.stdout.flush();\n",
    "\n",
    "        # create layers\n",
    "        l1 = CNN(3,3, 16,which_reg=current_exp_name); \n",
    "        l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "        l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "        l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "        l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "        l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "        # 2. graph \n",
    "        x = tf.placeholder(tf.float32,(batch_size,32,32,3))\n",
    "        y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "        layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "        layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "        layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "        layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "        layer5, layer5a = l5. feedforward(layer4a)\n",
    "        layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "        final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "        final_softmax = tf_softmax(final_layer)\n",
    "        cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "        correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "        accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,2,2,1])/batch_size\n",
    "        grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "        grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "        grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "        grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "        grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "        grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "        gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "        # train\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "        for iter in range(num_epoch):\n",
    "\n",
    "            # Training Accuracy    \n",
    "            for current_batch_index in range(0,len(train_images),batch_size):\n",
    "                current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "                sys.stdout.write(str(letter)+ ' '+' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "                sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "\n",
    "            # Test Accuracy    \n",
    "            for current_batch_index in range(0,len(test_images), batch_size):\n",
    "                current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "                sys.stdout.write(str(letter)+ ' '+' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "                sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "\n",
    "            # ======================== print reset ========================\n",
    "            train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "            test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "            if iter%50 == 0 :\n",
    "                sys.stdout.write(\"Current : \"+ str(iter) + \"\\t\" +\n",
    "                      \" Train Acc : \" + str(np.around(avg_acc_train/(len(train_images)/batch_size),3)) + \"\\t\" +\n",
    "                      \" Test Acc : \"  + str(np.around(avg_acc_test/(len(test_images)/batch_size),3)) + \"\\t\\n\")\n",
    "                sys.stdout.flush();\n",
    "            avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "            # ======================== print reset ========================\n",
    "\n",
    "        # save to episode all of them\n",
    "        current_exp_train_accuracy[episode,:] = train_acc; current_exp_test_accuracy[episode,:] = test_acc\n",
    "        current_weights_layer1[episode] = l1.getw().eval()\n",
    "        current_weights_layer2[episode] = l2.getw().eval()\n",
    "        current_weights_layer3[episode] = l3.getw().eval()\n",
    "        current_weights_layer4[episode] = l4.getw().eval()\n",
    "        current_weights_layer5[episode] = l5.getw().eval()\n",
    "        current_weights_layer6[episode] = l6.getw().eval()\n",
    "        \n",
    "        # send me\n",
    "        try: \n",
    "            send_notification_email(letter,episode)\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "    sess.close()\n",
    "\n",
    "    # save to gif\n",
    "    plot_rotation_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name); print(letter,' 1')\n",
    "    plot_rotation_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name); print(letter,' 2')\n",
    "    plot_rotation_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name); print(letter,' 3')\n",
    "    plot_rotation_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name); print(letter,' 4')\n",
    "    plot_rotation_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name); print(letter,' 5')\n",
    "    plot_rotation_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name); print(letter,' 6')\n",
    "\n",
    "    # save to image\n",
    "    plot_image_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "\n",
    "    # save all average and accuracy\n",
    "    plot_full_weight(current_weights_layer1,current_weights_layer2,current_weights_layer3,\n",
    "                         current_weights_layer4,current_weights_layer5,current_weights_layer6,\n",
    "                         current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.912Z"
    }
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "num_eps   = 10; num_epoch = 100; learning_rate = 0.0008; batch_size = 100; beta1,beta2,adam_e = 0.9,0.999,1e-9; lamda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.917Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# all the experiemnt (batch)\n",
    "current_batch_norm_type = 'batch_cnn_CIFAR'\n",
    "all_the_exp = ['Z','A','B','C','D','E','F','G','H','I','J','K','L']\n",
    "for letter in all_the_exp:\n",
    "    current_exp_name = letter\n",
    "    sess = tf.InteractiveSession()\n",
    "    current_exp_train_accuracy = np.zeros((num_eps,num_epoch))\n",
    "    current_exp_test_accuracy  = np.zeros((num_eps,num_epoch))\n",
    "    current_weights_layer1 = np.zeros((num_eps,3,3,3,16))\n",
    "    current_weights_layer2 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer3 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer4 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer5 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer6 = np.zeros((num_eps,3,3,16,10))\n",
    "\n",
    "    for episode in range(num_eps):\n",
    "        sys.stdout.write(\"\"\"\n",
    "        ================================================\n",
    "                    Starting Episode: \"\"\" + str(episode) +  \"\"\"\n",
    "        ================================================\\n\n",
    "        \"\"\");sys.stdout.flush();\n",
    "\n",
    "        # create layers\n",
    "        l1 = CNN(3,3, 16,which_reg=current_exp_name); l1n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        l2 = CNN(3,16,16,which_reg=current_exp_name); l2n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        l3 = CNN(3,16,16,which_reg=current_exp_name); l3n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        \n",
    "        l4 = CNN(3,16,16,which_reg=current_exp_name); l4n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        l5 = CNN(3,16,16,which_reg=current_exp_name); l5n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "        # 2. graph \n",
    "        x = tf.placeholder(tf.float32,(batch_size,32,32,3))\n",
    "        y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "        is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "        layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "        layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "        layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "        layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "        layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "        layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "        layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "        layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "        layer5, layer5a = l5. feedforward(layer4b)\n",
    "        layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "        layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "        final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "        final_softmax = tf_softmax(final_layer)\n",
    "        cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "        correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "        accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,2,2,1])/batch_size\n",
    "        grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "        grad5n = l5n.backprop(grad6p)\n",
    "        grad5p,grad5w,grad5_up = l5.backprop(grad5n)\n",
    "        grad4n = l4n.backprop(grad5p)\n",
    "        grad4p,grad4w,grad4_up = l4.backprop(grad4n,stride=2)\n",
    "        grad3n = l3n.backprop(grad4p)\n",
    "        grad3p,grad3w,grad3_up = l3.backprop(grad3n,stride=2)\n",
    "        grad2n = l2n.backprop(grad3p)\n",
    "        grad2p,grad2w,grad2_up = l2.backprop(grad2n,stride=2)\n",
    "        grad1n = l1n.backprop(grad2p)\n",
    "        grad1p,grad1w,grad1_up = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "        update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "        gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "        # train\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "        for iter in range(num_epoch):\n",
    "\n",
    "            # Training Accuracy    \n",
    "            for current_batch_index in range(0,len(train_images),batch_size):\n",
    "                current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy,gradient_update,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "                sys.stdout.write(str(letter)+ ' '+' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "                sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "\n",
    "            # Test Accuracy    \n",
    "            for current_batch_index in range(0,len(test_images), batch_size):\n",
    "                current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "                sys.stdout.write(str(letter)+ ' '+' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "                sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "\n",
    "            # ======================== print reset ========================\n",
    "            train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "            test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "            if iter%50 == 0 :\n",
    "                sys.stdout.write(\"Current : \"+ str(iter) + \"\\t\" +\n",
    "                      \" Train Acc : \" + str(np.around(avg_acc_train/(len(train_images)/batch_size),3)) + \"\\t\" +\n",
    "                      \" Test Acc : \"  + str(np.around(avg_acc_test/(len(test_images)/batch_size),3)) + \"\\t\\n\")\n",
    "                sys.stdout.flush();\n",
    "            avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "            # ======================== print reset ========================\n",
    "\n",
    "        # save to episode all of them\n",
    "        current_exp_train_accuracy[episode,:] = train_acc; current_exp_test_accuracy[episode,:] = test_acc\n",
    "        current_weights_layer1[episode] = l1.getw().eval()\n",
    "        current_weights_layer2[episode] = l2.getw().eval()\n",
    "        current_weights_layer3[episode] = l3.getw().eval()\n",
    "        current_weights_layer4[episode] = l4.getw().eval()\n",
    "        current_weights_layer5[episode] = l5.getw().eval()\n",
    "        current_weights_layer6[episode] = l6.getw().eval()\n",
    "        # send me\n",
    "        try: \n",
    "            send_notification_email(letter,episode)\n",
    "        except: \n",
    "            pass\n",
    "    sess.close()\n",
    "\n",
    "    # save to gif\n",
    "    plot_rotation_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name); print(letter,' 1')\n",
    "    plot_rotation_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name); print(letter,' 2')\n",
    "    plot_rotation_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name); print(letter,' 3')\n",
    "    plot_rotation_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name); print(letter,' 4')\n",
    "    plot_rotation_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name); print(letter,' 5')\n",
    "    plot_rotation_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name); print(letter,' 6')\n",
    "\n",
    "    # save to image\n",
    "    plot_image_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "\n",
    "    # save all average and accuracy\n",
    "    plot_full_weight(current_weights_layer1,current_weights_layer2,current_weights_layer3,\n",
    "                     current_weights_layer4,current_weights_layer5,current_weights_layer6,\n",
    "                     current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.928Z"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.932Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# FASHION\n",
    "# import Library and some random image data set\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "import seaborn    as sns \n",
    "import pandas     as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.random.seed(78); tf.set_random_seed(78)\n",
    "\n",
    "# get some of the STL data set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage import util \n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "from numpy import inf\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import animation\n",
    "%load_ext jupyternotify\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.transform import resize\n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.934Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "mnist = input_data.read_data_sets('../../../Dataset/FashionMNIST/', one_hot=True)\n",
    "x_data, train_label, y_data, test_label = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "\n",
    "train_images = x_data.reshape((55000,28,28,1))\n",
    "train_labels = train_label\n",
    "test_images  = y_data.reshape((10000,28,28,1))\n",
    "test_labels  = test_label\n",
    "\n",
    "print(train_images.shape,train_images.max(),train_images.min())\n",
    "print(train_labels.shape,train_labels.max(),train_labels.min())\n",
    "print(test_images.shape,test_images.max(),test_images.min())\n",
    "print(test_labels.shape,test_labels.max(),test_labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.940Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create the layers\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def tf_relu(x):   return tf.nn.relu(x)\n",
    "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
    "\n",
    "class CNN():\n",
    "\n",
    "    def __init__(self,k,inc,out, stddev=0.05,which_reg=0,act=tf_relu,d_act=d_tf_relu):\n",
    "        self.w              = tf.Variable(tf.random_normal([k,k,inc,out],stddev=stddev,seed=2,dtype=tf.float32))\n",
    "        self.m,self.v       = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
    "        self.act,self.d_act = act,d_act\n",
    "        self.regularizer    = which_reg\n",
    "    def getw(self): return self.w\n",
    "    def feedforward(self,input,stride=1,padding='SAME'):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides=[1,stride,stride,1],padding=padding) \n",
    "        self.layerA = self.act(self.layer)\n",
    "        return self.layer, self.layerA\n",
    "    def backprop(self,gradient,stride=1,padding='SAME'):\n",
    "        grad_part_1 = gradient\n",
    "        grad_part_2 = self.d_act(self.layer)\n",
    "        grad_part_3 = self.input\n",
    "\n",
    "        grad_middle = grad_part_1 * grad_part_2\n",
    "        grad        = tf.nn.conv2d_backprop_filter(input = grad_part_3,filter_sizes = tf.shape(self.w),  out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding) / batch_size\n",
    "        grad_pass   = tf.nn.conv2d_backprop_input (input_sizes = tf.shape(self.input),filter= self.w,out_backprop = grad_middle,strides=[1,stride,stride,1],padding=padding)\n",
    "\n",
    "        if self.regularizer == 'A': grad = grad + lamda * tf.sign(self.w)\n",
    "        if self.regularizer == 'B': grad = grad + lamda * 2.0 * self.w\n",
    "        if self.regularizer == 'C': grad = grad + lamda * (1.0/tf.sqrt(tf.square(self.w)+ 10e-8)) * self.w\n",
    "        if self.regularizer == 'D': grad = grad + lamda * -(2*self.w)/(1 + self.w**2)\n",
    "        if self.regularizer == 'E': grad = grad + lamda * -(1-tf.tanh(self.w) ** 2)\n",
    "        if self.regularizer == 'F': grad = grad + lamda * -(1-tf.tanh(self.w** 2) ** 2) * 2.0 * self.w \n",
    "        if self.regularizer == 'G': grad = grad + lamda * -(1-tf.tanh(tf.abs(self.w)) ** 2) * tf.sign(self.w)\n",
    "        if self.regularizer == 'H': grad = grad + lamda * -(1-tf.tanh(tf.abs(self.w)** 2) ** 2) * 2.0 * tf.abs(self.w) *  tf.sign(self.w)\n",
    "        if self.regularizer == 'I': grad = grad + lamda * tf.cos(self.w)\n",
    "        if self.regularizer == 'J': grad = grad + lamda * tf.sign(tf.sin(self.w)) * tf.cos(self.w)\n",
    "        if self.regularizer == 'K': grad = grad + lamda * (2)/(self.w + 10e-8)\n",
    "        if self.regularizer == 'L': grad = grad + lamda * (tf.log(self.w**2) + 2.0)\n",
    "        \n",
    "        update_w = []\n",
    "        update_w.append(tf.assign( self.m,self.m*beta1 + (1-beta1) * (grad)   ))\n",
    "        update_w.append(tf.assign( self.v,self.v*beta2 + (1-beta2) * (grad ** 2)   ))\n",
    "        m_hat = self.m / (1-beta1) ; v_hat = self.v / (1-beta2)\n",
    "        adam_middle = m_hat * learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
    "        update_w.append(tf.assign(self.w,tf.subtract(self.w,adam_middle  )))\n",
    "        return grad_pass,grad,update_w\n",
    "class tf_batch_norm_layer():\n",
    "    \n",
    "    def __init__(self,vector_shape,axis):\n",
    "        self.moving_mean = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.moving_vari = tf.Variable(tf.zeros(shape=[1,1,1,vector_shape],dtype=tf.float32))\n",
    "        self.axis        = axis\n",
    "    def feedforward(self,input,training_phase=True,eps = 1e-8):\n",
    "        self.input = input\n",
    "        self.input_size          = self.input.shape\n",
    "        self.batch,self.h,self.w,self.c = self.input_size[0].value,self.input_size[1].value,self.input_size[2].value,self.input_size[3].value\n",
    "\n",
    "        # Training Moving Average Mean         \n",
    "        def training_fn():\n",
    "            self.mean    = tf.reduce_mean(self.input,axis=self.axis ,keepdims=True)\n",
    "            self.var     = tf.reduce_mean(tf.square(self.input-self.mean),axis=self.axis,keepdims=True)\n",
    "            centered_data= (self.input - self.mean)/tf.sqrt(self.var + eps)\n",
    "            \n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean*0.9 + 0.1 * self.mean ))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari*0.9 + 0.1 * self.var  ))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        # Testing Moving Average Mean        \n",
    "        def  testing_fn():\n",
    "            centered_data   = (self.input - self.moving_mean)/tf.sqrt(self.moving_vari + eps)\n",
    "            update_variable = []\n",
    "            update_variable.append(tf.assign(self.moving_mean,self.moving_mean))\n",
    "            update_variable.append(tf.assign(self.moving_vari,self.moving_vari))\n",
    "            return centered_data,update_variable\n",
    "        \n",
    "        self.output,update_variable = tf.cond(training_phase,true_fn=training_fn,false_fn=testing_fn)\n",
    "        return self.output,update_variable\n",
    "    def backprop(self,grad,eps = 1e-8):\n",
    "        change_parts = 1.0 /(self.batch * self.h * self.w)\n",
    "        grad_sigma   = tf.reduce_sum( grad *  (self.input-self.mean)     ,axis=self.axis,keepdims=True) * -0.5 * (self.var+eps) ** -1.5\n",
    "        grad_mean    = tf.reduce_sum( grad *  (-1./tf.sqrt(self.var+eps)),axis=self.axis,keepdims=True) + grad_sigma * change_parts * 2.0 * tf.reduce_sum((self.input-self.mean),axis=self.axis,keepdims=True) * -1\n",
    "        grad_x       = grad * 1/(tf.sqrt(self.var+eps)) + grad_sigma * change_parts * 2.0 * (self.input-self.mean) + grad_mean * change_parts\n",
    "        return grad_x\n",
    "\n",
    "def save_to_image(data,name):\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = data\n",
    "    l1g,l2g,l3g,l4g,l5g,l6g = np.asarray(l1g),np.asarray(l2g),np.asarray(l3g),np.asarray(l4g),np.asarray(l5g),np.asarray(l6g)\n",
    "    plt.figure(figsize=(25,15))\n",
    "    plt.suptitle('Current Iter : ' + str(iter))\n",
    "    plt.subplot(231); plt.hist(l1g.ravel(),50); plt.title('layer 1')\n",
    "    plt.subplot(232); plt.hist(l2g.ravel(),50); plt.title('layer 2')\n",
    "    plt.subplot(233); plt.hist(l3g.ravel(),50); plt.title('layer 3')\n",
    "    plt.subplot(234); plt.hist(l4g.ravel(),50); plt.title('layer 4')\n",
    "    plt.subplot(235); plt.hist(l5g.ravel(),50); plt.title('layer 5')\n",
    "    plt.subplot(236); plt.hist(l6g.ravel(),50); plt.title('layer 6')\n",
    "    plt.savefig(name + str(iter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close('all')     \n",
    "def append_stat(current_list,data,number):\n",
    "    current_list[0].append(data[number].mean())\n",
    "    current_list[1].append(data[number].std())\n",
    "    current_list[2].append(skew    (data[number].ravel()))\n",
    "    current_list[3].append(kurtosis(data[number].ravel()))\n",
    "    current_list[4].append(np.count_nonzero(data[number]))\n",
    "    return current_list\n",
    "def transform_to_2d(data):\n",
    "    batch,width,height,chan = data.shape\n",
    "    return data.reshape((batch*width,height*chan))\n",
    "def save_to_image(main_data,one,two,three,four,five,six,experiment_name,tran_acc,test_acc,current_exp,iter):\n",
    "    plt.figure(figsize=(20,40))\n",
    "    G = gridspec.GridSpec(8, 6)\n",
    "\n",
    "    plt.figtext(0.5,1.0,\"Iter: \" + str(iter) + \" Histogram Per \" + experiment_name,ha=\"center\", va=\"top\", fontsize=35, color=\"black\")\n",
    "    plt.subplot(G[0, 0]).hist(main_data[0].ravel(),50,color='red');       plt.subplot(G[0, 0]).set_title(experiment_name+' 1')\n",
    "    plt.subplot(G[0, 1]).hist(main_data[1].ravel(),50,color='orange');    plt.subplot(G[0, 1]).set_title(experiment_name+' 2')\n",
    "    plt.subplot(G[0, 2]).hist(main_data[2].ravel(),50,color='yellow');  plt.subplot(G[0, 2]).set_title(experiment_name+' 3')\n",
    "    plt.subplot(G[0, 3]).hist(main_data[3].ravel(),50,color='green');    plt.subplot(G[0, 3]).set_title(experiment_name+' 4')\n",
    "    plt.subplot(G[0, 4]).hist(main_data[4].ravel(),50,color='blue');     plt.subplot(G[0, 4]).set_title(experiment_name+' 5')\n",
    "    plt.subplot(G[0, 5]).hist(main_data[5].ravel(),50,color='black');     plt.subplot(G[0, 5]).set_title(experiment_name+' 6')\n",
    "\n",
    "    plt.subplot(G[1, :]).set_title(\"Mean Per \"+ experiment_name)\n",
    "    plt.subplot(G[1, :]).plot(one[0]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[1, :]).plot(two[0]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[1, :]).plot(three[0],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[1, :]).plot(four[0],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[1, :]).plot(five[0],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[1, :]).plot(six[0],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[2, :]).set_title(\"Standard Deviation Per \"+ experiment_name)\n",
    "    plt.subplot(G[2, :]).plot(one[1]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[2, :]).plot(two[1]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[2, :]).plot(three[1],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[2, :]).plot(four[1],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[2, :]).plot(five[1],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[2, :]).plot(six[1],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[3, :]).set_title(\"Skewness Per \"+ experiment_name)\n",
    "    plt.subplot(G[3, :]).plot(one[2]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[3, :]).plot(two[2]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[3, :]).plot(three[2],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[3, :]).plot(four[2],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[3, :]).plot(five[2],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[3, :]).plot(six[2],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[4, :]).set_title(\"Kurtosis Per \"+ experiment_name)\n",
    "    plt.subplot(G[4, :]).plot(one[3]  ,c='red',alpha=0.9,label='1')\n",
    "    plt.subplot(G[4, :]).plot(two[3]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[4, :]).plot(three[3],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[4, :]).plot(four[3],c='green',alpha=0.9,label='4')\n",
    "    plt.subplot(G[4, :]).plot(five[3],c='blue',alpha=0.9,label='5')\n",
    "    plt.subplot(G[4, :]).plot(six[3],c='black',alpha=0.9,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[5, :]).set_title(\"# Non-Zero Per \"+ experiment_name)\n",
    "    plt.subplot(G[5, :]).plot(one[4]  ,c='red',alpha=0.9   ,label='1')\n",
    "    plt.subplot(G[5, :]).plot(two[4]  ,c='orange',alpha=0.9,label='2')\n",
    "    plt.subplot(G[5, :]).plot(three[4],c='yellow',alpha=0.9,label='3')\n",
    "    plt.subplot(G[5, :]).plot(four[4],c='green',alpha=0.9  ,label='4')\n",
    "    plt.subplot(G[5, :]).plot(five[4],c='blue',alpha=0.9   ,label='5')\n",
    "    plt.subplot(G[5, :]).plot(six[4],c='black',alpha=0.9   ,label='6')\n",
    "    plt.legend(bbox_to_anchor=(0., 0.95, 1., .05), loc=9,ncol=6, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(G[6, :]).set_title(\"Train/Test accuracy\")\n",
    "    plt.subplot(G[6, :]).plot(train_acc  ,c='red',alpha=0.9, label='Train')\n",
    "    plt.subplot(G[6, :]).plot(test_acc   ,c='blue',alpha=0.9,label='Test')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figtext(0.5,0,\"Correlation Matrix Per \"+ experiment_name,ha=\"center\", va=\"bottom\", fontsize=30, color=\"black\")\n",
    "    plt.subplot(G[7, 0]).imshow(np.corrcoef(transform_to_2d(main_data[0])),cmap='gray')\n",
    "    plt.subplot(G[7, 1]).imshow(np.corrcoef(transform_to_2d(main_data[1])),cmap='gray')\n",
    "    plt.subplot(G[7, 2]).imshow(np.corrcoef(transform_to_2d(main_data[2])),cmap='gray')\n",
    "    plt.subplot(G[7, 3]).imshow(np.corrcoef(transform_to_2d(main_data[3])),cmap='gray')\n",
    "    plt.subplot(G[7, 4]).imshow(np.corrcoef(transform_to_2d(main_data[4])),cmap='gray')\n",
    "    plt.subplot(G[7, 5]).imshow(np.corrcoef(transform_to_2d(main_data[5])),cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(current_exp + '/' + experiment_name + '/' + str(iter) + '.png')\n",
    "    plt.close('all')\n",
    "def plot_rotation_weight(current_layers,current_layer_number,current_batch_norm_type,current_exp_name): \n",
    "\n",
    "    def rotate(angle):ax.view_init(azim=angle)\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    colors = ['red','orange','yellow','green','blue','purple','black','gold','silver','cyan','pink']\n",
    "    number_of_eps = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax  = fig.add_subplot(111, projection='3d')\n",
    "    count = 0\n",
    "    for episode in range(num_eps+1):\n",
    "        if episode == num_eps:\n",
    "            ys   = current_layers.mean(0).flatten()\n",
    "            xmin = ys.min(); xmax = ys.max(); step = 0.005\n",
    "            hist,bins = np.histogram(ys, bins=np.linspace(xmin, xmax, (xmax-xmin)/step))\n",
    "            ax.bar(bins[:-1], hist, width=0.01,zs=episode, zdir='y', color=colors[episode], alpha=0.8)\n",
    "        else:\n",
    "            ys   = current_layers[episode].flatten()\n",
    "            xmin = ys.min(); xmax = ys.max(); step = 0.005\n",
    "            hist,bins = np.histogram(ys, bins=np.linspace(xmin, xmax, (xmax-xmin)/step))\n",
    "            ax.bar(bins[:-1], hist, width=0.01,zs=episode, zdir='y', color=colors[episode], alpha=0.4)\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Episode')\n",
    "    ax.get_yaxis().set_ticks(np.arange(num_eps+1))\n",
    "    ax.set_zlabel('Histogram')\n",
    "\n",
    "    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "    # make the grid lines transparent\n",
    "    ax.xaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    ax.yaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    # ax.zaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    animation.FuncAnimation(fig, rotate, frames=np.arange(0,362,2),interval=100) \\\n",
    "    .save(str(current_batch_norm_type)+'/'+str(current_exp_name)+'/weight_'+str(current_layer_number)+'.gif', dpi=80, writer='imagemagick')\n",
    "    plt.close('all')\n",
    "def plot_image_weight(current_layers,current_layer_number,current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy):\n",
    "    colors = ['red','orange','yellow','green','blue','purple','black','gold','silver','cyan','pink']\n",
    "    def rt(number): return np.around(number,4)\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    plt.rcParams.update({'font.size': 50})\n",
    "\n",
    "    fig.add_subplot(341); plt.hist(current_layers[0].flatten(),50,color=colors[0],alpha=0.8);plt.title('EPS: 1 Mean: '+str(rt(current_layers[0].mean())) +' STD: '+str(rt(current_layers[0].std())))\n",
    "    fig.add_subplot(342); plt.hist(current_layers[1].flatten(),50,color=colors[1],alpha=0.8);plt.title('EPS: 2 Mean: '+str(rt(current_layers[1].mean())) +' STD: '+str(rt(current_layers[1].std())))\n",
    "    fig.add_subplot(343); plt.hist(current_layers[2].flatten(),50,color=colors[2],alpha=0.8);plt.title('EPS: 3 Mean: '+str(rt(current_layers[2].mean())) +' STD: '+str(rt(current_layers[2].std())))\n",
    "    fig.add_subplot(344); plt.hist(current_layers[3].flatten(),50,color=colors[3],alpha=0.8);plt.title('EPS: 4 Mean: '+str(rt(current_layers[3].mean())) +' STD: '+str(rt(current_layers[3].std())))\n",
    "    \n",
    "    fig.add_subplot(345); plt.hist(current_layers[4].flatten(),50,color=colors[4],alpha=0.8);plt.title('EPS: 5 Mean: '+str(rt(current_layers[4].mean())) +' STD: '+str(rt(current_layers[4].std())))\n",
    "    fig.add_subplot(346); plt.hist(current_layers[5].flatten(),50,color=colors[5],alpha=0.8);plt.title('EPS: 6 Mean: '+str(rt(current_layers[5].mean())) +' STD: '+str(rt(current_layers[5].std())))\n",
    "    fig.add_subplot(347); plt.hist(current_layers[6].flatten(),50,color=colors[6],alpha=0.8);plt.title('EPS: 7 Mean: '+str(rt(current_layers[6].mean())) +' STD: '+str(rt(current_layers[6].std())))\n",
    "    fig.add_subplot(348); plt.hist(current_layers[7].flatten(),50,color=colors[7],alpha=0.8);plt.title('EPS: 8 Mean: '+str(rt(current_layers[7].mean())) +' STD: '+str(rt(current_layers[7].std())))\n",
    "    \n",
    "    fig.add_subplot(3,4,9);  plt.hist(current_layers[8].flatten(),50,color=colors[8],alpha=0.8);plt.title('EPS: 9 Mean: '+str(rt(current_layers[8].mean())) +' STD: '+str(rt(current_layers[8].std())))\n",
    "    fig.add_subplot(3,4,10); plt.hist(current_layers[9].flatten(),50,color=colors[9],alpha=0.8);plt.title('EPS: 10 Mean: '+str(rt(current_layers[9].mean())) +' STD: '+str(rt(current_layers[9].std())))\n",
    "    fig.add_subplot(3,4,11); plt.hist(current_layers.mean(0).flatten(),50,color=colors[10],alpha=0.8);plt.title('EPS: All Mean: '+str(rt(current_layers.mean(0).mean())) +' STD: '+str(rt(current_layers.mean(0).std())))\n",
    "    fig.add_subplot(3,4,12); \n",
    "    plt.plot(current_exp_train_accuracy.max(0),  ' ' ,color='red')\n",
    "    plt.plot(current_exp_train_accuracy.mean(0), '-' ,color='red',label='train mean')\n",
    "    plt.plot(current_exp_train_accuracy.min(0) , ' ' ,color='red')\n",
    "    plt.fill_between(range(num_epoch),current_exp_train_accuracy.max(0),current_exp_train_accuracy.min(0),facecolor='red', alpha=0.2)\n",
    "\n",
    "    plt.plot(current_exp_test_accuracy.max(0),  ' ' ,color='blue')\n",
    "    plt.plot(current_exp_test_accuracy.mean(0), '-' ,color='blue',label='test mean')\n",
    "    plt.plot(current_exp_test_accuracy.min(0) , ' ' ,color='blue')\n",
    "    plt.fill_between(range(num_epoch),current_exp_test_accuracy.max(0),current_exp_test_accuracy.min(0),facecolor='blue', alpha=0.2)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(current_batch_norm_type)+'/'+str(current_exp_name)+'/weight_'+str(current_layer_number)+'.png')\n",
    "    plt.close('all')\n",
    "def plot_full_weight(current_weights_layer1,current_weights_layer2,current_weights_layer3,\n",
    "                     current_weights_layer4,current_weights_layer5,current_weights_layer6,\n",
    "                     current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy):\n",
    "    \n",
    "    colors = ['red','orange','yellow','green','blue','purple','black','gold','silver','cyan','pink']\n",
    "    def rt(number): return np.around(number,4)\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    plt.rcParams.update({'font.size': 50})\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    \n",
    "    fig.add_subplot(331); plt.hist(current_weights_layer1.mean(0).flatten(),50,color=colors[0],alpha=0.8); plt.title('Layer 1 Histogram Mean :'+str(rt(current_weights_layer1.mean(0).mean()))+' STD: '+str(rt(current_weights_layer1.mean(0).std())))\n",
    "    fig.add_subplot(332); plt.hist(current_weights_layer2.mean(0).flatten(),50,color=colors[1],alpha=0.8); plt.title('Layer 2 Histogram Mean :'+str(rt(current_weights_layer2.mean(0).mean()))+' STD: '+str(rt(current_weights_layer2.mean(0).std())))\n",
    "    fig.add_subplot(333); plt.hist(current_weights_layer3.mean(0).flatten(),50,color=colors[2],alpha=0.8); plt.title('Layer 3 Histogram Mean :'+str(rt(current_weights_layer3.mean(0).mean()))+' STD: '+str(rt(current_weights_layer3.mean(0).std())))\n",
    "    \n",
    "    fig.add_subplot(334); plt.hist(current_weights_layer4.mean(0).flatten(),50,color=colors[3],alpha=0.8); plt.title('Layer 4 Histogram Mean :'+str(rt(current_weights_layer4.mean(0).mean()))+' STD: '+str(rt(current_weights_layer4.mean(0).std())))\n",
    "    fig.add_subplot(335); plt.hist(current_weights_layer5.mean(0).flatten(),50,color=colors[4],alpha=0.8); plt.title('Layer 5 Histogram Mean :'+str(rt(current_weights_layer5.mean(0).mean()))+' STD: '+str(rt(current_weights_layer5.mean(0).std())))\n",
    "    fig.add_subplot(336); plt.hist(current_weights_layer6.mean(0).flatten(),50,color=colors[5],alpha=0.8); plt.title('Layer 6 Histogram Mean :'+str(rt(current_weights_layer6.mean(0).mean()))+' STD: '+str(rt(current_weights_layer6.mean(0).std())))\n",
    "    \n",
    "    fig.add_subplot(gs[2,:]); \n",
    "    plt.plot(current_exp_train_accuracy.max(0),  ' ' ,color='red')\n",
    "    plt.plot(current_exp_train_accuracy.mean(0), '-' ,color='red',label='train mean')\n",
    "    plt.plot(current_exp_train_accuracy.min(0) , ' ' ,color='red')\n",
    "    plt.fill_between(range(num_epoch),current_exp_train_accuracy.max(0),current_exp_train_accuracy.min(0),facecolor='red', alpha=0.2)\n",
    "\n",
    "    plt.plot(current_exp_test_accuracy.max(0),  ' ' ,color='blue')\n",
    "    plt.plot(current_exp_test_accuracy.mean(0), '-' ,color='blue',label='test mean')\n",
    "    plt.plot(current_exp_test_accuracy.min(0) , ' ' ,color='blue')\n",
    "    plt.fill_between(range(num_epoch),current_exp_test_accuracy.max(0),current_exp_test_accuracy.min(0),facecolor='blue', alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.savefig(str(current_batch_norm_type)+'/'+str(current_exp_name)+'/z_all.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "def send_notification_email(letter,episode):\n",
    "    import smtplib, ssl\n",
    "\n",
    "    port = 587  # For starttls\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    sender_email = \"sendresultsforme@gmail.com\"\n",
    "    receiver_email = \"jae.duk.seo@gmail.com\"\n",
    "    password = \"Password123*\"\n",
    "    message = \"Subject: \" + str(letter) + \" : \"+str(episode)+\" is done!\"\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP(smtp_server, port) as server:\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.starttls(context=context)\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.945Z"
    }
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "num_eps   = 10; num_epoch = 150; learning_rate = 0.0008; batch_size = 20; beta1,beta2,adam_e = 0.9,0.999,1e-9; lamda = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.948Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# all the experiemnt (no batch)\n",
    "current_batch_norm_type = 'no_batch_cnn_FASHION'\n",
    "all_the_exp = ['Z','A','B','C','D','E','F','G','H','I','J','K','L']\n",
    "for letter in all_the_exp:\n",
    "    current_exp_name = letter\n",
    "    sess = tf.InteractiveSession()\n",
    "    current_exp_train_accuracy = np.zeros((num_eps,num_epoch))\n",
    "    current_exp_test_accuracy  = np.zeros((num_eps,num_epoch))\n",
    "    current_weights_layer1 = np.zeros((num_eps,3,3,1,16))\n",
    "    current_weights_layer2 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer3 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer4 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer5 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer6 = np.zeros((num_eps,3,3,16,10))\n",
    "\n",
    "    for episode in range(num_eps):\n",
    "        sys.stdout.write(\"\"\"\n",
    "        ================================================\n",
    "                    Starting Episode: \"\"\" + str(episode) +  \"\"\"\n",
    "        ================================================\\n\n",
    "        \"\"\");sys.stdout.flush();\n",
    "\n",
    "        # create layers\n",
    "        l1 = CNN(3,1, 16,which_reg=current_exp_name); \n",
    "        l2 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "        l3 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "\n",
    "        l4 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "        l5 = CNN(3,16,16,which_reg=current_exp_name); \n",
    "        l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "        # 2. graph \n",
    "        x = tf.placeholder(tf.float32,(batch_size,28,28,1))\n",
    "        y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "\n",
    "        layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "        layer2, layer2a = l2. feedforward(layer1a,stride=2)\n",
    "        layer3, layer3a = l3. feedforward(layer2a,stride=2)\n",
    "        layer4, layer4a = l4. feedforward(layer3a,stride=2)\n",
    "        layer5, layer5a = l5. feedforward(layer4a)\n",
    "        layer6, layer6a = l6. feedforward(layer5a)\n",
    "\n",
    "        final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "        final_softmax = tf_softmax(final_layer)\n",
    "        cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "        correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "        accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,2,2,1])/batch_size\n",
    "        grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "        grad5p,grad5w,grad5_up = l5.backprop(grad6p)\n",
    "        grad4p,grad4w,grad4_up = l4.backprop(grad5p,stride=2)\n",
    "        grad3p,grad3w,grad3_up = l3.backprop(grad4p,stride=2)\n",
    "        grad2p,grad2w,grad2_up = l2.backprop(grad3p,stride=2)\n",
    "        grad1p,grad1w,grad1_up = l1.backprop(grad2p,stride=2)\n",
    "\n",
    "        gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "        # train\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "        for iter in range(num_epoch):\n",
    "\n",
    "            # Training Accuracy    \n",
    "            for current_batch_index in range(0,len(train_images),batch_size):\n",
    "                current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy,gradient_update],feed_dict={x:current_data,y:current_label})\n",
    "                sys.stdout.write(str(letter)+ ' '+' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "                sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "\n",
    "            # Test Accuracy    \n",
    "            for current_batch_index in range(0,len(test_images), batch_size):\n",
    "                current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label})\n",
    "                sys.stdout.write(str(letter)+ ' '+' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "                sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "\n",
    "            # ======================== print reset ========================\n",
    "            train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "            test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "            if iter%50 == 0 :\n",
    "                sys.stdout.write(\"Current : \"+ str(iter) + \"\\t\" +\n",
    "                      \" Train Acc : \" + str(np.around(avg_acc_train/(len(train_images)/batch_size),3)) + \"\\t\" +\n",
    "                      \" Test Acc : \"  + str(np.around(avg_acc_test/(len(test_images)/batch_size),3)) + \"\\t\\n\")\n",
    "                sys.stdout.flush();\n",
    "            avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "            # ======================== print reset ========================\n",
    "\n",
    "        # save to episode all of them\n",
    "        current_exp_train_accuracy[episode,:] = train_acc; current_exp_test_accuracy[episode,:] = test_acc\n",
    "        current_weights_layer1[episode] = l1.getw().eval()\n",
    "        current_weights_layer2[episode] = l2.getw().eval()\n",
    "        current_weights_layer3[episode] = l3.getw().eval()\n",
    "        current_weights_layer4[episode] = l4.getw().eval()\n",
    "        current_weights_layer5[episode] = l5.getw().eval()\n",
    "        current_weights_layer6[episode] = l6.getw().eval()\n",
    "        # send me\n",
    "        try: \n",
    "            send_notification_email(letter,episode)\n",
    "        except: \n",
    "            pass\n",
    "    sess.close()\n",
    "\n",
    "    # save to gif\n",
    "    plot_rotation_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name); print(letter,' 1')\n",
    "    plot_rotation_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name); print(letter,' 2')\n",
    "    plot_rotation_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name); print(letter,' 3')\n",
    "    plot_rotation_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name); print(letter,' 4')\n",
    "    plot_rotation_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name); print(letter,' 5')\n",
    "    plot_rotation_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name); print(letter,' 6')\n",
    "\n",
    "    # save to image\n",
    "    plot_image_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "\n",
    "    # save all average and accuracy\n",
    "    plot_full_weight(current_weights_layer1,current_weights_layer2,current_weights_layer3,\n",
    "                         current_weights_layer4,current_weights_layer5,current_weights_layer6,\n",
    "                         current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.951Z"
    }
   },
   "outputs": [],
   "source": [
    "# set hyper parameter\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "num_eps   = 10; num_epoch = 150; learning_rate = 0.0008; batch_size = 20; beta1,beta2,adam_e = 0.9,0.999,1e-9; lamda = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-24T16:59:37.954Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# all the experiemnt (batch)\n",
    "current_batch_norm_type = 'batch_cnn_FASHION'\n",
    "all_the_exp = ['Z','A','B','C','D','E','F','G','H','I','J','K','L']\n",
    "for letter in all_the_exp:\n",
    "    current_exp_name = letter\n",
    "    sess = tf.InteractiveSession()\n",
    "    current_exp_train_accuracy = np.zeros((num_eps,num_epoch))\n",
    "    current_exp_test_accuracy  = np.zeros((num_eps,num_epoch))\n",
    "    current_weights_layer1 = np.zeros((num_eps,3,3,1,16))\n",
    "    current_weights_layer2 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer3 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer4 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer5 = np.zeros((num_eps,3,3,16,16))\n",
    "    current_weights_layer6 = np.zeros((num_eps,3,3,16,10))\n",
    "\n",
    "    for episode in range(num_eps):\n",
    "        sys.stdout.write(\"\"\"\n",
    "        ================================================\n",
    "                    Starting Episode: \"\"\" + str(episode) +  \"\"\"\n",
    "        ================================================\\n\n",
    "        \"\"\");sys.stdout.flush();\n",
    "\n",
    "        # create layers\n",
    "        l1 = CNN(3,3, 16,which_reg=current_exp_name); l1n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        l2 = CNN(3,16,16,which_reg=current_exp_name); l2n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        l3 = CNN(3,16,16,which_reg=current_exp_name); l3n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        \n",
    "        l4 = CNN(3,16,16,which_reg=current_exp_name); l4n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        l5 = CNN(3,16,16,which_reg=current_exp_name); l5n = tf_batch_norm_layer(16,(0,1,2))\n",
    "        l6 = CNN(3,16,10,which_reg=current_exp_name); \n",
    "\n",
    "        # 2. graph \n",
    "        x = tf.placeholder(tf.float32,(batch_size,28,28,1))\n",
    "        y = tf.placeholder(tf.float32,(batch_size,10))\n",
    "        is_train = tf.placeholder_with_default(True,())\n",
    "\n",
    "        layer1, layer1a = l1. feedforward(x,stride=2)\n",
    "        layer1b,update1 = l1n.feedforward(layer1a,is_train)\n",
    "        layer2, layer2a = l2. feedforward(layer1b,stride=2)\n",
    "        layer2b,update2 = l2n.feedforward(layer2a,is_train)\n",
    "        layer3, layer3a = l3. feedforward(layer2b,stride=2)\n",
    "        layer3b,update3 = l3n.feedforward(layer3a,is_train)\n",
    "        layer4, layer4a = l4. feedforward(layer3b,stride=2)\n",
    "        layer4b,update4 = l4n.feedforward(layer4a,is_train)\n",
    "        layer5, layer5a = l5. feedforward(layer4b)\n",
    "        layer5b,update5 = l5n.feedforward(layer5a,is_train)\n",
    "        layer6, layer6a = l6. feedforward(layer5b)\n",
    "\n",
    "        final_layer   = tf.reduce_mean(layer6a,(1,2))\n",
    "        final_softmax = tf_softmax(final_layer)\n",
    "        cost          = -tf.reduce_mean(y * tf.log(final_softmax + 1e-8))\n",
    "        correct_prediction = tf.equal(tf.argmax(final_softmax, 1), tf.argmax(y, 1))\n",
    "        accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        gradient = tf.tile((final_softmax-y)[:,None,None,:],[1,2,2,1])/batch_size\n",
    "        grad6p,grad6w,grad6_up = l6.backprop(gradient)\n",
    "        grad5n = l5n.backprop(grad6p)\n",
    "        grad5p,grad5w,grad5_up = l5.backprop(grad5n)\n",
    "        grad4n = l4n.backprop(grad5p)\n",
    "        grad4p,grad4w,grad4_up = l4.backprop(grad4n,stride=2)\n",
    "        grad3n = l3n.backprop(grad4p)\n",
    "        grad3p,grad3w,grad3_up = l3.backprop(grad3n,stride=2)\n",
    "        grad2n = l2n.backprop(grad3p)\n",
    "        grad2p,grad2w,grad2_up = l2.backprop(grad2n,stride=2)\n",
    "        grad1n = l1n.backprop(grad2p)\n",
    "        grad1p,grad1w,grad1_up = l1.backprop(grad1n,stride=2)\n",
    "\n",
    "        update_ops  = update1 + update2 + update3 + update4 + update5\n",
    "        gradient_update = grad6_up + grad5_up + grad4_up + grad3_up + grad2_up + grad1_up \n",
    "\n",
    "        # train\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        avg_acc_train = 0; avg_acc_test  = 0; train_acc = [];test_acc = []\n",
    "\n",
    "        for iter in range(num_epoch):\n",
    "\n",
    "            # Training Accuracy    \n",
    "            for current_batch_index in range(0,len(train_images),batch_size):\n",
    "                current_data  = train_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = train_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy,gradient_update,update_ops],feed_dict={x:current_data,y:current_label})\n",
    "                sys.stdout.write(str(letter)+ ' '+' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(train_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "                sys.stdout.flush(); avg_acc_train = avg_acc_train + sess_results[0]\n",
    "\n",
    "            # Test Accuracy    \n",
    "            for current_batch_index in range(0,len(test_images), batch_size):\n",
    "                current_data  = test_images[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                current_label = test_labels[current_batch_index:current_batch_index+batch_size].astype(np.float32)\n",
    "                sess_results  = sess.run([accuracy],feed_dict={x:current_data,y:current_label,is_train:False})\n",
    "                sys.stdout.write(str(letter)+ ' '+' Current Iter : ' + str(iter) + '/'+ str(num_epoch)  + ' batch : ' + str(current_batch_index) + '/'+ str(len(test_images)) + ' acc : ' + str(sess_results[0]) + '\\r')\n",
    "                sys.stdout.flush(); avg_acc_test = avg_acc_test + sess_results[0]   \n",
    "\n",
    "            # ======================== print reset ========================\n",
    "            train_acc.append(avg_acc_train/(len(train_images)/batch_size))\n",
    "            test_acc .append(avg_acc_test / (len(test_images)/batch_size))\n",
    "            if iter%50 == 0 :\n",
    "                sys.stdout.write(\"Current : \"+ str(iter) + \"\\t\" +\n",
    "                      \" Train Acc : \" + str(np.around(avg_acc_train/(len(train_images)/batch_size),3)) + \"\\t\" +\n",
    "                      \" Test Acc : \"  + str(np.around(avg_acc_test/(len(test_images)/batch_size),3)) + \"\\t\\n\")\n",
    "                sys.stdout.flush();\n",
    "            avg_acc_train = 0 ; avg_acc_test  = 0\n",
    "            # ======================== print reset ========================\n",
    "\n",
    "        # save to episode all of them\n",
    "        current_exp_train_accuracy[episode,:] = train_acc; current_exp_test_accuracy[episode,:] = test_acc\n",
    "        current_weights_layer1[episode] = l1.getw().eval()\n",
    "        current_weights_layer2[episode] = l2.getw().eval()\n",
    "        current_weights_layer3[episode] = l3.getw().eval()\n",
    "        current_weights_layer4[episode] = l4.getw().eval()\n",
    "        current_weights_layer5[episode] = l5.getw().eval()\n",
    "        current_weights_layer6[episode] = l6.getw().eval()\n",
    "        # send me\n",
    "        try: \n",
    "            send_notification_email(letter,episode)\n",
    "        except: \n",
    "            pass\n",
    "    sess.close()\n",
    "\n",
    "    # save to gif\n",
    "    plot_rotation_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name); print(letter,' 1')\n",
    "    plot_rotation_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name); print(letter,' 2')\n",
    "    plot_rotation_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name); print(letter,' 3')\n",
    "    plot_rotation_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name); print(letter,' 4')\n",
    "    plot_rotation_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name); print(letter,' 5')\n",
    "    plot_rotation_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name); print(letter,' 6')\n",
    "\n",
    "    # save to image\n",
    "    plot_image_weight(current_weights_layer1,'1',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer2,'2',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer3,'3',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer4,'4',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer5,'5',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "    plot_image_weight(current_weights_layer6,'6',current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)\n",
    "\n",
    "    # save all average and accuracy\n",
    "    plot_full_weight(current_weights_layer1,current_weights_layer2,current_weights_layer3,\n",
    "                     current_weights_layer4,current_weights_layer5,current_weights_layer6,\n",
    "                     current_batch_norm_type,current_exp_name,current_exp_train_accuracy,current_exp_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T02:01:04.962167Z",
     "start_time": "2019-01-21T01:59:54.222380Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
